<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>人工智能初步学习 | 好好学习，天天向上</title><meta name="author" content="星星之火"><meta name="copyright" content="星星之火"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="人工智能期末复习人工智能绪论人工智能定义人工智能是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的一门新技术科学。 人工智能、基因工程、纳米科学被认为是21世纪的3大尖端技术。 人工智能发展历史 1956，达特茅斯会议中，AI一词诞生 1970-1980，大规模数据和复杂任务不能完成，计算能力无法突破（低谷） 1982后，神经网络+5代计算机（专家系统） 1990-2000，D">
<meta property="og:type" content="article">
<meta property="og:title" content="人工智能初步学习">
<meta property="og:url" content="http://tjy531.com.cn/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/index.html">
<meta property="og:site_name" content="好好学习，天天向上">
<meta property="og:description" content="人工智能期末复习人工智能绪论人工智能定义人工智能是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的一门新技术科学。 人工智能、基因工程、纳米科学被认为是21世纪的3大尖端技术。 人工智能发展历史 1956，达特茅斯会议中，AI一词诞生 1970-1980，大规模数据和复杂任务不能完成，计算能力无法突破（低谷） 1982后，神经网络+5代计算机（专家系统） 1990-2000，D">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/waibaowebsite.jpg">
<meta property="article:published_time" content="2023-11-27T05:52:37.000Z">
<meta property="article:modified_time" content="2023-12-12T15:12:14.572Z">
<meta property="article:author" content="星星之火">
<meta property="article:tag" content="学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/waibaowebsite.jpg"><link rel="shortcut icon" href="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/waibaowebsite.jpg"><link rel="canonical" href="http://tjy531.com.cn/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: 星星之火","link":"链接: ","source":"来源: 好好学习，天天向上","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '人工智能初步学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-12-12 23:12:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><script>const preloader = {
  endLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = 'hidden';
    document.getElementById('loading-box').classList.remove("loaded")
  }
}

preloader.initLoading()
window.addEventListener('load',()=> { preloader.endLoading() })

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/waibaowebsite.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 菜单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="好好学习，天天向上"><span class="site-name">好好学习，天天向上</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fas fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 菜单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">人工智能初步学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-11-27T05:52:37.000Z" title="发表于 2023-11-27 13:52:37">2023-11-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-12T15:12:14.572Z" title="更新于 2023-12-12 23:12:14">2023-12-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%A4%A7%E5%AD%A6/">大学</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>29分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="人工智能初步学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/#post-comment"><span class="waline-comment-count" data-path="/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="人工智能期末复习"><a href="#人工智能期末复习" class="headerlink" title="人工智能期末复习"></a>人工智能期末复习</h1><h2 id="人工智能绪论"><a href="#人工智能绪论" class="headerlink" title="人工智能绪论"></a>人工智能绪论</h2><h3 id="人工智能定义"><a href="#人工智能定义" class="headerlink" title="人工智能定义"></a>人工智能定义</h3><p>人工智能是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的一门新技术科学。</p>
<p><strong>人工智能、基因工程、纳米科学</strong>被认为是21世纪的3大尖端技术。</p>
<h3 id="人工智能发展历史"><a href="#人工智能发展历史" class="headerlink" title="人工智能发展历史"></a>人工智能发展历史</h3><ol>
<li>1956，达特茅斯会议中，AI一词诞生</li>
<li>1970-1980，大规模数据和复杂任务不能完成，计算能力无法突破（低谷）</li>
<li>1982后，神经网络+5代计算机（专家系统）</li>
<li>1990-2000，DARPA无法实现，政府投入缩减（低谷）</li>
<li>2006-至今，突破性进展，进入发展热潮（深度学习）</li>
</ol>
<h2 id="人工智能数学基础"><a href="#人工智能数学基础" class="headerlink" title="人工智能数学基础"></a>人工智能数学基础</h2><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>假设函数 y &#x3D; f(x) 在某个区间上的导数存在，则在此区间上某点x<del>1</del><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_0.png" style="width: 250px;"></p>
<p>导数是用于研究函数在某一点附近的局部性质，用以刻画曲线或曲面的弯曲程度。</p>
<p>复合计算：</p>
<img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_1.png" style="width:300px;">

<p>高阶导数：导数 y’&#x3D;f’(x) 仍是 x 的函数，可对导函数再次求导。</p>
<p>函数f(x)的泰勒展开式：</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_2.png" alt="image-20231127143046876"></p>
<p>常用的泰勒展开</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_3.png" style="width:300px;"><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_5.png " style="width:300px;"></p>
<img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_4.png" style="width:300px">

<h3 id="概率论基础"><a href="#概率论基础" class="headerlink" title="概率论基础"></a>概率论基础</h3><h3 id="矩阵基础"><a href="#矩阵基础" class="headerlink" title="矩阵基础"></a>矩阵基础</h3><h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>搜索技术是问题求解的主要手段之一</p>
<p>搜索问题定义：可以用6个组成部分来形式化描述：</p>
<ul>
<li>状态空间S：所有可以的状态集合</li>
<li>初始状态S<del>0</del>：系统的初始状态</li>
<li>动作状态A：可用的动作集合</li>
<li>转移函数T(s,a): 确定在状态s下执行动作a后到达的状态</li>
<li>损耗函数c(s,a,s’): 在状态s通过动作a到达状态s’的损耗</li>
<li>目标测试函数G(s): 判断给定的状态s是否为目标状态</li>
</ul>
<p>解：将系统由初始状态到目标状态的一系列动作称为解。取得最小总损耗的解为最优解。</p>
<h3 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a>搜索策略</h3><h4 id="盲目搜索"><a href="#盲目搜索" class="headerlink" title="盲目搜索"></a>盲目搜索</h4><p>没有先验知识，按照事先确定的排序搜索</p>
<ul>
<li>深度优先搜索（选择当前最深的节点，先进后出）</li>
<li>宽度优先搜索（选择当前最浅的节点，先进先出）</li>
</ul>
<h5 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">初始化栈Z,令其为空;</span><br><span class="line">初始化访问表V,令其为空;</span><br><span class="line">将初始节点s放入栈z;</span><br><span class="line"><span class="keyword">while</span>(栈Z非空)</span><br><span class="line">       弹出栈顶节点n;</span><br><span class="line">       <span class="keyword">if</span>(n为目标节点)</span><br><span class="line">             返回成功;</span><br><span class="line">       <span class="keyword">if</span>(节点n不在访问表v中);</span><br><span class="line">              扩展节点n, N是当前节点n所有动作能够转移到的后继节点的集合;</span><br><span class="line">               将节点n加入访问表V中;</span><br><span class="line">               <span class="keyword">for</span> m in N:</span><br><span class="line">                   将m放入栈顶;</span><br><span class="line">返回失败;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="宽度优先搜索"><a href="#宽度优先搜索" class="headerlink" title="宽度优先搜索"></a>宽度优先搜索</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">初始化先入先出队列Q , 令其为空;</span><br><span class="line">初始化访问表V , 令其为空;</span><br><span class="line">将初始节点s 放入先入先出队列Q ;</span><br><span class="line"><span class="keyword">while</span>(队列Q 非空)</span><br><span class="line">     弹出队列Q 的队首节点n ;</span><br><span class="line">      <span class="keyword">if</span>(n 为目标节点)</span><br><span class="line">            返回成功;</span><br><span class="line">       <span class="keyword">if</span>(节点n 不在访问表V 中)</span><br><span class="line">            扩展节点n ,N 是当前节点n 所有动作能够转移到的后继节点的集合;</span><br><span class="line">            将节点n 加入访问表V 中;</span><br><span class="line">            <span class="keyword">for</span> m in N ;</span><br><span class="line">                将 m 放入队列Q 的队尾;</span><br><span class="line">返回失败;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">深度优先搜索</th>
<th align="center">宽度优先搜索</th>
</tr>
</thead>
<tbody><tr>
<td align="center">数据结构</td>
<td align="center">栈</td>
<td align="center">队列</td>
</tr>
<tr>
<td align="center">优点</td>
<td align="center">可以不存访问表，节省空间</td>
<td align="center">不存在死循环，单位耗散下最优</td>
</tr>
<tr>
<td align="center">缺点</td>
<td align="center">如果不用访问表，可能存在死循环</td>
<td align="center">需要空间较大</td>
</tr>
<tr>
<td align="center">共同点</td>
<td align="center">均会形成搜索树，只是扩展节点的顺序不同</td>
<td align="center"></td>
</tr>
</tbody></table>
<p>复杂度分析：？？？？</p>
<h4 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h4><p>根据先验知识，按照动态确定的排序搜索</p>
<ul>
<li>贪婪搜索</li>
<li>A^*^ 搜索</li>
</ul>
<h5 id="贪婪搜索"><a href="#贪婪搜索" class="headerlink" title="贪婪搜索"></a>贪婪搜索</h5><p>选择当前代价<strong>估计h值最小</strong>的节点，用优先队列数据结构存储候选节点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> 初始化以启发函数h 为键值的优先队列Q , 令其为空；</span><br><span class="line"> 初始化访问表V , 令其为空;</span><br><span class="line"> 将初始节点s 放入优先队列Q ;</span><br><span class="line"> <span class="keyword">while</span>(优先队列Q 非空)</span><br><span class="line">       弹出优先队列Q 中h 值最小的节点n ;</span><br><span class="line">        <span class="keyword">if</span>(n 为目标节点)</span><br><span class="line">              返回成功; //从目标节点n 由父节点指针回溯到初始节点s 的路径即为所得路径</span><br><span class="line">         <span class="keyword">if</span>(节点n 不在访问表V 中)</span><br><span class="line">        	    扩展节点n , 并设 N = 当前节点n 所有动作能够转移到的节点的集合;</span><br><span class="line">         	    将节点n 加入访问表V 中;</span><br><span class="line">            <span class="keyword">for</span> 节点 m <span class="keyword">in</span> N :</span><br><span class="line">                  <span class="keyword">if</span>(m 不在优先队列Q 中)</span><br><span class="line">                         将 m 放入优先队列Q , 将后继节点 m 的父节点指针指向n ;</span><br><span class="line">返回失败;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>缺点：找到的解可能不是最优的，贪婪搜索没有考虑到到达当前节点已有的代价</p>
<h5 id="A-搜索"><a href="#A-搜索" class="headerlink" title="A^*^ 搜索"></a>A^*^ 搜索</h5><p><strong>实际代价函数</strong> g(s)：<strong>起始节点到达节点</strong>s<strong>的代价值</strong></p>
<p><strong>总代价函数</strong>f(s)&#x3D;g(s)+h(s)</p>
<p><strong>A</strong>搜索：选择当前代价估计<strong>f值</strong>最小的节点，用优先队列数据结构存储候选节点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">初始化以启发函数h为键值的优先队列Q , 令其为空;</span><br><span class="line">初始化访问表V , 令其为空;</span><br><span class="line">将初始节点s 放入优先队列Q, g(s) = <span class="number">0</span>, f(s) = h(s);</span><br><span class="line"><span class="keyword">while</span>(优先队列Q 非空)</span><br><span class="line">       弹出优先队列Q 中f 值最小的节点n ;</span><br><span class="line">       <span class="keyword">if</span>(n 为目标节点)</span><br><span class="line">             返回成功; //从目标节点n 由父节点指针回溯到初始节点s 的路径即为所得路径</span><br><span class="line">       <span class="keyword">if</span>(n 不在访问表 V 中)  </span><br><span class="line">     		扩展节点n , 并设 N = 当前节点n 所有动作能够转移到的节点的集合;</span><br><span class="line">     		将节点n 加入访问表V 中;</span><br><span class="line">      		<span class="keyword">for</span> 节点m <span class="keyword">in</span> N :</span><br><span class="line">            new_g = g(n) + c(n, m);</span><br><span class="line">            new_f = new_g + h(m);</span><br><span class="line">              <span class="keyword">if</span>(m 在优先队列Q 中)</span><br><span class="line">               	<span class="keyword">if</span>(new_f &lt; Q 中的f(m))</span><br><span class="line">                 	将后继节点m 的父节点指针指向n;</span><br><span class="line">                 	更新Q 中的f(m) =new_f, g(m) = new_g;</span><br><span class="line">              <span class="keyword">else</span></span><br><span class="line">	 			f(m) = new_f ;</span><br><span class="line">                   g(m) =new_g ;</span><br><span class="line">                   将m 放入优先队列Q , 将后继节点m 的父节点指针指向n ;</span><br><span class="line">返回失败;</span><br></pre></td></tr></table></figure>

<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312072005619.png" alt="image-20231207200530422"></p>
<p>证明：</p>
<p>1.<strong>假设</strong>n<del>j</del><strong>为</strong>n<del>i</del><strong>的后继节点，根据一致性可得</strong>f(n<del>j</del> )≥f(n<del>i</del>)，节点总是比后继节点代价小</p>
<p>2.<strong>其次沿着任意路径上的节点</strong>f(n)<strong>值是非递减的</strong></p>
<p>3.<strong>当</strong>A搜索扩展到**n<del>i</del><strong>时，到达</strong>n<del>i</del>**的最优路径已经找到（反证法）</p>
<p>​		a)<strong>假设尚未找到，则最优路径上必有一点</strong>n<del>i</del>‘未扩展</p>
<p>​		b)<strong>由于</strong>n<del>i</del><strong>为</strong>最优路径的终点，由<strong>2</strong>得<strong>f(n<del>i</del> )≥f(n<del>i</del>′ )<strong>，那么</strong>n<del>i</del>′</strong>应先于**n<del>i</del>**扩展，与假设矛盾（即若是有其它未发现的节点到n<del>i</del>更优，那么按照算法来说这个节点应当先于n<del>i</del> 被找到）</p>
<table>
<thead>
<tr>
<th></th>
<th>贪婪搜索</th>
<th>A^*^ 搜索</th>
</tr>
</thead>
<tbody><tr>
<td>数据结构</td>
<td>优先队列</td>
<td>优先队列</td>
</tr>
<tr>
<td>理论保证</td>
<td>无</td>
<td>若h是可许的，那么找到的解是最优的，若h 具有一致性，那么找到的解是最优的</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h4 id="局部搜索"><a href="#局部搜索" class="headerlink" title="局部搜索"></a>局部搜索</h4><p>在许多问题中，我们只关心搜索算法返回的状态是否达到目标，而不关心从初始状态开始到达目标的路径。</p>
<p><strong>优点：不需要维护搜索树；占用内存少（不用存储路径）；在连续的并且状态空间很大的问题中，通常都可以找到足够好的解；以时间换精度</strong></p>
<p>局部搜索算法</p>
<ul>
<li>爬山法</li>
<li>模拟退火</li>
<li>遗传算法</li>
</ul>
<h5 id="爬山法"><a href="#爬山法" class="headerlink" title="爬山法"></a>爬山法</h5><p><strong>核心：不断移动至邻域内的最优点</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">当前解 ← 初始解</span><br><span class="line">repeat：</span><br><span class="line">	L ← 当前解的邻域</span><br><span class="line">	新解 ← L中评估值最高的解</span><br><span class="line">	<span class="keyword">if</span> 新解的评估值 &gt; 当前解的评估值：</span><br><span class="line">	        当前解 ← 新解</span><br><span class="line">	<span class="keyword">else</span>：</span><br><span class="line">                结束循环</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>特点</p>
<p>避免了遍历全部节点，但往往只能找到一个局部最优解</p>
<p>解决方式：多次随机初始化</p>
<h5 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h5><p>在爬山法的基础上，用温度控制搜索的随机程度。每一个解都被赋予一个能量函数E(i),并要求目标状态处于最低能量状态。</p>
<ul>
<li>当<strong>温度较高时，此时解的能量几乎没有作用，“粒子”的行为比较“活跃”，选择下一状态的策略更接近随机游走</strong></li>
<li>当<strong>温度逐渐降低时，会更加偏向与能量小的解。能量下降，“粒子”的热运动逐渐减弱，故选择下一状态的策略更倾向于选择更优的解</strong></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">当前解 ← 初始解</span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span> to ∞ do：</span><br><span class="line">	T ← 第t次迭代的温度</span><br><span class="line">	L ← 当前解的邻域</span><br><span class="line">	新解 ← L中随机选择一个解</span><br><span class="line">	<span class="keyword">if</span> 𝐸(𝑗)≤𝐸(𝑖)：</span><br><span class="line">		接受新解：当前解 ← 新解</span><br><span class="line">	<span class="keyword">else</span>：</span><br><span class="line"> 		转移接受概率P ← exp(−(𝐸(j)−𝐸(i))/𝐾𝑇)</span><br><span class="line"> 		以P的概率接受新解</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在温度足够高时，概率几乎为1，每一个状态都有可能被访问。当温度逐渐降低，能够被访问到的状态将逐渐收敛到几个能量最小的状态中，从而找到局部最优解。</p>
<table>
<thead>
<tr>
<th></th>
<th>物理退火过程</th>
<th>模拟退火算法</th>
</tr>
</thead>
<tbody><tr>
<td>对象</td>
<td>物理系统的某一状态</td>
<td>组合优化问题的某一个解</td>
</tr>
<tr>
<td>评估</td>
<td>状态的能量</td>
<td>解的评估函数值</td>
</tr>
<tr>
<td>目标</td>
<td>能量最低的状态</td>
<td>优化问题的最优解</td>
</tr>
<tr>
<td>控制变量</td>
<td>温度</td>
<td>搜索控制参数T</td>
</tr>
</tbody></table>
<h5 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h5><p><strong>模拟生物种群基因的变异，交叉融合，自然选择等算子</strong></p>
<p><strong>实现对最优化问题解的参数空间进行高效的搜索的过程</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">初始化：随机生成 N 个个体的种群</span><br><span class="line">计算种群中每个个体的适应度函数</span><br><span class="line">生成一个新的种群：</span><br><span class="line">	 选择：根据适应度函数有放回的选择N对父母个体</span><br><span class="line">	交叉：每一对父母个体用交叉算子生成下一代个体</span><br><span class="line">	变异：每一个生成的个体执行变异算子</span><br><span class="line">	判断是否找到最优解个体，如果否，则继续生成下一代种群</span><br></pre></td></tr></table></figure>

<img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081348843.png" alt="image-20231208134857642" style="zoom:80%;" />

<h5 id="三种算法对比"><a href="#三种算法对比" class="headerlink" title="三种算法对比"></a>三种算法对比</h5><ul>
<li>爬山法从初始解开始，对领域内的局部空间进行有限的探索，并移动到邻域内的最优解，算法一直循环该过程，直到达到局部最优解时终止。优点在于简单直观，容易实现，缺点是容易陷入局部最优解的问题，对于存在多个局部最优解的问题，可能无法找到全局最优解</li>
<li>模拟退火将爬山法与随机游走结合起来，在随机游走阶段，可随机地选择邻域内的一个状态作为下一个状态，有利于摆脱局部最优。该算法在一定程度上避免困在局部最优，同时也提高了搜索的效率</li>
<li>遗传算法的思想来源于自然界的生物演化，通过模拟生物种群基因的变异、交叉融合、自然选择等算子，实现对最优化问题解的参数空间进行高效的搜索过程。该算法能够在求解较为复杂的组合优化问题时，通常能够在有效时间内获得较好的结果。</li>
</ul>
<h4 id="对抗搜索"><a href="#对抗搜索" class="headerlink" title="对抗搜索"></a>对抗搜索</h4><p><strong>多</strong>Agent环境，其中每个Agent需要考虑到其他Agent行动及其对自身的影响，其他Agent的不可预测性可能导致该Agent问题求解过程中的偶发性。</p>
<p>竞争环境：每个Agent的目标之间有冲突</p>
<p>博弈：有完整信息的、确定性的、轮流行动的、两个游戏者的零和游戏</p>
<p>确定的、完全可观察的环境中两个Agent必须轮流行动，在游戏结束时效用值总是相等且符号相反</p>
<p>博弈要求具备在无法计算最优决策情况下也要做出决策。</p>
<ul>
<li>智能体P ：&#x3D;{1, …, N} （通常轮流玩）</li>
<li>状态空间S ：所有可能的状态集合</li>
<li>初始状态s_0 ：系统的起始状态</li>
<li>动作空间A ：可用的动作集合</li>
<li>转移函数T(s,a) ：确定在状态s下执行动作a后到达的状态</li>
<li>终止函数G(s) ：判断给定的状态s是否为终止状态</li>
<li>收益函数U(p)  ：在终止状态p的收益</li>
<li>目标是找到一个策略：状态到动作的映射</li>
</ul>
<h5 id="极大极小搜索"><a href="#极大极小搜索" class="headerlink" title="极大极小搜索"></a>极大极小搜索</h5><p>基本思想：使用一个收益评估函数v(p) 对给定的中间节点p 进行评估，并通过搜索找到使收益评估函数最大（或最小）的动作</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(节点p 是终止节点)            //收益函数中的第一种情况</span><br><span class="line">       返回节点p 的收益函数𝑈(𝑝)</span><br><span class="line"><span class="keyword">if</span>(节点p 是极大方)               //收益函数中的第二种情况</span><br><span class="line">       𝑣 :=−∞</span><br><span class="line">       <span class="keyword">for</span> x <span class="keyword">in</span> 子节点集合</span><br><span class="line">            𝑣 :=<span class="built_in">max</span>⁡(𝑣 , 极小极大搜索(𝑥 , 否))//递归计算极小方节点的收益函数</span><br><span class="line">       返回v</span><br><span class="line"><span class="keyword">else</span>                                      //收益函数中的第三种情况</span><br><span class="line">        𝑣 :=+∞</span><br><span class="line">       <span class="keyword">for</span> x <span class="keyword">in</span> 子节点集合</span><br><span class="line">               𝑣 :=<span class="built_in">min</span>⁡(𝑣 , 极小极大搜索(𝑥 , 是))       //递归计算极大方节点的收益函数</span><br><span class="line">      返回v</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>优点： 能找到最优策略</p>
<p>缺点：需要展开整个搜索树</p>
<h5 id="alpha-beta-剪枝搜索"><a href="#alpha-beta-剪枝搜索" class="headerlink" title="alpha-beta 剪枝搜索"></a>alpha-beta 剪枝搜索</h5><p>Alpha-Beta 剪枝搜索通过避免不必要的节点搜索来提高算法的运行效率，是对极大极小搜索算法的优化。</p>
<p><strong>基本思想</strong>：如果当前节点已知对手存在一个策略是自己获得的收益少于之前某个节点能够获得的收益，那玩家一定不会选择当前节点，故无需继续搜索当前节点的剩余子节点。</p>
<p>引入alpha,beta 两个变量</p>
<p>alpha: 表示到目前为止的路径上发现的max玩家当前的最优值</p>
<p>beta：表示到目前为止的路径上发现的min玩家当前的最优值</p>
<p>如果在某一个节点有alpha&gt;&#x3D;beta, 则说明该玩家当前的最优策略Beta 劣于之前已有的最优策略alpha，故无需搜索当前节点的剩余子节点，可以进行剪枝操作（象棋举例：已知当自己下a 时对面吃掉自己的马是最坏的情况，但当自己下b 时会知道对面最少可以吃掉自己的车，所以这个时候就知道没有必要再去思考下b 还会带来的更坏的后果了，因为此时已经比下a的情况更糟糕了）</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081519848.png" alt="image-20231208151901768"></p>
<p>初始化： alpha &#x3D; -inf , beta &#x3D;  inf </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(节点 p 是终止节点)          //收益函数中的第一种情况</span><br><span class="line">        返回 节点p 的收益函数 𝑈(𝑝)</span><br><span class="line"><span class="keyword">if</span>(节点 p 是极大方)             //收益函数中的第二种情况</span><br><span class="line">        𝑣 :=−∞ </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> 节点 p 的子节点集合</span><br><span class="line">         𝑣 :=<span class="built_in">max</span>⁡(𝑣,  AlphaBeta搜索(𝑥 ,𝛼,𝛽, 否))//递归计算极小方节点的收益函数</span><br><span class="line"> 		  𝛼:=<span class="built_in">max</span>⁡(𝛼,𝑣)</span><br><span class="line">		  <span class="keyword">if</span> (𝛼≥𝛽)              //剪枝, 降低搜索量</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        返回𝑣</span><br><span class="line"><span class="keyword">else</span>                   //收益函数中的第三种情况: 节点 p 是极小方</span><br><span class="line">        𝑣 :=+∞ ;</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> 节点 p 的子节点集合</span><br><span class="line">                𝑣 ≔<span class="built_in">min</span>⁡(𝑣,  AlphBeta搜索(𝑥, 𝛼, 𝛽, 是))//递归计算极小方节点的收益函数</span><br><span class="line">                𝛽:=<span class="built_in">min</span>⁡(𝛽,  𝑣)</span><br><span class="line">                <span class="keyword">if</span> (𝛼≥𝛽)      //剪枝, 降低搜索量</span><br><span class="line">                         <span class="keyword">break</span></span><br><span class="line">        返回𝑣</span><br></pre></td></tr></table></figure>

<h5 id="蒙特卡洛树搜索（MTCS）"><a href="#蒙特卡洛树搜索（MTCS）" class="headerlink" title="蒙特卡洛树搜索（MTCS）"></a>蒙特卡洛树搜索（MTCS）</h5><p>一种<strong>概率和启发式驱动</strong>的搜索算法，将经典的树搜索实现与<strong>强</strong>化学<strong>习的机器学习</strong>原理相结合</p>
<p>树搜索中存在当前最佳动作实际上不是最佳动作的可能性</p>
<p>“**探索-**开发权衡”策略：学习阶段通过定期评估其他备选方案，而不是当前感知的最佳策略</p>
<p><strong>探索扩展树的宽度</strong>，有助于确保 MCTS 不会忽略任何可能更好的路径，大量重复时变得低效</p>
<p><strong>开发扩展树的深度</strong>，坚持具有最大估计值的单一路径</p>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><ul>
<li>盲目搜索，没有利用问题定义本身之外的知识，而是根据事先对比好的某种固定排序，依次调用动作，以探求得到目标。</li>
<li>启发式搜索，利用问题定义本身之外的知识来引导搜索，主要通过访问启发函数来估计每个节点到目标点的代价或损耗</li>
<li>局部搜索，用在当前解的领域内来寻找更优解</li>
<li>对抗搜索，出现在多个智能体的对抗性博弈中，在其它智能体通过搜索寻找它们的最优解的情况下寻找最优策略</li>
</ul>
<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>本质是上根据数据中的例子学习<img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081606591.png" alt="image-20231208160606508" style="zoom:70%;" /></p>
<p>通过损失函数来评价结果的好坏</p>
<h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081609416.png" alt="image-20231208160930307"></p>
<p><strong>过拟合</strong>：函数表达能力强，使得模型在训练集上损失很少，但在测试集上损失很大泛化能力差</p>
<p><strong>欠拟合</strong>：函数表达能力弱，使得模型在训练集和测试集上损失都很大。</p>
<h3 id="无监督学习与半监督学习"><a href="#无监督学习与半监督学习" class="headerlink" title="无监督学习与半监督学习"></a>无监督学习与半监督学习</h3><h4 id="一个经典算法-k-means-算法"><a href="#一个经典算法-k-means-算法" class="headerlink" title="一个经典算法 k means 算法"></a>一个经典算法 k means 算法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">确定好𝑘</span><br><span class="line">随机选择𝑘 个中心</span><br><span class="line">将每个点与离它最近的中心相连（归属于此聚类）</span><br><span class="line">将每个聚类的点求平均，算作新的中心</span><br><span class="line">重复第<span class="number">3</span>&amp;<span class="number">4</span>步，直到收敛</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h4><p>邻接矩阵A：第(i,j) 位置存放的是i,j 之间的相似度</p>
<p>对角度数矩阵D：第(i,i)位置存放的是所有与i相连的边权和。非对角位置为0</p>
<p>拉普拉斯矩阵定义为 ： L &#x3D; D - A</p>
<p>优化和泛化的区别：优化是寻找损失函数最小的f的过程，泛化是使f 在没有见过的数据也有很好的表现能力。总的来说，优化是使f 在见过的数据表现更好，泛化是使f 在没有见过的数据上表现也很好</p>
<h2 id="线性回归方法"><a href="#线性回归方法" class="headerlink" title="线性回归方法"></a>线性回归方法</h2><p>特点：解释性强，简单，泛化能力稳定</p>
<p>应用领域：经济学，社会领域</p>
<h3 id="优化方式"><a href="#优化方式" class="headerlink" title="优化方式"></a>优化方式</h3><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>激活函数使用：Sigmoid 函数</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312061559329.png" alt="123" style="zoom:67%;" />g(z)&#x3D;1&#x2F;(1+e^(-z) )∈(0,1)</p>
<h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>激活函数使用： softmanx函数</p>
<p>y<del>i</del>&#x3D;e^(u_i)^&#x2F;(∑<del>(j&#x3D;1)</del>^k^e(u<del>j</del> ) )≥0</p>
<p>∑<del>(i&#x3D;1)</del>^k^y<del>i</del> &#x3D;1</p>
<h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>基本思想：针对普通的损失函数1&#x2F;2N ∑<del>i</del>(w^T x<del>i</del>-y<del>i</del> )^2^ ,希望在使它结果最小的同时，w^2^的大小也能够限制，即满足w^2^ &lt;&#x3D; C ,这样可以限制解的空间，帮助解决过拟合问题。（L2 正则，当超参数λ越大，约束越强）</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092110022.png" alt="image-20231209211023902"></p>
<h3 id="套索回归"><a href="#套索回归" class="headerlink" title="套索回归"></a>套索回归</h3><p>基本思想：在优化1&#x2F;2N ∑<del>i</del>(w^T x<del>i</del>-y<del>i</del> )^2^的同时，满足 |w| &lt;&#x3D; c,即希望解比较稀疏（因为现实中存在很多特征是无用的），其非0 项不超过c个，在实际中，往往使用 |w| &lt;&#x3D; c 作为替代品。（L1正则）</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092112081.png" alt="image-20231209211225034"></p>
<h3 id="支持向量机方法（SVM）"><a href="#支持向量机方法（SVM）" class="headerlink" title="支持向量机方法（SVM）"></a>支持向量机方法（SVM）</h3><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092116628.png" alt="image-20231209211600567" style="zoom: 33%;" />

<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092131513.png" alt="image-20231209213149466"> 当 a<del>i</del> 不等于 0 的数据才会被称为支持向量，这些数据点其实就是距离分割平面最近的那些点，所以比较少。支持向量机就相当于是由支持向量计算分隔方案的算法。</p>
<p> 支持向量机解的稀疏性: 训练完成后, 大部分的训练样本都不需保留, 最终模型仅与支持向量有关</p>
<p>对于数据不是线性可分的，可以把约束放松</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092141971.png" alt="image-20231209214152921"></p>
<p>对于<strong>不存在一个能正确划分两类样本的超平面</strong>的方案：</p>
<p><strong>将样本从原始空间映射到一个</strong>更高维的特征空间**,** <strong>使得样本在这个特征空间内线性可分</strong>.</p>
<p>如果最后算出有 r 个 a<del>i</del> ≠ 0 ，那么只需要计算 r 次核函数的内积操作，就可以判断出点 x 的类别。</p>
<h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>一个树结构的每个<strong>中间节点对数据的某一个特征进行判断</strong>，根据判断结果的不同指向相应的子节点。<strong>每一个叶子节点，代表的是对符合所有从根节点到该叶子节点路径上判断条件的数据给出的一个预测值</strong>。这种树称为决策树</p>
<h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><ol>
<li>初始化一个根节点，对应所有的训练数据</li>
<li>选择一个特征，设置一个分割条件</li>
<li>依据该条件构造根的两个叶子，每个叶子对应一部分数据</li>
<li>重复以上步骤，直到到达一定的终止条件</li>
</ol>
<h5 id="如何选择最优划分属性"><a href="#如何选择最优划分属性" class="headerlink" title="如何选择最优划分属性"></a>如何选择最优划分属性</h5><p>一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，结点的纯度越高越好</p>
<h5 id="属性划分方法"><a href="#属性划分方法" class="headerlink" title="属性划分方法"></a>属性划分方法</h5><h6 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h6><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051339414.png" alt="image-20231205133948308">信息增益越大，则意味着使用属性 a 来进行划分所获得的纯度提升越大</p>
<p>示例：</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051357754.png" alt="image-20231205135711675"><em><em>该数据集包含17个训练样本，</em>|y|</em><strong>&#x3D;2，其中正例占</strong>p**<del>1</del>&#x3D;8&#x2F;17 ，反例占<strong>p<del>2</del></strong>&#x3D;9&#x2F;17 ，计算得到根结点的信息熵为:<img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051357951.png" alt="image-20231205135749888"></p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051406814.png" alt="image-20231205140658747"></p>
<p>存在的问题：信息增益对可取数目较多的属性有所偏好（因为可取的数目越多，划分的就越细，子集合也越多，子集也就更纯，但这样的决策树不具有泛化能力）</p>
<h6 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h6><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051447591.png" alt="image-20231205144722527"></p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051447440.png" alt="image-20231205144734402"></p>
<p>IV(a) 称为属性 a 的“固有值” ，属性 a 的可能取值数目越多，那么 IV(a)通常越大</p>
<p>存在的问题：对可取值数目较少的属性有所偏好</p>
<h6 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h6><p>数据集D的纯度可用“基尼值”来度量</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051508825.png" alt="image-20231205150827763">Gini(D) 越小，数据集 D 的纯度越高</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051509017.png" alt="image-20231205150946971">选择使划分后基尼指数最小的属性作为最优划分属性</p>
<p>在ID<del>3</del>决策树中以信息增益作为准则来划分属性</p>
<p>在 C4.5 中使用的是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选取增益率最高的</p>
<p>CART采用“基尼指数”来选择划分属性（减少对数运算）</p>
<h4 id="过拟合处理方法：剪枝"><a href="#过拟合处理方法：剪枝" class="headerlink" title="过拟合处理方法：剪枝"></a>过拟合处理方法：剪枝</h4><h5 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h5><p>在决策树的训练过程中加入限制条件，避免违反这些条件的分割</p>
<h6 id="准则"><a href="#准则" class="headerlink" title="准则"></a>准则</h6><ul>
<li>限制树的最大深度</li>
<li>限制树的最大叶子数目</li>
<li>限制每片叶子最少的样本数</li>
<li>…</li>
</ul>
<h5 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h5><p>先训练一个规模足够大的决策树，然后再删去多余树的分支</p>
<h6 id="准则-1"><a href="#准则-1" class="headerlink" title="准则"></a>准则</h6><ul>
<li>该子树没能使验证集上的误差有所减少</li>
<li>该子树不包含有足够大分割增益的分割</li>
<li>…</li>
</ul>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>模型相对简单，具有较好的解释性，但是预测效果比不上更高级的模型</p>
<h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>集成学习思想：是集合一系列弱模型的预测结果，从而实现更稳定，表现更好的模型</p>
<h4 id="两种集成方式"><a href="#两种集成方式" class="headerlink" title="两种集成方式"></a>两种集成方式</h4><h5 id="平行的集成方式"><a href="#平行的集成方式" class="headerlink" title="平行的集成方式"></a>平行的集成方式</h5><p>引导聚集方法(Bagging（ bootstrap aggregating）) </p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312021934718.png" alt="image-20231202193428587"></p>
<h6 id="例子：随机森林"><a href="#例子：随机森林" class="headerlink" title="例子：随机森林"></a>例子：随机森林</h6><p>训练多个决策树，为避免在训练中多个决策树给出相同的预测，随机森林在训练每个决策树时会引入一定的随机性</p>
<p>特点：</p>
<ul>
<li>训练每个决策树时，随机选取部分训练数据进行训练（子树之间独立）</li>
<li>在每次分割叶子节点时，随机选取特征的一个子集，从该子集中选取最优的分割条件</li>
</ul>
<p>预测结果选择：</p>
<ul>
<li>回归问题：预测输出为所有决策树预测的<strong>均值</strong></li>
<li>分类问题：对所有决策树的预测类别进行投票，<strong>得票最高的类别</strong>作为输出结果</li>
</ul>
<h6 id="关于随机性的探讨"><a href="#关于随机性的探讨" class="headerlink" title="关于随机性的探讨"></a>关于随机性的探讨</h6><ul>
<li>特征&#x2F;数据集采样会降低单个决策树的效果，但这样能增加不同决策树之间的独立性与差异性</li>
<li>将许多这样的决策树以这种方式组合起来，通常能够得到比不引入随机性更好的单个决策树效果。差异较大的模型互相弥补了各自的短板</li>
</ul>
<p><strong>关于为什么要采取有放回的采样</strong></p>
<ul>
<li><strong>引入多样性：</strong> 允许同一样本在不同的子集中出现，这导致了每个决策树都是在略有不同的数据子集上训练的。这增加了每个决策树的多样性，有助于防止过拟合。如果使用无放回抽样，每个子集的样本都是独立的，可能导致每个决策树过度拟合于特定的子集。</li>
<li><strong>减小方差：</strong> 由于每个决策树的训练集都是通过有放回抽样生成的，因此每个树都是在略微不同的数据集上训练的。当多个决策树组成随机森林时，它们的预测会取平均值，从而减小了预测的方差，提高了整体模型的稳定性。</li>
<li><strong>处理大量特征：</strong> 随机森林通常应用于高维数据，有放回抽样可以使每个决策树使用不同的特征子集进行训练。这有助于每个树专注于不同的特征，提高了整体模型对于特征的利用效率。</li>
</ul>
<h5 id="串行的集成方式"><a href="#串行的集成方式" class="headerlink" title="串行的集成方式"></a>串行的集成方式</h5><p>提升算法 (Boosting)</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312021935533.png" alt="image-20231202193502462"></p>
<p>基础模型是一个一个训练的，后一个的训练依赖以前基础模型的训练结果</p>
<h6 id="例子：梯度提升决策树（GBDT）"><a href="#例子：梯度提升决策树（GBDT）" class="headerlink" title="例子：梯度提升决策树（GBDT）"></a>例子：梯度提升决策树（GBDT）</h6><p>基本思想：不断训练新的决策树，以弥补已经训练好的决策树的误差</p>
<p>和随机森林的区别：各个子模型之间存在更强的依赖关系</p>
<h6 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h6><ul>
<li>新子树拟合已有子模型的结果相对于数据标签的残差或负梯度，子树间不独立</li>
<li>多颗子树不断提升集成模型总体的效果</li>
<li>目前针对表格数据最有竞争力模型之一，使用非常广泛</li>
</ul>
<h4 id="理解集成学习的优势"><a href="#理解集成学习的优势" class="headerlink" title="理解集成学习的优势"></a>理解集成学习的优势</h4><ul>
<li>特征会存在冗余性</li>
<li>可以从多个视图挖掘信息</li>
<li>集成学习中的每个基础模型可以充分挖掘分给它的那部分特征，然后一起整合预测结果</li>
</ul>
<h4 id="bagging-bootstrap-aggregating-和boosting-的区别"><a href="#bagging-bootstrap-aggregating-和boosting-的区别" class="headerlink" title="bagging(bootstrap aggregating) 和boosting 的区别"></a>bagging(bootstrap aggregating) 和boosting 的区别</h4><ul>
<li>bagging是将训练样本从数据集中多次抽取，构建多个弱学习器，每个学习器给与的权重是一样的，而boosting 是在训练期间迭代构建强学习器，对于误差小的学习器给与更大的权重</li>
<li>bagging随机抽取多组样本集合，分别训练不同的模型，采取并行模式，多个模型同时运行，对于回归任务输出结果采取平均值，对于分类任务采取投票</li>
<li>boosting 使用多个模型，采用串行模式，每次迭代都对上一次的模型进行改进。boosting 会根据错误率不断调整样例的权重，错误率越大则权重越大</li>
</ul>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>选择relu 不选择sigmoid ：sigmoid 函数在远离0 点的时候导数非常小，影响优化,relu 的梯度十分好求，对于优化是十分方便的</p>
<h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>概念：用来评估一个句子或短语有多“像”是一个自然语言的工具。如果一个句子或短语更符合自然语言的表达方式，那么该句子或短语在语言模型的打分就应该更高；反之，则更低。</p>
<p>语言模型可以用到许多实用的场景，如文本纠错、翻译、语言生成等</p>
<h4 id="概率模型：n-gram-模型"><a href="#概率模型：n-gram-模型" class="headerlink" title="概率模型：n-gram 模型"></a>概率模型：n-gram 模型</h4><p>用字符序列代表句子，该字符序列的概率越大，则该序列更像自然语言。通常采用链式法则计算概率：P(c1c2c3)&#x3D;p(c1)*p(c2|c1)*p(c3|c1c2)</p>
<p>链式法则的优点：</p>
<ul>
<li>条件独立性</li>
<li>无后效性</li>
</ul>
<p>但这存在一个问题，对于很长的句子，越靠后的词的条件概率就会非常复杂，它对应的子序列在一个文章出现的概率也就越低，需要搜集更多的文章才能准确估计这个子序列的出现频率。所以一般采用n-gram 模型来解决这个问题。</p>
<p><strong>目的：简化条件概率计算</strong></p>
<p>近似假设 P(c<del>i</del>│c<del>1</del>…c<del>i-1</del> )≈P(c<del>i</del>|c<del>i-n+1</del>…c<del>i-1</del>)</p>
<p><strong>条件概率仅依赖前</strong>n-1<strong>个字符</strong></p>
<p><strong>对于固定</strong>n&#x3D;k<strong>，称之为</strong>k**-gram**模型</p>
<p>k&#x3D;1**: unigram model **    P(c<del>1</del>…c<del>N</del> )&#x3D;∏<del>i</del>P(c<del>i</del>)， 即每一个字相互独立，又称背包模型，bag-of-words）</p>
<p><strong>举例：</strong>P(清华大学)≈P(清)P(华)P(大)P(学)</p>
<p>k&#x3D;2**: Markov model** （马尔可夫模型）, bi-gram model</p>
<p>k<strong>越大，</strong>n**-gram**方法越精确</p>
<p><strong>词序列随着</strong>k指数变多，需要更多的语料才能准确估计概率</p>
<h4 id="最大似然估计MLE"><a href="#最大似然估计MLE" class="headerlink" title="最大似然估计MLE"></a>最大似然估计MLE</h4><p>用语料中的频率来近似概率。显然，搜集的语料越多，得到的概率就会越准确。从统计学角度可以证明，当语料有限时，是最“精准”的概率估计方式，这种方式就叫作最大似然估计。</p>
<h4 id="困惑度-preplexity"><a href="#困惑度-preplexity" class="headerlink" title="困惑度(preplexity)"></a>困惑度(preplexity)</h4><p>困惑度越低的语言模型生成语言的句子质量越高</p>
<p>PP(c<del>1</del>…c<del>N</del>)&#x3D;P(c<del>1</del>…c<del>N</del> )^(-1&#x2F;N)^</p>
<p>对数困惑度</p>
<p>log⁡PP(c<del>1</del>…c<del>N</del>)&#x3D;-1&#x2F;N ∑<del>i</del>log⁡P(c<del>i</del>|c<del>1</del>…c<del>i-1</del>) </p>
<h4 id="字模型和词模型"><a href="#字模型和词模型" class="headerlink" title="字模型和词模型"></a>字模型和词模型</h4><p><strong>字模型：</strong></p>
<ul>
<li><strong>常见汉字约2500</strong>字；建模与计算简单</li>
<li><strong>相对不精确，需要比较复杂的模型</strong></li>
</ul>
<p><strong>词模型：</strong></p>
<ul>
<li><strong>相对精确</strong></li>
<li><strong>词量巨大（海量专业名词），新词不断产生，需要分词</strong></li>
</ul>
<h4 id="中文和英文的差别"><a href="#中文和英文的差别" class="headerlink" title="中文和英文的差别"></a>中文和英文的差别</h4><p>英文更关注的是词性的区别，没有分词困扰</p>
<p>英文和中文的主要区别：</p>
<ul>
<li>英文中有大量的特定名词</li>
<li>英文中存在缩写与连写</li>
<li>英文中有不同的词性变换</li>
</ul>
<h3 id="向量语义"><a href="#向量语义" class="headerlink" title="向量语义"></a>向量语义</h3><h5 id="分布假设"><a href="#分布假设" class="headerlink" title="分布假设"></a>分布假设</h5><p><strong>单纯基于频率计算的模型忽略了词的语义信息</strong>。所以人们想到了分布假设。分布假设认为，两个词的词义越相似，那么它们在自然语言中出现的分布会越相近。以青菜白菜举例，二者都会经常出现在有关吃饭、菜谱等有关话题的句子中。</p>
<p>从分布假设角度来说，如果要表达一个词的意义，可以利用上下文的分布作为该词的特征。</p>
<h5 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h5><p>用向量（一些实数）来表达一个词的语义</p>
<p>近义词则对应向量空间距离近</p>
<h5 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h5><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081651753.png" alt="image-20231208165126665" style="zoom:50%;" />

<p>词义相近的词则夹角小（相似度大）</p>
<p>词义差距大的词则夹角大（相似度低）</p>
<h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>思路：采用机器学习方式，把词向量作为参数进行优化求解</p>
<h4 id="连续词袋模型CBOW"><a href="#连续词袋模型CBOW" class="headerlink" title="连续词袋模型CBOW"></a>连续词袋模型CBOW</h4><p>给定词的上下文，推断该词出现的概率</p>
<p>P(w│c<del>(-k)</del>…c<del>k</del> )&#x3D;∏<del>1≤|i|≤k</del> P(w|c<del>i</del>) 将联合分布分解为每一个上下文词C<del>i</del>与当前词w的条件概率的乘积。在这种独立假设的情况下，上下文中的每一个词的先后关系并不被模型考虑。</p>
<p>这种忽略词出现顺序只在乎是否出现的方法称为词袋模型。</p>
<p>对于CBOW来说，给定一个上下文，输出对于整个词表所有词的概率分布，我们希望对于候选词的概率要大一点。</p>
<p>余弦相似度<strong>：</strong>cos⁡&lt;w,c<del>i</del>&gt; ≈  w^T^ c<del>i</del></p>
<p>P(+│w,c<del>i</del> )：特定词适合和上下文一起出现的概率</p>
<p>P(-│w,c<del>i</del>): 其它词不适合和上下文一起出现的概率</p>
<p>P(-│w,c<del>i</del>)+P(+│w,c<del>i</del> )&#x3D;1</p>
<p>P(+│w,c<del>i</del> )&#x3D;σ(w^T^c<del>i</del> )&#x3D;1&#x2F;(1+e^ (-w^T^ c<del>i</del> ) )</p>
<p>P(-│w,c<del>i</del> )&#x3D;1-σ(w^T^ c<del>i</del>)&#x3D;e^ (-w^T^ c<del>i</del> ) &#x2F;(1+e^ (-w^T^ c<del>i</del> ) )</p>
<p>最后的结果：P(+│w,c<del>(-k)</del>…c<del>k</del> )&#x3D;∏_(1≤|i|≤k )P(+|w,c<del>i</del>)</p>
<p><strong>一般为了数值稳定，优化对数概率</strong></p>
<p>L(w,c)&#x3D;∑_((w,c<del>i</del> )∈D^+) log⁡P(+|w,c_i) </p>
<h4 id="跳字模型skip-gram"><a href="#跳字模型skip-gram" class="headerlink" title="跳字模型skip-gram"></a>跳字模型skip-gram</h4><p>给定一个词，推断其上下文词出现的概率，计算P(c<del>-k</del>…c<del>k</del>|w),也采用独立假设。</p>
<p>word2vec 只考虑得到的词向量，并不在意得到的结果，词向量是word2vec的参数。需要注意到，skip-gram 比CBOW 更不容易过拟合。</p>
<h3 id="基于神经网络的语言模型"><a href="#基于神经网络的语言模型" class="headerlink" title="基于神经网络的语言模型"></a>基于神经网络的语言模型</h3><p>能够在计算条件概率时将每个词的语义信息考虑进去。</p>
<p><strong>采用神经网络表示条件概率</strong>P(w|w<del>(i-1)</del>)</p>
<h4 id="基于神经网络的bigram模型"><a href="#基于神经网络的bigram模型" class="headerlink" title="基于神经网络的bigram模型"></a>基于神经网络的bigram模型</h4><p>找到一个函数，给定一个词表L&#x3D;{w^1^,w^2^,w^3^,…,w^n^},以及当前词w<del>i-1</del>作为输入，输出一个长度为 v 的向量 o &#x3D; (o<del>1</del>,o<del>2</del>,…,o<del>n</del>),其中o<del>i</del>,表示词表中第i 个词出现在w<del>i-1</del>之后的概率。</p>
<p> 损失函数</p>
<p>L(θ;w<del>(i-1)</del>,w<del>i</del> )&#x3D;-log⁡P(w<del>i</del> |w<del>(i-1)</del>)&#x3D;log y<del>wi</del>&#x3D;log⁡(∑<del>(k∈L)</del> exp⁡(β<del>k</del> ) )-β<del>wi</del></p>
<p><strong>为什么此时不需要构造负例进行训练了？</strong></p>
<p><strong>多分类问题的</strong>softmax<strong>输出自然包含了负例</strong></p>
<h4 id="基于神经网络的ngram-模型"><a href="#基于神经网络的ngram-模型" class="headerlink" title="基于神经网络的ngram 模型"></a>基于神经网络的ngram 模型</h4><p>与bigram 模型的不同之处在于输入的不在是一个词，而是n 个词拼接在一起，其余部分相同。<img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101653052.png" alt="image-20231210165312862" style="zoom:40%;" /></p>
<h4 id="基于LSTM-的语言模型"><a href="#基于LSTM-的语言模型" class="headerlink" title="基于LSTM 的语言模型"></a>基于LSTM 的语言模型</h4><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101656355.png" alt="image-20231210165625275"></p>
<h3 id="基于神经网络的机器翻译"><a href="#基于神经网络的机器翻译" class="headerlink" title="基于神经网络的机器翻译"></a>基于神经网络的机器翻译</h3><p><strong>翻译问题本质上寻找一个函数</strong>f<strong>，给定某种语言的句子</strong>X<strong>，输出另一种语言中对应的句子</strong></p>
<h4 id="基于LSTM的seq2seq模型"><a href="#基于LSTM的seq2seq模型" class="headerlink" title="基于LSTM的seq2seq模型"></a>基于LSTM的seq2seq模型</h4><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101659134.png" alt="image-20231210165922032"></p>
<p>编码器将输入序列 x 编码为一个特征向量 h ,再通过解码器将 h 转化为完整的目标句子 y </p>
<h4 id="beam-search"><a href="#beam-search" class="headerlink" title="beam search"></a>beam search</h4><p><strong>保留</strong>k<strong>个贪心候选序列</strong></p>
<p>对于每次时间步解码器的输出选择前k 个概率最大的词，然后作为下一个时间步的输入之一</p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101704128.png" alt="image-20231210170400980"></p>
<h4 id="基于注意力机制的seq2seq模型"><a href="#基于注意力机制的seq2seq模型" class="headerlink" title="基于注意力机制的seq2seq模型"></a>基于注意力机制的seq2seq模型</h4><p>LSTM模型的缺点：</p>
<ul>
<li><p>对于固定维度的句子，LSTM模型都只会将其映射到一个固定维度的特征向量 h ,但这个固定维度是模型的瓶颈，会限制模型的表达能力</p>
</li>
<li><p>输入词和输出词之间是有对应关系的，LSTM不能表达出这这关系</p>
</li>
</ul>
<p>加上注意力机制，输出状态会更加和它相似的输入状态，对于相似的注意力更高，特征向量是由自身对于编码器的不同时间步的输出的不同权重和得到的，所以每一个输出词都有自己对应的特征向量h</p>
<h4 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h4><p>对于输入序列的每一个词X<del>i</del>,都有对应的 k<del>i</del> , q<del>i</del> ,v<del>i</del> ,代表的意思是关键字，询问和值，使用这三个变量是为了提取出输入向量x<del>i</del> 中的不同信息。</p>
<p>k<del>i</del>,q<del>i</del>,v<del>i</del>&#x3D;[W<del>k</del> x<del>i</del>,W<del>q</del> x<del>i</del>,W<del>v</del> x<del>i</del>]</p>
<p>α<del>j</del>&#x3D;softmax(β<del>j</del>&#x2F;√d)     β<del>j</del>&#x3D;q<del>i</del>^T^ k<del>j</del></p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312111425410.png" alt="image-20231211142548287"></p>
<p>输出的是值向量的加权求和</p>
<h5 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h5><p><strong>计算</strong>m<strong>个维度为</strong>d&#x2F;m<strong>的独立自注意力分布和自注意力特征</strong></p>
<p>α<del>j</del>^l^&#x3D;softmax((β<del>j</del>^l^)&#x2F;√(d&#x2F;m)); β<del>j</del>^l^&#x3D;q<del>i</del>^l^^T  k<del>j</del>^l^;h<del>SA</del>^l^&#x3D;∑<del>j</del> α<del>j</del>^l^ v<del>j</del>^l^</p>
<p><strong>合并得到最终的维度为</strong>d<strong>的自注意力特征</strong>h_SA&#x3D;concat(h<del>SA</del>^1^,…,h<del>SA</del>^m^)</p>
<p><strong>多头注意力机制与原自注意力机制</strong>计算量相同</p>
<p><strong>每个独立头维度降低，表达能力相对减弱</strong></p>
<p><strong>保证整体不消耗额外计算</strong></p>
<p><strong>可以完整使用矩阵运算完成</strong></p>
<p><strong>自注意力机制本质上为线性运算</strong>，所以还需要添加非线性层。</p>
<h5 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h5><p><strong>自注意力机制有顺序不变性</strong>，所以需要添加位置编码来提供位置信息</p>
<p>p<del>i</del><strong>表示</strong>x<del>i</del><strong>所处的位置信息，以</strong>[x<del>i</del>,p<del>i</del>]<strong>作为自注意力模块的输入</strong></p>
<p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312111453789.png" alt="image-20231211145359645"></p>
<h3 id="语言模型预训练"><a href="#语言模型预训练" class="headerlink" title="语言模型预训练"></a>语言模型预训练</h3><h5 id="GPT-generative-pretained-Transformer-与BERT-bidirectional-encoder-representation-from-transformer-的区别和联系"><a href="#GPT-generative-pretained-Transformer-与BERT-bidirectional-encoder-representation-from-transformer-的区别和联系" class="headerlink" title="GPT (generative pretained Transformer )与BERT(bidirectional encoder representation from transformer)的区别和联系"></a>GPT (generative pretained Transformer )与BERT(bidirectional encoder representation from transformer)的区别和联系</h5><p>联系</p>
<ul>
<li>二者都使用了transformer架构</li>
<li>都采用预训练的策略，在大规模文本语料库上进行预训练，然后在特定任务上进行微调。</li>
</ul>
<p>区别</p>
<ul>
<li>GPT采用的是自回归的预训练模型。模型在预训练阶段通过预测下一个词的方式学习语言表示</li>
<li>bert 采用的是掩码语言模型的预训练目标。在输入序列中，随机掩盖一些词，并要求模型预测这些被掩盖的词</li>
<li>GPT采用的是单侧从左向右预测的方式，bert通过掩盖部分输入词，实现双向上下文理解，使得模型在预测缺失词时能够同时考虑左右两侧的信息</li>
<li>GPT是生成是模型，可以生成连续的文本序列，bert主要用于特征提取，通常在预训练后将其输出用于下游任务的任务特定模型</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://tjy531.com.cn">星星之火</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://tjy531.com.cn/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/">http://tjy531.com.cn/2023/11/27/人工智能初步学习/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://tjy531.com.cn" target="_blank">好好学习，天天向上</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a></div><div class="post_share"><div class="social-share" data-image="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/waibaowebsite.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/12/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2023/11/20/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/" title="图像分割论文学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">图像分割论文学习</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="waline-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/waibaowebsite.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">星星之火</div><div class="author-info__description">一万年太久，只争朝夕</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">26</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/TJY531"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/TJY531" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:3187937600@qq.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">努力提升自己</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">人工智能期末复习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E7%BB%AA%E8%AE%BA"><span class="toc-number">1.1.</span> <span class="toc-text">人工智能绪论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.1.</span> <span class="toc-text">人工智能定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="toc-number">1.1.2.</span> <span class="toc-text">人工智能发展历史</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.</span> <span class="toc-text">人工智能数学基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">1.2.1.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.2.</span> <span class="toc-text">概率论基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%9F%BA%E7%A1%80"><span class="toc-number">1.2.3.</span> <span class="toc-text">矩阵基础</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.</span> <span class="toc-text">搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E7%AD%96%E7%95%A5"><span class="toc-number">1.3.1.</span> <span class="toc-text">搜索策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B2%E7%9B%AE%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">盲目搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.1.1.</span> <span class="toc-text">深度优先搜索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%AE%BD%E5%BA%A6%E4%BC%98%E5%85%88%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.1.2.</span> <span class="toc-text">宽度优先搜索</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%90%AF%E5%8F%91%E5%BC%8F%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">启发式搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%B4%AA%E5%A9%AA%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.2.1.</span> <span class="toc-text">贪婪搜索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#A-%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.2.2.</span> <span class="toc-text">A^*^ 搜索</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B1%80%E9%83%A8%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.3.</span> <span class="toc-text">局部搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%88%AC%E5%B1%B1%E6%B3%95"><span class="toc-number">1.3.1.3.1.</span> <span class="toc-text">爬山法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB"><span class="toc-number">1.3.1.3.2.</span> <span class="toc-text">模拟退火</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.1.3.3.</span> <span class="toc-text">遗传算法</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%89%E7%A7%8D%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">1.3.1.3.4.</span> <span class="toc-text">三种算法对比</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%8A%97%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.4.</span> <span class="toc-text">对抗搜索</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%9E%81%E5%A4%A7%E6%9E%81%E5%B0%8F%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.4.1.</span> <span class="toc-text">极大极小搜索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#alpha-beta-%E5%89%AA%E6%9E%9D%E6%90%9C%E7%B4%A2"><span class="toc-number">1.3.1.4.2.</span> <span class="toc-text">alpha-beta 剪枝搜索</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%A0%91%E6%90%9C%E7%B4%A2%EF%BC%88MTCS%EF%BC%89"><span class="toc-number">1.3.1.4.3.</span> <span class="toc-text">蒙特卡洛树搜索（MTCS）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E6%AF%94"><span class="toc-number">1.3.1.5.</span> <span class="toc-text">对比</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.</span> <span class="toc-text">机器学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.1.</span> <span class="toc-text">监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">过拟合与欠拟合</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%8D%8A%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.2.</span> <span class="toc-text">无监督学习与半监督学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E4%B8%AA%E7%BB%8F%E5%85%B8%E7%AE%97%E6%B3%95-k-means-%E7%AE%97%E6%B3%95"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">一个经典算法 k means 算法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B0%B1%E8%81%9A%E7%B1%BB"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">谱聚类</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">线性回归方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F"><span class="toc-number">1.5.1.</span> <span class="toc-text">优化方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">梯度下降法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%88%86%E7%B1%BB"><span class="toc-number">1.5.2.</span> <span class="toc-text">二分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB"><span class="toc-number">1.5.3.</span> <span class="toc-text">多分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B2%AD%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.4.</span> <span class="toc-text">岭回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A5%97%E7%B4%A2%E5%9B%9E%E5%BD%92"><span class="toc-number">1.5.5.</span> <span class="toc-text">套索回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%96%B9%E6%B3%95%EF%BC%88SVM%EF%BC%89"><span class="toc-number">1.5.6.</span> <span class="toc-text">支持向量机方法（SVM）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.6.</span> <span class="toc-text">决策树模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.6.0.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="toc-number">1.6.0.2.</span> <span class="toc-text">训练过程</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%E6%9C%80%E4%BC%98%E5%88%92%E5%88%86%E5%B1%9E%E6%80%A7"><span class="toc-number">1.6.0.2.1.</span> <span class="toc-text">如何选择最优划分属性</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B1%9E%E6%80%A7%E5%88%92%E5%88%86%E6%96%B9%E6%B3%95"><span class="toc-number">1.6.0.2.2.</span> <span class="toc-text">属性划分方法</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A"><span class="toc-number">1.6.0.2.2.1.</span> <span class="toc-text">信息增益</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%A2%9E%E7%9B%8A%E7%8E%87"><span class="toc-number">1.6.0.2.2.2.</span> <span class="toc-text">增益率</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%9F%BA%E5%B0%BC%E6%8C%87%E6%95%B0"><span class="toc-number">1.6.0.2.2.3.</span> <span class="toc-text">基尼指数</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%EF%BC%9A%E5%89%AA%E6%9E%9D"><span class="toc-number">1.6.0.3.</span> <span class="toc-text">过拟合处理方法：剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%A2%84%E5%89%AA%E6%9E%9D"><span class="toc-number">1.6.0.3.1.</span> <span class="toc-text">预剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%86%E5%88%99"><span class="toc-number">1.6.0.3.1.1.</span> <span class="toc-text">准则</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%90%8E%E5%89%AA%E6%9E%9D"><span class="toc-number">1.6.0.3.2.</span> <span class="toc-text">后剪枝</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%87%86%E5%88%99-1"><span class="toc-number">1.6.0.3.2.1.</span> <span class="toc-text">准则</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">1.6.0.4.</span> <span class="toc-text">优缺点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.7.</span> <span class="toc-text">集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D%E9%9B%86%E6%88%90%E6%96%B9%E5%BC%8F"><span class="toc-number">1.7.0.1.</span> <span class="toc-text">两种集成方式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%B9%B3%E8%A1%8C%E7%9A%84%E9%9B%86%E6%88%90%E6%96%B9%E5%BC%8F"><span class="toc-number">1.7.0.1.1.</span> <span class="toc-text">平行的集成方式</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-number">1.7.0.1.1.1.</span> <span class="toc-text">例子：随机森林</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E%E9%9A%8F%E6%9C%BA%E6%80%A7%E7%9A%84%E6%8E%A2%E8%AE%A8"><span class="toc-number">1.7.0.1.1.2.</span> <span class="toc-text">关于随机性的探讨</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%B8%B2%E8%A1%8C%E7%9A%84%E9%9B%86%E6%88%90%E6%96%B9%E5%BC%8F"><span class="toc-number">1.7.0.1.2.</span> <span class="toc-text">串行的集成方式</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90%EF%BC%9A%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%88GBDT%EF%BC%89"><span class="toc-number">1.7.0.1.2.1.</span> <span class="toc-text">例子：梯度提升决策树（GBDT）</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-number">1.7.0.1.2.2.</span> <span class="toc-text">特点</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-number">1.7.0.2.</span> <span class="toc-text">理解集成学习的优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bagging-bootstrap-aggregating-%E5%92%8Cboosting-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.7.0.3.</span> <span class="toc-text">bagging(bootstrap aggregating) 和boosting 的区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">1.8.</span> <span class="toc-text">神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89"><span class="toc-number">1.9.</span> <span class="toc-text">计算机视觉</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86"><span class="toc-number">1.10.</span> <span class="toc-text">自然语言处理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.1.</span> <span class="toc-text">语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B%EF%BC%9An-gram-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.1.1.</span> <span class="toc-text">概率模型：n-gram 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1MLE"><span class="toc-number">1.10.1.2.</span> <span class="toc-text">最大似然估计MLE</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%B0%E6%83%91%E5%BA%A6-preplexity"><span class="toc-number">1.10.1.3.</span> <span class="toc-text">困惑度(preplexity)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AD%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%8D%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.1.4.</span> <span class="toc-text">字模型和词模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%AD%E6%96%87%E5%92%8C%E8%8B%B1%E6%96%87%E7%9A%84%E5%B7%AE%E5%88%AB"><span class="toc-number">1.10.1.5.</span> <span class="toc-text">中文和英文的差别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E8%AF%AD%E4%B9%89"><span class="toc-number">1.10.2.</span> <span class="toc-text">向量语义</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%81%87%E8%AE%BE"><span class="toc-number">1.10.2.0.1.</span> <span class="toc-text">分布假设</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E8%AF%8D%E5%90%91%E9%87%8F"><span class="toc-number">1.10.2.0.2.</span> <span class="toc-text">词向量</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="toc-number">1.10.2.0.3.</span> <span class="toc-text">相似度</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#word2vec"><span class="toc-number">1.10.3.</span> <span class="toc-text">word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BF%9E%E7%BB%AD%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8BCBOW"><span class="toc-number">1.10.3.1.</span> <span class="toc-text">连续词袋模型CBOW</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B7%B3%E5%AD%97%E6%A8%A1%E5%9E%8Bskip-gram"><span class="toc-number">1.10.3.2.</span> <span class="toc-text">跳字模型skip-gram</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.4.</span> <span class="toc-text">基于神经网络的语言模型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84bigram%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.4.1.</span> <span class="toc-text">基于神经网络的bigram模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84ngram-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.4.2.</span> <span class="toc-text">基于神经网络的ngram 模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8ELSTM-%E7%9A%84%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.4.3.</span> <span class="toc-text">基于LSTM 的语言模型</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91"><span class="toc-number">1.10.5.</span> <span class="toc-text">基于神经网络的机器翻译</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8ELSTM%E7%9A%84seq2seq%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.5.1.</span> <span class="toc-text">基于LSTM的seq2seq模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#beam-search"><span class="toc-number">1.10.5.2.</span> <span class="toc-text">beam search</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84seq2seq%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.5.3.</span> <span class="toc-text">基于注意力机制的seq2seq模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transformer-%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.10.5.4.</span> <span class="toc-text">Transformer 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-number">1.10.5.4.1.</span> <span class="toc-text">多头注意力机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.10.5.4.2.</span> <span class="toc-text">位置编码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-number">1.10.6.</span> <span class="toc-text">语言模型预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#GPT-generative-pretained-Transformer-%E4%B8%8EBERT-bidirectional-encoder-representation-from-transformer-%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB"><span class="toc-number">1.10.6.0.1.</span> <span class="toc-text">GPT (generative pretained Transformer )与BERT(bidirectional encoder representation from transformer)的区别和联系</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/06/Conv2Former%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/" title="Conv2Former阅读记录">Conv2Former阅读记录</a><time datetime="2025-01-06T12:04:59.000Z" title="发表于 2025-01-06 20:04:59">2025-01-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/01/05/%E5%88%9D%E6%AD%A5%E6%8E%A5%E8%A7%A6%E6%95%99%E5%91%98/" title="初步接触教员">初步接触教员</a><time datetime="2025-01-05T14:21:33.000Z" title="发表于 2025-01-05 22:21:33">2025-01-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/02/05/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" title="多标签图像分类评价指标">多标签图像分类评价指标</a><time datetime="2024-02-05T06:13:09.000Z" title="发表于 2024-02-05 14:13:09">2024-02-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/" title="无题">无题</a><time datetime="2023-12-31T03:23:31.781Z" title="发表于 2023-12-31 11:23:31">2023-12-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/" title="人工智能初步学习">人工智能初步学习</a><time datetime="2023-11-27T05:52:37.000Z" title="发表于 2023-11-27 13:52:37">2023-11-27</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 星星之火</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>function loadWaline () {
  function initWaline () {
    const waline = Waline.init(Object.assign({
      el: '#waline-wrap',
      serverURL: 'https://comment.tjy531.com.cn/',
      pageview: false,
      dark: 'html[data-theme="dark"]',
      path: window.location.pathname,
      comment: true,
    }, {"requiredMeta":["nick","mail"],"locale":{"placeholder":"昵称和邮箱为必填项，为了您能及时收到相关信息，请确保邮箱的正确性"}}))
  }

  const walineCSSLoad = document.getElementById('waline-css')

  if (typeof Waline === 'object') {
    walineCSSLoad ? initWaline() : getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css','waline-css').then(initWaline)
  }
  else {
    getCSS('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.css','waline-css').then(() => {
      getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(initWaline)
    })
  }
}

if ('Waline' === 'Waline' || !false) {
  if (false) btf.loadComment(document.getElementById('waline-wrap'),loadWaline)
  else setTimeout(loadWaline, 0)
} else {
  function loadOtherComment () {
    loadWaline()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/loadingwebsite.gif" data-original="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }

        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const getComment = () => {
    const loadWaline = () => {
      Waline.RecentComments({
        serverURL: 'https://comment.tjy531.com.cn/',
        count: 6
      }).then(({comments}) => {
        const walineArray = comments.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.objectId,
            'date': e.insertedAt,
          }
        })
        saveToLocal.set('waline-newest-comments', JSON.stringify(walineArray), 10/(60*24))
        generateHtml(walineArray)
      }).catch(e => {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.textContent= "无法获取评论，请确认相关配置是否正确"
      }) 
    }

    if (typeof Waline === 'object') loadWaline()
    else getScript('https://cdn.jsdelivr.net/npm/@waline/client/dist/waline.min.js').then(loadWaline)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('waline-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="true" data-text="study?,study!,ok" data-fontsize="10px" data-random="true" async="async"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>
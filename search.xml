<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Conv2Former阅读记录</title>
      <link href="/2025/01/06/Conv2Former%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
      <url>/2025/01/06/Conv2Former%E9%98%85%E8%AF%BB%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h1 id="Conv2Former阅读记录"><a href="#Conv2Former阅读记录" class="headerlink" title="Conv2Former阅读记录"></a>Conv2Former阅读记录</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>        simplify the self-attention by leveraging a convolutional modulation operation. 通过卷积调制操作简化自注意力机制。</p><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><p>        Models like ResNet mostly aggregate responses with large receptive fields by stacking multiple building blocks and adopting the pyamid network architecture but <strong>neglect the importance of explicitly modeling the global contextual information</strong>.像ResNet 这样的模型大多通过堆叠多个构建块和采取金字塔结构来获得大的感受野，但是这会忽略一个重要点：直接获得全局语境信息 。</p><p>        SENet <strong>introduce attention-based mechanisms into CNNs to capture long-range dependencies</strong> ,attaining surprisingly good performance.SENet 在CNNs中引入注意力机制来捕获长距离依赖，获得了令人惊讶的性能。</p><p>        The self-attention mechanism in Transformers is able to model global pairwise dependencies,providing a more efficient way to encode spatial information. Nevertheless, the computational cost caused by the self-attention when processing high-resolution images is considerable.自注意力所带来的计算开销是相当大的。</p><h3 id="卷积调制"><a href="#卷积调制" class="headerlink" title="卷积调制"></a>卷积调制</h3><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202501062209959.png" title="" alt="loading-ag-147" width="461"><p>        As shown in the left part of the above picture,self-attention computes the output of each pixel by a weighted summation of all other positions.This process can also be mimicked by computing the Hadamard proudct between the output of a large-kernel convolution and value representations, which we call convolutional modulation as depicted in the right part of the above picture.自注意力每个像素对应的输出来自其它所有位置像素的加权求和，这个过程也可以通过计算大卷积核的哈达玛积来模仿和表达，这就是卷积调制。</p><p>        The difference is that the convolution kernels are static while the attention matrix generated by self-attention can adapt to the input content.不同之处在于卷积调制的卷积核是静态的，而自注意力所产生的信息与输入内容有关。</p><p>        Simply replacing the self-attention in ViTs with the proposed convolutional modulation operation yields the porposed network,termed Conv2Former. 用卷积调制操作代替ViTs中的自注意力所产生的网络称为Conv2Former。</p><p>        Another main contribution of this paper is that we show Conv2Former can benefit more from convolutions with larger kernels, like 11×11 and 21×21. It alse show that the method using 11×11 depthwise convolutions performs even better than the recent works using super large kernel convolutions.论文另一个主要的贡献就是Conv2Former 能够随着卷积核的增大持续获得更多的收益。</p><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202501072037045.png" alt="loading-ag-142"></p>]]></content>
      
      
      <categories>
          
          <category> 图像分割 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初步接触教员</title>
      <link href="/2025/01/05/%E5%88%9D%E6%AD%A5%E6%8E%A5%E8%A7%A6%E6%95%99%E5%91%98/"/>
      <url>/2025/01/05/%E5%88%9D%E6%AD%A5%E6%8E%A5%E8%A7%A6%E6%95%99%E5%91%98/</url>
      
        <content type="html"><![CDATA[<p>       2024年以前，我不知什么是资本主义，不知什么是共产主义，不知道社会主义国家和资本主义国家的区别，也很少关注国际上的事情，是个跟随网络舆论，容易听信他人的人。  </p><p>        在2024年时，大概是内心急需一个方向，一个思想来填充自己贫瘠的精神世界，意外地接触到许多宣传赞美教员的内容，自己也就开始好奇教员这个人了，当时只买了两本毛选，想着看看到底有什么魔力，值得这么多人崇敬他。</p><p>        初读毛选第一卷的《中国社会各阶级的分析》时，我被教员通俗易懂，条理清晰的文字所折服，他所写的，很多都是我从未想到过的。我惊叹于他能清楚地写出不同阶级之间的区别和联系，直击要害。“谁是我们的敌人？谁是我们的朋友？这个问题是革命的首要问题。”这句话是大众所熟知的，我佩服他能开门见山地指出这个关键问题，他这句话，在我看来是十分有道理的，要团结真正的朋友，攻击真正得到敌人，这也是能运用到如今的学习或生活上的。这篇文章我读了三次，却依然不是大懂，我认为是自身没有在社会中深入实践，不能体会究竟哪种人属于哪种阶级。</p><p>        后来又读了大名鼎鼎的《矛盾论》和《实践论》，自此开始了我对于教员的崇拜，我开始感觉，自己好像认识了这个世界，我仿佛发现了新大陆，我乐于沉浸书中的内容，这是从前的我不曾有的。每每阅读，我的内心会不由自主地赞叹：“对啊，真是这样的，我为什么没有从这个角度想过。”我时常觉得自己的思考的过于浅薄，不能真正理解他所要表达的含义。但是没关系，我从他的文字中，了解到了他。</p><p>        教员的文字十分有感染力，我很爱他的一些诗词，譬如“待到山花烂漫时，她在丛中笑。”、“埋骨何须桑梓地，人生无处不青山。”、“俱往矣，数风流人物，还看今朝。”、“世上无难事，只要肯攀登。”、“天若有情天亦老，人间正道是沧桑。”、“为有牺牲多壮志，敢教日月换新天。”中华儿女多奇智，不爱红装爱武装。“。</p><p>        朋友们说我对于毛主席有些过于痴迷，就像是他的唯粉一样。我不置可否，因为他是真的让人全身心佩服。他是我的偶像，是我要认真学习的榜样。我以为世人皆虚伪，但他是如此纯粹，今年是他131年周年纪念日，我和朋友有幸前往韶山，途中所遇到的大部分是中老年人，也有部分和我一样的小年轻，我们都有一个共同点，就是胸前都佩戴了毛主席的勋章，都是打心底里敬爱毛主席。</p><p>        今后也是要多读教员的书籍，他的文字，就是我们生活中的舵手，指引方向。</p>]]></content>
      
      
      <categories>
          
          <category> 思想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 共产主义 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多标签图像分类评价指标</title>
      <link href="/2024/02/05/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/"/>
      <url>/2024/02/05/%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h1 id="多标签图像分类评价指标"><a href="#多标签图像分类评价指标" class="headerlink" title="多标签图像分类评价指标"></a>多标签图像分类评价指标</h1><ol><li><h2 id="mAP-mean-average-precision平均准确率均值"><a href="#mAP-mean-average-precision平均准确率均值" class="headerlink" title="mAP(mean average precision平均准确率均值)"></a>mAP(mean average precision平均准确率均值)</h2><p>mAP 是取所有标签计算得到的AP的平均，AP(Averge Precision ) 就是计算PR 曲线的面积，而PR(Precision Recall) 曲线就是纵坐标是Precision, 横坐标是 Recall 所构成的曲线。</p><p>Precision 和 Recall 分子相同，都是预测正确的正样本（TP）其计算公式分别为</p><p>$Precision &#x3D; \frac{TP}{TP+FP}$   （查准率）</p><p>预测正确的正样本数量占所有预测为正样本的比值。 当 Precision 为 0 时，可能意味着预测的正样本都是错误的，当 Precision为 1 时，意味着预测的正样本都是正确的，但不代表所有的正样本都被预测出来，FP 为 0。</p><p>$Recall &#x3D; \frac{TP}{TP+FN}$  （查全率）</p><p>预测正确的正样本数量占所有真实正样本数量的比值 。当 Recall 为1时，所有的正样本都被预测出来，但也可能是因为模型预测的样本全为 正样本，这样 FN 就为0 ，但这种模型是没有用的 。</p><p>TP(True Positive):预测为真，实际为真</p><p>TN(True Negative):预测为假，实际为假</p><p>FP(False Positive): 预测为真，实际为假</p><p>FN(False Negative):预测为假，实际为真</p></li><li><p>CP CR CF1 OP OR OF1</p><ol><li>CP</li><li>CR</li><li>CF1</li><li>OP</li><li>OR</li><li>OF1</li></ol></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2023/12/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/"/>
      <url>/2023/12/31/%E8%AE%A1%E7%AE%97%E6%9C%BA%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h1 id="第一章-计算机系统结构的基本概念"><a href="#第一章-计算机系统结构的基本概念" class="headerlink" title="第一章 计算机系统结构的基本概念"></a>第一章 计算机系统结构的基本概念</h1><h2 id="考点1-1：计算机系统结构的概念，特别是层次结构。"><a href="#考点1-1：计算机系统结构的概念，特别是层次结构。" class="headerlink" title="考点1.1：计算机系统结构的概念，特别是层次结构。"></a>考点1.1：计算机系统结构的概念，特别是层次结构。</h2><p>计算机系统结构概念的实质是确定计算机系统中软、硬件的界面，界面之上是软件实现的功能，界面之下是硬件和固件实现的功能。</p><p>从计算机语言的角度</p><table><thead><tr><th>L6应用语言虚拟机</th><th>计算机满足某种用途专门设计，程序由应用程序包翻译到L5</th></tr></thead><tbody><tr><td>L5 高级语言虚拟机</td><td>C&#x2F;C++等，程序由编译器翻译到L4或L3上</td></tr><tr><td>L4汇编语言虚拟机</td><td>程序由汇编程序翻译成L2和L3上的语言，再由相应计算机实现</td></tr><tr><td>L3 操作系统虚拟机</td><td>指令集由传统机器级指令和操作系统级指令组成，程序在L2和L3解释执行</td></tr><tr><td>L2 传统机器级</td><td>程序在L1的微程序上解释执行，由微程序解释指令集的过程又称为仿真</td></tr><tr><td>L1 微程序机器级</td><td>计算机硬件设计人员编写微指令由硬件解释实现</td></tr></tbody></table><p><strong>L1-L3是用解释的方式实现，L4-L6则是用翻译 的方式实现</strong></p><h3 id="翻译和解释的区别"><a href="#翻译和解释的区别" class="headerlink" title="翻译和解释的区别"></a>翻译和解释的区别</h3><ul><li>翻译是指把L(i+1)程序全部转化为L(i)程序，然后去执行新产生的L(i)程序，L(i+1)程序不会再被访问</li><li>解释是每当一条L(i+1)指令被译码后，就去执行一串等效的L(i)指令，然后再去取下一条L(i+1)指令，重复执行</li><li>解释执行比执行编译后生成的代码所花的时间多，但占用的存储空间较少</li></ul><h2 id="考点1-2：计算机系统结构的发展，冯诺依曼结构，软件、器件等对系统结构的影响"><a href="#考点1-2：计算机系统结构的发展，冯诺依曼结构，软件、器件等对系统结构的影响" class="headerlink" title="考点1.2：计算机系统结构的发展，冯诺依曼结构，软件、器件等对系统结构的影响"></a>考点1.2：计算机系统结构的发展，冯诺依曼结构，软件、器件等对系统结构的影响</h2><h3 id="冯诺依曼结构是什么和特点"><a href="#冯诺依曼结构是什么和特点" class="headerlink" title="冯诺依曼结构是什么和特点"></a>冯诺依曼结构是什么和特点</h3><p>组成: 运算器、存储器、控制器、输入设备、输出设备</p><p>特点：（重点）</p><ol><li>计算机以运算器为中心</li><li>在存储器中，指令和数据同等对待</li><li>存储器是按地址访问、按顺序线性编址的一维结构，单元位数固定</li><li>指令的执行顺序是顺序的，按照在存储器中存放的顺序执行</li><li>指令由操作码和地址码组成 </li><li>指令和数据均以二进制编码表示，采用二进制运算</li></ol><h3 id="软件对系统结构的影响"><a href="#软件对系统结构的影响" class="headerlink" title="软件对系统结构的影响"></a>软件对系统结构的影响</h3><p>对于新型号的计算机，用户希望以前购买的软件依旧可以被使用，这需要新的计算机有可移植性。实现可移植性的三种方式：</p><ul><li>系列机</li><li>模拟和仿真</li><li>统一高级语言</li></ul><h3 id="器件发展对系统结构的影响"><a href="#器件发展对系统结构的影响" class="headerlink" title="器件发展对系统结构的影响"></a>器件发展对系统结构的影响</h3><p>VLSI 技术的发展速度很快，早期由于芯片价格昂贵和集成度的限制，许多高性能系统结构无法实现，而现在则不仅能够在高性能计算机中实现，而且也能应用到个人计算机中</p><table><thead><tr><th>分代</th><th>器件特征</th><th>系统结构特征</th><th>软件技术特征</th></tr></thead><tbody><tr><td>第一代</td><td>电子管和继电器</td><td>存储程序计算机程序控制I&#x2F;O</td><td>机器语言 汇编语言</td></tr><tr><td>第二代</td><td>晶体管、磁芯印刷电路</td><td>浮点数据表示 寻址技术 中断、I&#x2F;O处理机</td><td>高级语言和编译批处理监控系统</td></tr><tr><td>第三代</td><td>SSI MSI 多层印刷电路微程序</td><td>流水线、Cache 先行处理 系列机</td><td>多道程序 分时操作系统</td></tr><tr><td>第四代</td><td>LSI VLSI 半导体存储器</td><td>向量分布 分布式存储器</td><td>并行与分布处理</td></tr><tr><td>第五代</td><td>高性能微处理器                 高密度电路</td><td>对称式多处理机</td><td>大规模、可扩展并行与分布处理</td></tr></tbody></table><h2 id="考点1-3：计算机系统结构的分类：特别是冯式分类法和费林分类法"><a href="#考点1-3：计算机系统结构的分类：特别是冯式分类法和费林分类法" class="headerlink" title="考点1.3：计算机系统结构的分类：特别是冯式分类法和费林分类法"></a>考点1.3：计算机系统结构的分类：特别是冯式分类法和费林分类法</h2><h3 id="1-3计算机系统结构的Flynn分类法是按什么分类的？共分为哪几类？"><a href="#1-3计算机系统结构的Flynn分类法是按什么分类的？共分为哪几类？" class="headerlink" title="1.3计算机系统结构的Flynn分类法是按什么分类的？共分为哪几类？"></a><strong>1.3计算机系统结构的Flynn分类法是按什么分类的？共分为哪几类？</strong></h3><p>按照<strong>指令流和数据流的多倍性</strong>进行分类的，分为单指令单数据流SISD，单指令多数据流SIMD、多指令单数据流MISD、多指令多数据流MIMD</p><p>补充：冯氏分类法</p><p>按<strong>系统最大并行度</strong>分类</p><ul><li>字串位串WSBS</li><li>字并位串WPBS</li><li>字串位并WSBP</li><li>字并位并WPBP</li></ul><h2 id="考点1-4：并行性的概念以及提高并行性的三种方法。"><a href="#考点1-4：并行性的概念以及提高并行性的三种方法。" class="headerlink" title="考点1.4：并行性的概念以及提高并行性的三种方法。"></a>考点1.4：并行性的概念以及<strong>提高并行性</strong>的三种方法。</h2><p>概念：是指计算机系统在同一时刻或同一时间间隔内进行多种运算或操作。包括<strong>同时性和并发性</strong>。时间上存在相互重叠就存在并行性。</p><p>实现并行的一个途径是在组成上引入<strong>并行和重叠</strong>技术，实现并行主存系统。</p><p><strong>并行性的不同等级：</strong></p><p>处理数据角度上：</p><ol><li>字串位串</li><li>字串位并</li><li>字并位串</li><li>字并位并</li></ol><p>执行程序角度上</p><ol><li>指令内部并行</li><li>指令级并行</li><li>线程级并行</li><li>任务级并行</li><li>作业级并行</li></ol><h3 id="提高并行性的技术途径"><a href="#提高并行性的技术途径" class="headerlink" title="提高并行性的技术途径"></a>提高并行性的技术途径</h3><ol><li>时间重叠</li><li>资源重复</li><li>资源共享</li></ol><p>在发展高性能单处理机的过程中，起主导作用的是时间重叠原理</p><h2 id="考点1-5：定量分析技术，要掌握如何计算评估计算机的性能。"><a href="#考点1-5：定量分析技术，要掌握如何计算评估计算机的性能。" class="headerlink" title="考点1.5：定量分析技术，要掌握如何计算评估计算机的性能。"></a>考点1.5：定量分析技术，要掌握如何计算评估计算机的性能。</h2><h3 id="定量分析技术"><a href="#定量分析技术" class="headerlink" title="定量分析技术"></a>定量分析技术</h3><h3 id="计算机系统设计的原则和定量原理"><a href="#计算机系统设计的原则和定量原理" class="headerlink" title="计算机系统设计的原则和定量原理"></a>计算机系统设计的原则和定量原理</h3><ul><li>以经常性事件为重点</li><li>Amdahl定律</li><li>CPU性能公式</li><li>程序的局部性原理（时间和空间局部性）</li></ul><h3 id="计算机系统的性能评测"><a href="#计算机系统的性能评测" class="headerlink" title="计算机系统的性能评测"></a>计算机系统的性能评测</h3><ul><li><p>执行时间和吞吐量</p></li><li><p>基准程序</p><ul><li>基准程序用于测试和比较性能</li></ul></li><li><p>性能比较</p><ul><li>总执行时间，可以直接用计算机执行所有测试程序的总时间来比较各不同机器的性能</li><li>加权执行时间</li></ul></li></ul><h3 id="Amdahl定理"><a href="#Amdahl定理" class="headerlink" title="Amdahl定理"></a>Amdahl定理</h3><p>内容：加快某部件执行速度所能获得的系统性能加速比，受限制该部件的执行时间占系统中总执行时间的百分比。如果仅对计算任务中的一部分进行性能改进，则改进得越多所得到的总体性能的提升就越有限。</p><h4 id="加速比定义（重点）"><a href="#加速比定义（重点）" class="headerlink" title="加速比定义（重点）"></a>加速比定义（重点）</h4><p>$加速比&#x3D;\frac{1}{(1-可改进比例)+\frac{可改进比例}{部件加速比}}$</p><h4 id="CPU性能公式（重点）"><a href="#CPU性能公式（重点）" class="headerlink" title="CPU性能公式（重点）"></a>CPU性能公式（重点）</h4><p>CPU时间&#x3D;IC×CPI ×时钟周期时间</p><p>IC：指令个数</p><p>CPI：平均指令周期数                                                                   </p><h1 id="作业"><a href="#作业" class="headerlink" title="作业"></a>作业</h1><p><strong>1.2说明计算机系统结构、计算机组成与计算机实现之间的相互关系</strong></p><p>设计主存系统时，确定<strong>主存容量、编址方式、寻址范围</strong>等属于计算机系统结构范围，确定<strong>主存周期、逻辑上是否采用并行主存、逻辑设计</strong>等属于计算机组成范围，而对于<strong>旋转存储芯片类型、微组装技术、线路设计</strong>等属于计算机实现范围</p><p>计算机组成是计算机系统结构的逻辑实现。计算机实现是计算机组成的物理实现。一种系统结构可以有多种组成。一种组成可以有多种实现</p><p><strong>1.3计算机系统结构的Flynn分类法是按什么分类的？共分为哪几类？</strong></p><p>按照<strong>指令流和数据流的多倍性</strong>进行分类的，分为单指令单数据流SISD，单指令多数据流SIMD、多指令单数据流MISD、多指令多数据流MIMD</p><p><strong>1.4计算机系统设计中经常使用的设计原则和定量原理是什么？说出它们的含义</strong>（重点）</p><ol><li>以经常事件为重点，对于经常发生的情况，赋予它优先的处理权和资源使用权</li><li>Amdahl定律，加快某部件执行速度所获得的系统性能加速比，受限于该部件的执行时间占系统总时间的百分比</li><li>CPU性能公式</li><li>程序局部性原理，程序在执行时所访问地址的分布不是随机的，而是相对簇聚</li></ol><p><strong>1.5从执行程序的角度和处理数据的角度来看，计算机系统中的并行性等级从低到高可分为哪几级？</strong></p><p>字串位串（不存在并行性）、字串位并（开始出现并行性）、字并位串（较高的并行性）、全并行（最高的并行）</p><p>指令级内部并行、指令集并行、线程级并行、任务级并行、作业级并行</p><p>1.6、1.7、1.8（重点）</p><h1 id="第二章-计算机指令集结构"><a href="#第二章-计算机指令集结构" class="headerlink" title="第二章  计算机指令集结构"></a>第二章  计算机指令集结构</h1><p>指令集的基本要求：完整性、规整性、高效率和兼容性</p><h2 id="指令集结构"><a href="#指令集结构" class="headerlink" title="指令集结构"></a>指令集结构</h2><p>堆栈结构、累加器结构、通用寄存器结构（根据操作数的来源不同分为寄存器-存储器结构，寄存器-寄存器结构）</p><p>通用寄存器的优点：</p><ol><li>寄存器的访问速度比存储器快</li><li>对于编译器而言，能更加容易、有效地分配和使用寄存器</li><li>寄存器可以用来存放变量</li></ol><h2 id="考点2-1：指令集结构的功能设计：CISC和RISC的各自特点及优缺点。"><a href="#考点2-1：指令集结构的功能设计：CISC和RISC的各自特点及优缺点。" class="headerlink" title="考点2.1：指令集结构的功能设计：CISC和RISC的各自特点及优缺点。"></a>考点2.1：指令集结构的功能设计：CISC和RISC的各自特点及优缺点。</h2><h3 id="RISC和CISC的特点和区别"><a href="#RISC和CISC的特点和区别" class="headerlink" title="RISC和CISC的特点和区别"></a>RISC和CISC的特点和区别</h3><ul><li>CISC的策略是增强指令功能，把越来越多的功能交由硬件来实现，指令的数量也是越来越多</li><li>RISC的策略则是尽可能地把指令集简化、不仅指令的条数少，而且指令的功能也比较简单</li></ul><table><thead><tr><th>比较内容</th><th>CISC</th><th>RISC</th></tr></thead><tbody><tr><td>指令格式</td><td>变长编码</td><td>定长编码</td></tr><tr><td>寻址方式</td><td>各种都有</td><td>只有load&#x2F;store指令可以访存</td></tr><tr><td>CPI</td><td>远远大于1</td><td>为1</td></tr></tbody></table><p>CISC指令集结构的功能设计</p><ol><li>面向目标程序增强指令功能</li><li>面向高级语言的优化实现来改进指令集     </li><li>面向操作系统的优化实现改进指令集</li></ol><p>2.6 CISC的缺点</p><ol><li>各种指令的使用频率相差悬殊</li><li>指令集的复杂性使得计算机系统结构变得复杂，增加了研制时间和成本，还容易造成设计错误</li><li>不利于单片集成</li><li>运行速度慢</li><li>不利于采用流水技术来提高系统的性能</li></ol><p><strong>2.7简述RISC指令集结构的设计原则</strong></p><ol><li>选取使用频率最高的指令，并补充一些最有用的指令</li><li>每条指令功能应尽可能简单，并在一个机器周期内完成</li><li>所有指令长度均相同</li><li>只有load和store操作指令才可以访问存储器</li><li>以简单、有效的方式支持高级语言</li></ol><h2 id="考点2-2：指令系统的设计包括哪些？"><a href="#考点2-2：指令系统的设计包括哪些？" class="headerlink" title="考点2.2：指令系统的设计包括哪些？"></a>考点2.2：指令系统的设计包括哪些？</h2><p><strong>2.5指令集结构设计所涉及的内容有哪些？</strong></p><ol><li><strong>指令集功能设计。分为RISC 和CISC</strong></li><li><strong>寻址方式的设计。可以通过基准程序进行测试统计，根据使用频率来设置必要的寻址方式</strong></li><li><strong>操作数表示和操作数类型。浮点型数据类型、整数数据类型、字符型等</strong></li><li><strong>寻址方式的表达。可以将寻址方式编码与操作码中，也可以将寻址方式作为一个单独的字段表示</strong></li><li><strong>指令格式的设计。有变长编码格式、定长编码、混合编码</strong></li></ol><h2 id="考点2-3：MIPS指令流水线的实现要有个基本认识。"><a href="#考点2-3：MIPS指令流水线的实现要有个基本认识。" class="headerlink" title="考点2.3：MIPS指令流水线的实现要有个基本认识。"></a>考点2.3：MIPS指令流水线的实现要有个基本认识。</h2><h2 id="MIPS指令（比较重要）P82"><a href="#MIPS指令（比较重要）P82" class="headerlink" title="MIPS指令（比较重要）P82"></a>MIPS指令（比较重要）P82</h2><h2 id="MIPS-流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的）-3-37图也是重点"><a href="#MIPS-流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的）-3-37图也是重点" class="headerlink" title="MIPS 流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的） 3.37图也是重点"></a>MIPS 流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的） 3.37图也是重点</h2><ol><li>取指令(IF)周期</li><li>指令译码&#x2F;读寄存器(ID)周期</li><li>执行&#x2F;有效地址计算(EX)  周期</li><li>存储器访问&#x2F;分支完成(MEM)周期</li><li>写回(WB)周期</li></ol><h1 id="作业-1"><a href="#作业-1" class="headerlink" title="作业"></a>作业</h1><p><strong>2.2 区分不同指令集结构的主要因素是什么？根据这个主要因素可将指令集结构分为哪3类？</strong></p><p>CPU中用来存储操作数的存储单元</p><p>堆栈型结构、累加器结构和通用寄存器型结构</p><p><strong>2.4指令集应该满足哪几个基本要求？</strong></p><p>完整性、规整性、高效率和兼容性</p><p><strong>2.5指令集结构设计所涉及的内容有哪些？</strong></p><ol><li><strong>指令集功能设计。分为RISC 和CISC</strong></li><li><strong>寻址方式的设计。可以通过基准程序进行测试统计，根据使用频率来设置必要的寻址方式</strong></li><li><strong>操作数表示和操作数类型。浮点型数据类型、整数数据类型、字符型等</strong></li><li><strong>寻址方式的表达。可以将寻址方式编码与操作码中，也可以将寻址方式作为一个单独的字段表示</strong></li><li><strong>指令格式的设计。有变长编码格式、定长编码、混合编码</strong></li></ol><p>2.6 CISC的缺点</p><ol><li>各种指令的使用频率相差悬殊</li><li>指令集的复杂性使得计算机系统结构变得复杂，增加了研制时间和成本，还容易造成设计错误</li><li>不利于单片集成</li><li>运行速度慢</li><li>不利于采用流水技术来提高系统的性能</li></ol><p><strong>2.7简述RISC指令集结构的设计原则</strong></p><ol><li>选取使用频率最高的指令，并补充一些最有用的指令</li><li>每条指令功能应尽可能简单，并在一个机器周期内完成</li><li>所有指令长度均相同</li><li>只有load和store操作指令才可以访问存储器</li><li>以简单、有效的方式支持高级语言</li></ol><p>2.9</p><p><strong>2.11根据CPU性能公式，简述RISC和CISC的性能特点</strong></p><p>CPU性能公式为 CPU时间 &#x3D; IC × CPI × T</p><p>相同功能的CISC目标程序的指令条数少于RISC，但是CISC的 CPI 和 T 都大于 RISC ,所以 CISC 目标程序的执行时间比RISC 的更长。</p><h1 id="第三章（流水线技术是重点）"><a href="#第三章（流水线技术是重点）" class="headerlink" title="第三章（流水线技术是重点）"></a>第三章（流水线技术是重点）</h1><h2 id="考点3-1：流水线的基本概念"><a href="#考点3-1：流水线的基本概念" class="headerlink" title="考点3.1：流水线的基本概念"></a>考点3.1：流水线的基本概念</h2><p>在计算机中，把一个重复的过程分解为若干个子过程，每个子过程由专门的功能部件来实现。将多个处理过程在时间上错开，依次通过各功能段，这样，每个子过程就可以与其他子过程并行进行，这就是流水线技术。</p><p>特点</p><ul><li>将一个大的处理功能部件分解为多个独立的功能部件，并依靠它们的并行工作来提高吞吐率</li><li>各段的时间应尽可能相等</li><li>每一段后面都要设计一个流水寄存器</li><li>适合大量重复的时序过程</li><li>需要有通过时间和排空时间（指第一个任务和最后一个任务进入流水线到流出结果的那个时间段）</li></ul><p>流水线的分类</p><ol><li><p>单功能流水线：只能完成一种固定功能的流水线：只能做加法</p></li><li><p>多功能流水线：流水线各段可以进行不同的连接，以实现不同的功能</p><ol><li>静态流水线：要先做完加法，才能做乘法</li><li>动态流水线：加法还没结束时，乘法已经开始了</li></ol></li><li><p>线性流水线与非线性流水线：按照是否有反馈回路分类的</p></li></ol><h2 id="考点3-2：流水线的性能指标"><a href="#考点3-2：流水线的性能指标" class="headerlink" title="考点3.2：流水线的性能指标"></a>考点3.2：流水线的性能指标</h2><p>衡量流水线性能的主要指标有：吞吐量、加速比和效率</p><p>吞吐量(TP)是指在单位时间内流水线所完成的任务数量或输出结果的数量</p><p>加速比S是指使用顺序处理方式处理一批任务所用的时间与按流水处理方式处理同一批任务所用的时间之比</p><p>效率E是指流水线中的设备实现使用时间与整个运行时间的比值</p><h2 id="流水线所带来的问题（指令相关性造成的冲突"><a href="#流水线所带来的问题（指令相关性造成的冲突" class="headerlink" title="流水线所带来的问题（指令相关性造成的冲突)"></a>流水线所带来的问题（指令相关性造成的冲突)</h2><ol><li>瓶颈问题</li><li>流水线的额外开销（流水寄存器延迟和时钟偏移开销）</li><li>冲突问题（如果流水线中的指令或数据之间存在关联，则它们可能要相互等待，引起访问冲突，造成流水线的停顿）</li></ol><h3 id="考点3-3：流水线的相关与冲突"><a href="#考点3-3：流水线的相关与冲突" class="headerlink" title="考点3.3：流水线的相关与冲突"></a>考点3.3：流水线的相关与冲突</h3><h4 id="相关的三种类型：数据相关（真相关）、名相关（反相关，输出相关）、控制相关"><a href="#相关的三种类型：数据相关（真相关）、名相关（反相关，输出相关）、控制相关" class="headerlink" title="相关的三种类型：数据相关（真相关）、名相关（反相关，输出相关）、控制相关"></a>相关的三种类型：数据相关（真相关）、名相关（反相关，输出相关）、控制相关</h4><h4 id="流水线冲突"><a href="#流水线冲突" class="headerlink" title="流水线冲突"></a>流水线冲突</h4><p>是指对于具体的流水线来说，由于相关的存在，使得指令流中的下一条指令不能在指定的时钟周期开始执行</p><p>分为三种类型：结构冲突、数据冲突、控制冲突</p><ol><li>结构冲突：因硬件资源满足不了指令重叠执行的要求而发生的冲突</li><li>数据冲突：需要用到前面指令的执行结果而发生的冲突</li><li>控制冲突：流水线遇到分支指令和其他会改变PC值的指令所引起的冲突。</li></ol><p>数据冲突包含3种：</p><ul><li>写后读冲突RAW</li><li>读后写冲突WAR</li><li>写后写冲突WAW</li></ul><h2 id="MIPS指令（比较重要）P82-1"><a href="#MIPS指令（比较重要）P82-1" class="headerlink" title="MIPS指令（比较重要）P82"></a>MIPS指令（比较重要）P82</h2><h2 id="MIPS-流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的）-3-37图也是重点-1"><a href="#MIPS-流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的）-3-37图也是重点-1" class="headerlink" title="MIPS 流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的） 3.37图也是重点"></a>MIPS 流水线的实现（分成了哪些阶段5个、这些指令是如何去控制这些指令的执行的） 3.37图也是重点</h2><ol><li>取指令(IF)周期</li><li>指令译码&#x2F;读寄存器(ID)周期</li><li>执行&#x2F;有效地址计算(EX)  周期</li><li>存储器访问&#x2F;分支完成(MEM)周期</li><li>写回(WB)周期</li></ol><h2 id="考点3-4：流水线的性能计算"><a href="#考点3-4：流水线的性能计算" class="headerlink" title="考点3.4：流水线的性能计算"></a>考点3.4：流水线的性能计算</h2><p>3.11</p><h1 id="作业-2"><a href="#作业-2" class="headerlink" title="作业"></a>作业</h1><p>3.2 简述流水线技术的特点</p><ol><li>把一个大的处理功能部件分解为多个独立的功能部件，依靠它们的并行工作来提高吞吐率</li><li>各段的时间应尽可能相等，否则将引起流水线堵塞和断流</li><li>每一个功能部件后都有一个流水寄存器</li><li>适用于大量重复的时序过程</li><li>有通过时间和排空时间。在这段时间都不是满负荷工作</li></ol><p>3.4 解决流水线瓶颈问题有哪些常用方法？</p><p>细分瓶颈段和重复设置瓶颈段</p><p>3.6</p><p>3.11                                                                                                                                                                                        </p><h1 id="第四章-指令级并行"><a href="#第四章-指令级并行" class="headerlink" title="第四章 指令级并行"></a>第四章 指令级并行</h1><h2 id="考点4-1：指令级并行的概念"><a href="#考点4-1：指令级并行的概念" class="headerlink" title="考点4.1：指令级并行的概念"></a>考点4.1：指令级并行的概念</h2><p>ILP是指指令之间存在的一种潜在的并行性，利用它，计算机可以并行执行两条或多条以上的指令。 开发ILP的途径有两种，一种是资源重复，通过重复设置多个处理部件，来实现同时执行多条指令；另一种是采用流水线技术，使指令重叠并行执行                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </p><p>2、如何应对冲突和提高效率，比如 动态调度和静态调度（两个差异）动态调度的优点</p><ul><li>指令调度是一种用来避免冲突的主要方法，但它并不改变相关</li><li>为了保持程序执行的正确性，必须保持的最关键的两个属性是：数据流和异常行为</li></ul><p>静态调度和动态调度</p><ol><li>静态调度的流水线依靠<strong>编译器</strong>对代码进行静态调度，以减少相关和冲突。之所以称之为静态调度，是因为它不在程序执行的过程中而是编译期间进行代码调度和优化。</li><li>静态调度通过把相关的指令拉开“距离”来减少可能产生的停顿</li><li>动态调度是在程序的执行过程中，依靠专门硬件对代码进行调度</li><li>动态调度能够处理一些编译时情况不明的相关，并简化了编译器</li><li>使本来是面向某一流水线优化编译的代码在其他动态调度的流水线上也能高效地执行</li><li>动态调度的优点是以硬件复杂性的显著增加为代价的</li></ol><h2 id="考点4-2：什么是动态分支预测技术，具有什么优缺点。"><a href="#考点4-2：什么是动态分支预测技术，具有什么优缺点。" class="headerlink" title="考点4.2：什么是动态分支预测技术，具有什么优缺点。"></a>考点4.2：什么是动态分支预测技术，具有什么优缺点。</h2><p>动态分支预测技术（是什么、现在的代表性方法）</p><p>用硬件动态进行分支处理的方法是   在程序运行时，根据分支指令过去的表现来预测其将来的行为。如果分支行为发生了变化，预测结果也跟着改变，因此有更好地预测准确度和适应性。</p><p>目的：</p><ul><li>预测分支是否成功</li><li>尽快找到分支目标地址</li></ul><p>要解决的问题：</p><ul><li>如何记录分支的历史信息</li><li>如何根据这些信息来预测分支的去向，甚至提前取出分支目标处的指令</li></ul><p>方法：</p><ol><li><p>采用分支历史表BHT</p><ol><li>最简单的动态分支预测方法</li></ol></li><li><p>采用分支目标缓冲器BTB</p><ol><li>能够提供足够的指令流，相比于BHT，可以提前一拍（在IF段）知道分支目标地址，下一条指令地址、以及预测的结果</li></ol></li><li><p>基于硬件的前瞻执行</p><ol><li>硬件太复杂</li><li>对分支指令的结果进行猜测，然后按照这个猜测结果继续取、流出和执行后续的指令</li></ol></li></ol><h2 id="考点4-3：循环展开和指令调度"><a href="#考点4-3：循环展开和指令调度" class="headerlink" title="考点4.3：循环展开和指令调度"></a>考点4.3：循环展开和指令调度</h2><p>减少这种附加的循环控制开销在程序执行时间中所占的比例，可以采用循环展开技术。</p><p>循环展开：把循环体的代码复制多次并按顺序排放，然后相应调整循环结束条件。这种技术给编译器进行指令调度提供了更大的空间，多次循环的代码可以一起调度，而且消除了中间的分支指令。</p><p>通过<strong>循环展开、寄存器重命名和指令调度</strong>，可以有效地开发出指令级并行。</p><p>循环展开和指令调度时要注意以下几个方面</p><ol><li>保证正确性</li><li>注意有效性</li><li>使用不同的寄存器</li><li>删除多余的测试指令和分支指令</li><li>注意对存储器数据的相关性分析</li><li>注意新的相关性</li></ol><h1 id="作业-3"><a href="#作业-3" class="headerlink" title="作业"></a>作业</h1><p>简述Tomasulo算法的基本思想</p><ol><li>记录和检测指令相关，操作数一旦就绪就立即执行，把发生RAW冲突的可能性减小到最小</li><li>通过寄存器换名来消除WAR冲突和WAW冲突。寄存器换名是通过保留站来实现，它保存等待流出和正在流出指令所需要的操作数</li></ol><p>4.4</p><p>4.5</p><h1 id="第五章-存储系统的层次结构"><a href="#第五章-存储系统的层次结构" class="headerlink" title="第五章 存储系统的层次结构"></a>第五章 存储系统的层次结构</h1><h2 id="考点5-1：存储系统的层次结构"><a href="#考点5-1：存储系统的层次结构" class="headerlink" title="考点5.1：存储系统的层次结构"></a>考点5.1：存储系统的层次结构</h2><p>三级存储系统</p><p>采用<strong>Cache、主存储器和磁盘存储器</strong>构成的三级存储系统。这个存储系统可以看成是由“Cache-主存”层次和“主存-辅存”层次构成的系统</p><p>5.2 简述“Cache-主存”层次与“主存-辅存”层次的区别</p><table><thead><tr><th></th><th>CACHE-主存</th><th>主存-辅存</th></tr></thead><tbody><tr><td>目的</td><td>为了弥补主存速度的不足</td><td>为了弥补主存容量的不足</td></tr><tr><td>存储管理的实现</td><td>全部由专用硬件实现</td><td>主要由软件实现</td></tr><tr><td>访问速度的比值</td><td>几比一</td><td>几万比一</td></tr><tr><td>典型的块大小</td><td>几十个字节</td><td>几百到几千个字节</td></tr><tr><td>CPU对第二级的访问方式</td><td>可直接访问</td><td>均通过第一级</td></tr><tr><td>失效时CPU是否切换</td><td>不切换</td><td>切换到其他进程</td></tr></tbody></table><p>主存和辅存的设备有哪些（了解别名 如硬盘） </p><p>cache ：高速缓冲存储器</p><h2 id="考点5-2：Cache基本知识，包括映像规则以及Cache的一些改进方法"><a href="#考点5-2：Cache基本知识，包括映像规则以及Cache的一些改进方法" class="headerlink" title="考点5.2：Cache基本知识，包括映像规则以及Cache的一些改进方法"></a>考点5.2：Cache基本知识，包括映像规则以及Cache的一些改进方法</h2><h3 id="cache部分很重要（cache的构造方式、映像规则和之间的优缺点、问题；最关键的是设计cache的时候保证CPU访问的命中率高、降低不命中，率采取什么思路来完成）"><a href="#cache部分很重要（cache的构造方式、映像规则和之间的优缺点、问题；最关键的是设计cache的时候保证CPU访问的命中率高、降低不命中，率采取什么思路来完成）" class="headerlink" title="cache部分很重要（cache的构造方式、映像规则和之间的优缺点、问题；最关键的是设计cache的时候保证CPU访问的命中率高、降低不命中，率采取什么思路来完成）"></a>cache部分很重要（cache的构造方式、映像规则和之间的优缺点、问题；最关键的是设计cache的时候保证CPU访问的命中率高、降低不命中，率采取什么思路来完成）</h3><p>Cache 基本结构和原理</p><p>Cache 是按块进行管理的。Cache 和主存均被分割成大小相同的块，信息以块为单位调入Cache.</p><table><thead><tr><th>主存地址</th><th>块地址</th><th>块内偏移</th></tr></thead></table><p>Cache基本工作原理</p><ol><li>当CPU要访问存储系统时，它先把地址送入主存地址寄存器</li><li>通过主存-Cache地址转换部件判断要访问的块是否已经在Cache,如果命中，就会把主存地址的块地址转为在Cache 中的块地址放入Cache地址寄存器，块内地址保持不变，然后用该地址去访问Cache，把得到的数据送入到CPU中</li><li>如果没有命中，就需要用主存地址去访问主存，把相应的块调入Cache中，继续 步骤2。</li></ol><p>映像规则</p><p>目的：确定一个块从主存调入到Cache时可以放到哪些位置</p><p>分为三种</p><ul><li>全相连映像：主存的任一块可以被放入到Cache 的任意一个位置</li><li>直接映像：主存的每一个块只能被放到Cache中的唯一的一个位置</li><li>组相连映像：Cache 被等分为若干个组，每组由若干个块构成。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   是直接映像和全相联映射的一种折中：一个主存块首先直接映射到唯一的一个组中，然后这个块可以被放入到这个组中的任意一个位置。n路组相连表示每个组中有n个块。n越大，Cache空间的利用率就高，块冲突的概率就越小，不命中率就低。</li><li>全相连映射的不命中率最低，直接映射的不命中率最高</li><li>从降低不命中率来看，n值越高越好，但是，n的增大会使Cache的实现复杂度和成本增加，它不一定可以使得整个计算机系统的性能提高</li></ul><h2 id="一致性问题往往出现在写操作方面，如何保证一致性，如何实现写策略。"><a href="#一致性问题往往出现在写操作方面，如何保证一致性，如何实现写策略。" class="headerlink" title="一致性问题往往出现在写操作方面，如何保证一致性，如何实现写策略。"></a>一致性问题往往出现在写操作方面，如何保证一致性，如何实现写策略。</h2><p>不一致性问题：当处理机进行“写”访问时，往Cache写入新的数据后，则Cache中相应单元的内容已发生变化，而主存中该单元的内容却仍然是原来的。</p><p>写策略：</p><ol><li>写直达法。是指不仅把数据写入Cache 中相应的块而且也写入下一级存储器</li><li>写回法。只把数据写入Cache中相应的块，不写入下一级存储器。这些最新的数据只有在相应的块被替换时，才被写回下一级存储器。会设置一个“修改位”，用于指示该块是否被修改过。当一个块被替换但是没有被修改过，就不必写回下一级存储器。</li><li>写回法的优点是速度快，写直达法的优点是易于实现，而且下一级存储器中的数据总是最新的。</li></ol><h2 id="Cache的容量比较小，所以Cache失效的原因在哪里，降低Cache不命中率的一些方法。"><a href="#Cache的容量比较小，所以Cache失效的原因在哪里，降低Cache不命中率的一些方法。" class="headerlink" title="Cache的容量比较小，所以Cache失效的原因在哪里，降低Cache不命中率的一些方法。"></a>Cache的容量比较小，所以Cache失效的原因在哪里，降低Cache不命中率的一些方法。</h2><p>了解一下“减少cache不命中开销”、”“有哪些方法</p><p>改进Cache 性能方式：降低不命中率、减少不命中开销和减少命中时间</p><p>降低不命中率的方式</p><ul><li>增加Cache块的大小</li><li>增加Cache 容量</li><li>提高相联度</li></ul><p>减少Cache不命中开销</p><ul><li><p>两级Cache</p></li><li><p>让读不命中优先于写</p></li><li><p>请求字处理技术          </p><p>​</p></li></ul><p>减少命中时间</p><ul><li>容量小，结构简单的Cache</li><li>虚拟Cache</li><li>追踪Cache</li></ul><h1 id="作业-4"><a href="#作业-4" class="headerlink" title="作业"></a>作业</h1><p>5.2 简述“Cache-主存”层次与“主存-辅存”层次的区别</p><table><thead><tr><th></th><th>CACHE-主存</th><th>主存-辅存</th></tr></thead><tbody><tr><td>目的</td><td>为了弥补主存速度的不足</td><td>为了弥补主存容量的不足</td></tr><tr><td>存储管理的实现</td><td>全部由专用硬件实现</td><td>主要由软件实现</td></tr><tr><td>访问速度的比值</td><td>几比一</td><td>几万比一</td></tr><tr><td>典型的块大小</td><td>几十个字节</td><td>几百到几千个字节</td></tr><tr><td>CPU对第二级的访问方式</td><td>可直接访问</td><td>均通过第一级</td></tr><tr><td>失效时CPU是否切换</td><td>不切换</td><td>切换到其他进程</td></tr></tbody></table><p> 5.7简述“虚拟索引+物理标识”Cache的基本思想。它有什么优缺点？</p><ol><li>直接用虚地址中的页内位移作为访问Cache的索引，但标识的却是物理地址。CPU发出访存请求后，在进行虚实地址转换的同时，可并行进行标识的读取。在完成地址转换之后，再把得到的物理地址与标识进行比较</li><li>优点：有虚拟Cache 和物理Cache 的好处</li><li>局限性：Cache 容量受到限制</li></ol><h1 id="第六章-输入-x2F-输出系统"><a href="#第六章-输入-x2F-输出系统" class="headerlink" title="第六章 输入&#x2F;输出系统"></a>第六章 输入&#x2F;输出系统</h1><h2 id="输入输出系统的作用是什么"><a href="#输入输出系统的作用是什么" class="headerlink" title="输入输出系统的作用是什么"></a>输入输出系统的作用是什么</h2><p>是计算机系统中的一个重要组成部分，它完成计算机与外界的信息交换，或者给计算机提供大容量的外部存储器。</p><h2 id="总线的设计方法"><a href="#总线的设计方法" class="headerlink" title="总线的设计方法"></a>总线的设计方法</h2><table><thead><tr><th>特性</th><th>高性能</th><th>低价格</th></tr></thead><tbody><tr><td>总线独立性</td><td>独立的地址和数据总线</td><td>数据和地址分时共用同一套总线</td></tr><tr><td>数据总线宽度</td><td>越宽越快</td><td>越窄越便宜</td></tr><tr><td>传输块的大小</td><td>块越大总线开销越小</td><td>单字传送更简单</td></tr><tr><td>总线主设备数量</td><td>多个（需要仲裁）</td><td>单个（无须仲裁）</td></tr><tr><td>分离事务</td><td>采用，因为分离的请求包和回答包能提高总线带宽</td><td>不采用，因为持续连接成本更低，而且延迟更小</td></tr><tr><td>定时方式</td><td>同步</td><td>异步</td></tr></tbody></table><p>选择同步还是异步：如果设备类型较少且距离较少，则宜采用同步总线，否则应采用异步总线。</p><h2 id="考点6-1：通道处理机的作用和功能以及工作过程。"><a href="#考点6-1：通道处理机的作用和功能以及工作过程。" class="headerlink" title="考点6.1：通道处理机的作用和功能以及工作过程。"></a>考点6.1：通道处理机的作用和功能以及工作过程。</h2><h3 id="通道技术的作用和功能、种类"><a href="#通道技术的作用和功能、种类" class="headerlink" title="通道技术的作用和功能、种类"></a>通道技术的作用和功能、种类</h3><p>作用</p><ul><li>为了把对外设的管理工作从CPU中分离出来，使CPU摆脱繁重的输入&#x2F;输出负担，也为了使设备能共享输入&#x2F;输出接口</li><li>由通道处理机来专门负责整个计算机系统的输入&#x2F;输出工作</li></ul><p>功能</p><ol><li>接收CPU发来的I&#x2F;O指令，并根据指令要求选择指定外设与通道相连接</li><li>执行通道程序</li><li>给出外设中要进行读&#x2F;写操作的数据所在的地址</li><li>给出主存缓冲区的首地址</li><li>控制外设与主存缓冲区之间的数据传送的长度</li><li>指定传送工作结束时要进行的操作</li><li>检查外设的工作状态是否正常</li><li>在数据传送中完成必要的格式变换</li></ol><p>种类</p><ol><li>字节多路转换通道：通道以字节交叉方式轮流为外设提供服务</li><li>选择通道：每次只能选择一个外设工作</li><li>字组多路转换通道：控制多个外设设备并以组交叉的方式传输数据</li></ol><p>工作过程</p><ol><li>在用户程序中使用访管指令进行管理程序，由管理程序来编制一个通道程序，并且启动通道</li><li>通道处理机执行通道程序，完成指定的数据输入&#x2F;输出工作</li><li>通道程序结束后向CPU发中断请求</li></ol><h1 id="作业-5"><a href="#作业-5" class="headerlink" title="作业"></a>作业</h1><p>6.2RAID有哪些分级？各有何特点？</p><p>有六种</p><ol><li><p>RAID 0，把数据分布在多个盘上，非冗余阵列，无冗余信息</p></li><li><p>RAID 1，使用双备份磁盘。每当把数据写入一个磁盘时，将该数据也写入到另一个冗余盘中，可靠性高，但是效率低</p></li><li><p>RAID 2，交叉式汉明编码阵列。原理上比较优越，但冗余信息的开销太大，因此未被广泛应用</p></li><li><p>……..</p></li></ol><p>6.3同步总线和异步总线各有什么优缺点？</p><p>同步总线</p><ul><li><p>总线上所有设备通过统一的总线系统时钟进行同步</p></li><li><p>优点是成本低，  不需要设备之间互相确定时序逻辑</p></li><li><p>缺点是总线操作必须以相同的速度运行</p><p>​</p></li></ul><p>异步总线</p><ul><li>异步总线上的设备之间没有统一的系统时钟，设备自己内部定时。设备之间的信息传送用总线发送器和接收器来控制。</li><li>优点是适用于更广泛的设备类型，扩充时不必担心时钟时序和时钟同步问题</li><li>缺点是在传输中，异步总线需要额外的同步开销</li></ul><p>6.4通道分为哪3种类型？它们分别为那种外围设备服务？</p><ul><li>字节多路转换通道。这是一种简单的共享通道，主要为多台低速或中速的外设服务</li><li>字组多路转换通道，它适于为高速设备服务</li><li>选择通道，它主要是为高速外设（如磁盘存储器等）服务的</li></ul><h1 id="第七章-互连网络"><a href="#第七章-互连网络" class="headerlink" title="第七章 互连网络"></a>第七章 互连网络</h1><h2 id="互联网络是干什么用的，基本概念"><a href="#互联网络是干什么用的，基本概念" class="headerlink" title="互联网络是干什么用的，基本概念"></a>互联网络是干什么用的，基本概念</h2><p>互联网络设计一种由开关元件按照一定拓扑结构和控制方式构成的网络，用来实现计算机系统中节点之间的相互连接。</p><p>三大要素：互连结构、开关元件和控制方式</p><h2 id="考点7-1：互连网络的作用"><a href="#考点7-1：互连网络的作用" class="headerlink" title="考点7.1：互连网络的作用"></a>考点7.1：互连网络的作用</h2><p>互联网络已成为<strong>单指令流多数据流计算机和多指令流多数据流计算机</strong>的关键组成之一。随着各个领域对高性能计算的要求越来越高，多处理机和多计算机系统的规模越来越大，对处理器之间或处理机与存储模块之间的通信的速度和灵活度的要求也越来越高。因此，它对计算机系统的<strong>性能价格比</strong>有着决定性的影响。</p><h2 id="静态互联、动态互联（交叉开关、多级互联网络）"><a href="#静态互联、动态互联（交叉开关、多级互联网络）" class="headerlink" title="静态互联、动态互联（交叉开关、多级互联网络）"></a>静态互联、动态互联（交叉开关、多级互联网络）</h2><ul><li>静态互联网络是指各节点之间有固定的连接通路，且在运行中不能改变的网络</li><li>动态互联网络是指由交换开关构成，可按运行程序要求动态改变连接状态的网络</li></ul><h2 id="7-5要知道相关的消息传递机制，和应用在哪里（有印象）"><a href="#7-5要知道相关的消息传递机制，和应用在哪里（有印象）" class="headerlink" title="7.5要知道相关的消息传递机制，和应用在哪里（有印象）"></a>7.5要知道相关的消息传递机制，和应用在哪里（有印象）</h2><h1 id="作业-6"><a href="#作业-6" class="headerlink" title="作业"></a>作业</h1><h1 id="第八章-多处理机"><a href="#第八章-多处理机" class="headerlink" title="第八章  多处理机"></a>第八章  多处理机</h1><h2 id="考点8-1：并行处理面临的挑战。"><a href="#考点8-1：并行处理面临的挑战。" class="headerlink" title="考点8.1：并行处理面临的挑战。"></a>考点8.1：并行处理面临的挑战。</h2><ol><li>有限的并行性</li><li>较大的通信开销</li></ol><h2 id="考点8-2：分布式共享存储器系统结构以及对称式共享存储系统结构的结构特点。"><a href="#考点8-2：分布式共享存储器系统结构以及对称式共享存储系统结构的结构特点。" class="headerlink" title="考点8.2：分布式共享存储器系统结构以及对称式共享存储系统结构的结构特点。"></a>考点8.2：分布式共享存储器系统结构以及对称式共享存储系统结构的结构特点。</h2><p>根据系统中处理器个数的多少，可把现有MIMD计算机分为两类,每一类代表了一种存储器的结构和互连策略。</p><ol><li>第一类计算机采用集中式共享存储器结构，处理器数量少，各处理器可共享一个集中式的物理存储器。因为只有单一的主存，而且这个主存相对于各处理器的关系也是对称的，所以这类计算机经常称为对称式共享存储器多处理机SMP</li><li>第二类计算机是分布式共享存储器多处理机。在这类计算机中，存储器在物理上是分布的。它支持构建规模较大的多处理机系统。这种结构要求有高带宽的互连网络<ol><li>如果大多数访存都是针对本地节点的存储器后，则可以降低对存储器和互连网络的带宽要求</li><li>对本地存储器的访问延迟事件小</li><li>处理器之间的通信较为复杂，且各处理器之间的访问延迟较大</li></ol></li></ol><p>消息传递机制用于多处理机的通讯，了解一下</p><p>对称式多处理结构、分布式多处理机结构 </p><p>访问方式有哪些</p><p>多处理机机型的远程访问对计算机性能的影响（课堂上讲过一道例题，了解一下）</p><h1 id="作业-7"><a href="#作业-7" class="headerlink" title="作业"></a>作业</h1>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>人工智能初步学习</title>
      <link href="/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/11/27/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%88%9D%E6%AD%A5%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="人工智能期末复习"><a href="#人工智能期末复习" class="headerlink" title="人工智能期末复习"></a>人工智能期末复习</h1><h2 id="人工智能绪论"><a href="#人工智能绪论" class="headerlink" title="人工智能绪论"></a>人工智能绪论</h2><h3 id="人工智能定义"><a href="#人工智能定义" class="headerlink" title="人工智能定义"></a>人工智能定义</h3><p>人工智能是研究、开发用于模拟、延伸和扩展人类智能的理论、方法、技术及应用系统的一门新技术科学。</p><p><strong>人工智能、基因工程、纳米科学</strong>被认为是21世纪的3大尖端技术。</p><h3 id="人工智能发展历史"><a href="#人工智能发展历史" class="headerlink" title="人工智能发展历史"></a>人工智能发展历史</h3><ol><li>1956，达特茅斯会议中，AI一词诞生</li><li>1970-1980，大规模数据和复杂任务不能完成，计算能力无法突破（低谷）</li><li>1982后，神经网络+5代计算机（专家系统）</li><li>1990-2000，DARPA无法实现，政府投入缩减（低谷）</li><li>2006-至今，突破性进展，进入发展热潮（深度学习）</li></ol><h2 id="人工智能数学基础"><a href="#人工智能数学基础" class="headerlink" title="人工智能数学基础"></a>人工智能数学基础</h2><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>假设函数 y &#x3D; f(x) 在某个区间上的导数存在，则在此区间上某点x<del>1</del><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_0.png" style="width: 250px;"></p><p>导数是用于研究函数在某一点附近的局部性质，用以刻画曲线或曲面的弯曲程度。</p><p>复合计算：</p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_1.png" style="width:300px;"><p>高阶导数：导数 y’&#x3D;f’(x) 仍是 x 的函数，可对导函数再次求导。</p><p>函数f(x)的泰勒展开式：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_2.png" alt="image-20231127143046876"></p><p>常用的泰勒展开</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_3.png" style="width:300px;"><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_5.png " style="width:300px;"></p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AI_4.png" style="width:300px"><h3 id="概率论基础"><a href="#概率论基础" class="headerlink" title="概率论基础"></a>概率论基础</h3><h3 id="矩阵基础"><a href="#矩阵基础" class="headerlink" title="矩阵基础"></a>矩阵基础</h3><h2 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h2><p>搜索技术是问题求解的主要手段之一</p><p>搜索问题定义：可以用6个组成部分来形式化描述：</p><ul><li>状态空间S：所有可以的状态集合</li><li>初始状态S<del>0</del>：系统的初始状态</li><li>动作状态A：可用的动作集合</li><li>转移函数T(s,a): 确定在状态s下执行动作a后到达的状态</li><li>损耗函数c(s,a,s’): 在状态s通过动作a到达状态s’的损耗</li><li>目标测试函数G(s): 判断给定的状态s是否为目标状态</li></ul><p>解：将系统由初始状态到目标状态的一系列动作称为解。取得最小总损耗的解为最优解。</p><h3 id="搜索策略"><a href="#搜索策略" class="headerlink" title="搜索策略"></a>搜索策略</h3><h4 id="盲目搜索"><a href="#盲目搜索" class="headerlink" title="盲目搜索"></a>盲目搜索</h4><p>没有先验知识，按照事先确定的排序搜索</p><ul><li>深度优先搜索（选择当前最深的节点，先进后出）</li><li>宽度优先搜索（选择当前最浅的节点，先进先出）</li></ul><h5 id="深度优先搜索"><a href="#深度优先搜索" class="headerlink" title="深度优先搜索"></a>深度优先搜索</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">初始化栈Z,令其为空;</span><br><span class="line">初始化访问表V,令其为空;</span><br><span class="line">将初始节点s放入栈z;</span><br><span class="line"><span class="keyword">while</span>(栈Z非空)</span><br><span class="line">       弹出栈顶节点n;</span><br><span class="line">       <span class="keyword">if</span>(n为目标节点)</span><br><span class="line">             返回成功;</span><br><span class="line">       <span class="keyword">if</span>(节点n不在访问表v中);</span><br><span class="line">              扩展节点n, N是当前节点n所有动作能够转移到的后继节点的集合;</span><br><span class="line">               将节点n加入访问表V中;</span><br><span class="line">               <span class="keyword">for</span> m in N:</span><br><span class="line">                   将m放入栈顶;</span><br><span class="line">返回失败;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="宽度优先搜索"><a href="#宽度优先搜索" class="headerlink" title="宽度优先搜索"></a>宽度优先搜索</h5><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">初始化先入先出队列Q , 令其为空;</span><br><span class="line">初始化访问表V , 令其为空;</span><br><span class="line">将初始节点s 放入先入先出队列Q ;</span><br><span class="line"><span class="keyword">while</span>(队列Q 非空)</span><br><span class="line">     弹出队列Q 的队首节点n ;</span><br><span class="line">      <span class="keyword">if</span>(n 为目标节点)</span><br><span class="line">            返回成功;</span><br><span class="line">       <span class="keyword">if</span>(节点n 不在访问表V 中)</span><br><span class="line">            扩展节点n ,N 是当前节点n 所有动作能够转移到的后继节点的集合;</span><br><span class="line">            将节点n 加入访问表V 中;</span><br><span class="line">            <span class="keyword">for</span> m in N ;</span><br><span class="line">                将 m 放入队列Q 的队尾;</span><br><span class="line">返回失败;</span><br><span class="line"></span><br></pre></td></tr></table></figure><table><thead><tr><th align="center"></th><th align="center">深度优先搜索</th><th align="center">宽度优先搜索</th></tr></thead><tbody><tr><td align="center">数据结构</td><td align="center">栈</td><td align="center">队列</td></tr><tr><td align="center">优点</td><td align="center">可以不存访问表，节省空间</td><td align="center">不存在死循环，单位耗散下最优</td></tr><tr><td align="center">缺点</td><td align="center">如果不用访问表，可能存在死循环</td><td align="center">需要空间较大</td></tr><tr><td align="center">共同点</td><td align="center">均会形成搜索树，只是扩展节点的顺序不同</td><td align="center"></td></tr></tbody></table><p>复杂度分析：？？？？</p><h4 id="启发式搜索"><a href="#启发式搜索" class="headerlink" title="启发式搜索"></a>启发式搜索</h4><p>根据先验知识，按照动态确定的排序搜索</p><ul><li>贪婪搜索</li><li>A^*^ 搜索</li></ul><h5 id="贪婪搜索"><a href="#贪婪搜索" class="headerlink" title="贪婪搜索"></a>贪婪搜索</h5><p>选择当前代价<strong>估计h值最小</strong>的节点，用优先队列数据结构存储候选节点</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"> 初始化以启发函数h 为键值的优先队列Q , 令其为空；</span><br><span class="line"> 初始化访问表V , 令其为空;</span><br><span class="line"> 将初始节点s 放入优先队列Q ;</span><br><span class="line"> <span class="keyword">while</span>(优先队列Q 非空)</span><br><span class="line">       弹出优先队列Q 中h 值最小的节点n ;</span><br><span class="line">        <span class="keyword">if</span>(n 为目标节点)</span><br><span class="line">              返回成功; //从目标节点n 由父节点指针回溯到初始节点s 的路径即为所得路径</span><br><span class="line">         <span class="keyword">if</span>(节点n 不在访问表V 中)</span><br><span class="line">            扩展节点n , 并设 N = 当前节点n 所有动作能够转移到的节点的集合;</span><br><span class="line">             将节点n 加入访问表V 中;</span><br><span class="line">            <span class="keyword">for</span> 节点 m <span class="keyword">in</span> N :</span><br><span class="line">                  <span class="keyword">if</span>(m 不在优先队列Q 中)</span><br><span class="line">                         将 m 放入优先队列Q , 将后继节点 m 的父节点指针指向n ;</span><br><span class="line">返回失败;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>缺点：找到的解可能不是最优的，贪婪搜索没有考虑到到达当前节点已有的代价</p><h5 id="A-搜索"><a href="#A-搜索" class="headerlink" title="A^*^ 搜索"></a>A^*^ 搜索</h5><p><strong>实际代价函数</strong> g(s)：<strong>起始节点到达节点</strong>s<strong>的代价值</strong></p><p><strong>总代价函数</strong>f(s)&#x3D;g(s)+h(s)</p><p><strong>A</strong>搜索：选择当前代价估计<strong>f值</strong>最小的节点，用优先队列数据结构存储候选节点</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">初始化以启发函数h为键值的优先队列Q , 令其为空;</span><br><span class="line">初始化访问表V , 令其为空;</span><br><span class="line">将初始节点s 放入优先队列Q, g(s) = <span class="number">0</span>, f(s) = h(s);</span><br><span class="line"><span class="keyword">while</span>(优先队列Q 非空)</span><br><span class="line">       弹出优先队列Q 中f 值最小的节点n ;</span><br><span class="line">       <span class="keyword">if</span>(n 为目标节点)</span><br><span class="line">             返回成功; //从目标节点n 由父节点指针回溯到初始节点s 的路径即为所得路径</span><br><span class="line">       <span class="keyword">if</span>(n 不在访问表 V 中)  </span><br><span class="line">     扩展节点n , 并设 N = 当前节点n 所有动作能够转移到的节点的集合;</span><br><span class="line">     将节点n 加入访问表V 中;</span><br><span class="line">      <span class="keyword">for</span> 节点m <span class="keyword">in</span> N :</span><br><span class="line">            new_g = g(n) + c(n, m);</span><br><span class="line">            new_f = new_g + h(m);</span><br><span class="line">              <span class="keyword">if</span>(m 在优先队列Q 中)</span><br><span class="line">               <span class="keyword">if</span>(new_f &lt; Q 中的f(m))</span><br><span class="line">                 将后继节点m 的父节点指针指向n;</span><br><span class="line">                 更新Q 中的f(m) =new_f, g(m) = new_g;</span><br><span class="line">              <span class="keyword">else</span></span><br><span class="line"> f(m) = new_f ;</span><br><span class="line">                   g(m) =new_g ;</span><br><span class="line">                   将m 放入优先队列Q , 将后继节点m 的父节点指针指向n ;</span><br><span class="line">返回失败;</span><br></pre></td></tr></table></figure><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312072005619.png" alt="image-20231207200530422"></p><p>证明：</p><p>1.<strong>假设</strong>n<del>j</del><strong>为</strong>n<del>i</del><strong>的后继节点，根据一致性可得</strong>f(n<del>j</del> )≥f(n<del>i</del>)，节点总是比后继节点代价小</p><p>2.<strong>其次沿着任意路径上的节点</strong>f(n)<strong>值是非递减的</strong></p><p>3.<strong>当</strong>A搜索扩展到**n<del>i</del><strong>时，到达</strong>n<del>i</del>**的最优路径已经找到（反证法）</p><p>​a)<strong>假设尚未找到，则最优路径上必有一点</strong>n<del>i</del>‘未扩展</p><p>​b)<strong>由于</strong>n<del>i</del><strong>为</strong>最优路径的终点，由<strong>2</strong>得<strong>f(n<del>i</del> )≥f(n<del>i</del>′ )<strong>，那么</strong>n<del>i</del>′</strong>应先于**n<del>i</del>**扩展，与假设矛盾（即若是有其它未发现的节点到n<del>i</del>更优，那么按照算法来说这个节点应当先于n<del>i</del> 被找到）</p><table><thead><tr><th></th><th>贪婪搜索</th><th>A^*^ 搜索</th></tr></thead><tbody><tr><td>数据结构</td><td>优先队列</td><td>优先队列</td></tr><tr><td>理论保证</td><td>无</td><td>若h是可许的，那么找到的解是最优的，若h 具有一致性，那么找到的解是最优的</td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h4 id="局部搜索"><a href="#局部搜索" class="headerlink" title="局部搜索"></a>局部搜索</h4><p>在许多问题中，我们只关心搜索算法返回的状态是否达到目标，而不关心从初始状态开始到达目标的路径。</p><p><strong>优点：不需要维护搜索树；占用内存少（不用存储路径）；在连续的并且状态空间很大的问题中，通常都可以找到足够好的解；以时间换精度</strong></p><p>局部搜索算法</p><ul><li>爬山法</li><li>模拟退火</li><li>遗传算法</li></ul><h5 id="爬山法"><a href="#爬山法" class="headerlink" title="爬山法"></a>爬山法</h5><p><strong>核心：不断移动至邻域内的最优点</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">当前解 ← 初始解</span><br><span class="line">repeat：</span><br><span class="line">L ← 当前解的邻域</span><br><span class="line">新解 ← L中评估值最高的解</span><br><span class="line"><span class="keyword">if</span> 新解的评估值 &gt; 当前解的评估值：</span><br><span class="line">        当前解 ← 新解</span><br><span class="line"><span class="keyword">else</span>：</span><br><span class="line">                结束循环</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>特点</p><p>避免了遍历全部节点，但往往只能找到一个局部最优解</p><p>解决方式：多次随机初始化</p><h5 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h5><p>在爬山法的基础上，用温度控制搜索的随机程度。每一个解都被赋予一个能量函数E(i),并要求目标状态处于最低能量状态。</p><ul><li>当<strong>温度较高时，此时解的能量几乎没有作用，“粒子”的行为比较“活跃”，选择下一状态的策略更接近随机游走</strong></li><li>当<strong>温度逐渐降低时，会更加偏向与能量小的解。能量下降，“粒子”的热运动逐渐减弱，故选择下一状态的策略更倾向于选择更优的解</strong></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">当前解 ← 初始解</span><br><span class="line"><span class="keyword">for</span> t = <span class="number">1</span> to ∞ do：</span><br><span class="line">T ← 第t次迭代的温度</span><br><span class="line">L ← 当前解的邻域</span><br><span class="line">新解 ← L中随机选择一个解</span><br><span class="line"><span class="keyword">if</span> 𝐸(𝑗)≤𝐸(𝑖)：</span><br><span class="line">接受新解：当前解 ← 新解</span><br><span class="line"><span class="keyword">else</span>：</span><br><span class="line"> 转移接受概率P ← exp(−(𝐸(j)−𝐸(i))/𝐾𝑇)</span><br><span class="line"> 以P的概率接受新解</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在温度足够高时，概率几乎为1，每一个状态都有可能被访问。当温度逐渐降低，能够被访问到的状态将逐渐收敛到几个能量最小的状态中，从而找到局部最优解。</p><table><thead><tr><th></th><th>物理退火过程</th><th>模拟退火算法</th></tr></thead><tbody><tr><td>对象</td><td>物理系统的某一状态</td><td>组合优化问题的某一个解</td></tr><tr><td>评估</td><td>状态的能量</td><td>解的评估函数值</td></tr><tr><td>目标</td><td>能量最低的状态</td><td>优化问题的最优解</td></tr><tr><td>控制变量</td><td>温度</td><td>搜索控制参数T</td></tr></tbody></table><h5 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h5><p><strong>模拟生物种群基因的变异，交叉融合，自然选择等算子</strong></p><p><strong>实现对最优化问题解的参数空间进行高效的搜索的过程</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">初始化：随机生成 N 个个体的种群</span><br><span class="line">计算种群中每个个体的适应度函数</span><br><span class="line">生成一个新的种群：</span><br><span class="line"> 选择：根据适应度函数有放回的选择N对父母个体</span><br><span class="line">交叉：每一对父母个体用交叉算子生成下一代个体</span><br><span class="line">变异：每一个生成的个体执行变异算子</span><br><span class="line">判断是否找到最优解个体，如果否，则继续生成下一代种群</span><br></pre></td></tr></table></figure><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081348843.png" alt="image-20231208134857642" style="zoom:80%;" /><h5 id="三种算法对比"><a href="#三种算法对比" class="headerlink" title="三种算法对比"></a>三种算法对比</h5><ul><li>爬山法从初始解开始，对领域内的局部空间进行有限的探索，并移动到邻域内的最优解，算法一直循环该过程，直到达到局部最优解时终止。优点在于简单直观，容易实现，缺点是容易陷入局部最优解的问题，对于存在多个局部最优解的问题，可能无法找到全局最优解</li><li>模拟退火将爬山法与随机游走结合起来，在随机游走阶段，可随机地选择邻域内的一个状态作为下一个状态，有利于摆脱局部最优。该算法在一定程度上避免困在局部最优，同时也提高了搜索的效率</li><li>遗传算法的思想来源于自然界的生物演化，通过模拟生物种群基因的变异、交叉融合、自然选择等算子，实现对最优化问题解的参数空间进行高效的搜索过程。该算法能够在求解较为复杂的组合优化问题时，通常能够在有效时间内获得较好的结果。</li></ul><h4 id="对抗搜索"><a href="#对抗搜索" class="headerlink" title="对抗搜索"></a>对抗搜索</h4><p><strong>多</strong>Agent环境，其中每个Agent需要考虑到其他Agent行动及其对自身的影响，其他Agent的不可预测性可能导致该Agent问题求解过程中的偶发性。</p><p>竞争环境：每个Agent的目标之间有冲突</p><p>博弈：有完整信息的、确定性的、轮流行动的、两个游戏者的零和游戏</p><p>确定的、完全可观察的环境中两个Agent必须轮流行动，在游戏结束时效用值总是相等且符号相反</p><p>博弈要求具备在无法计算最优决策情况下也要做出决策。</p><ul><li>智能体P ：&#x3D;{1, …, N} （通常轮流玩）</li><li>状态空间S ：所有可能的状态集合</li><li>初始状态s_0 ：系统的起始状态</li><li>动作空间A ：可用的动作集合</li><li>转移函数T(s,a) ：确定在状态s下执行动作a后到达的状态</li><li>终止函数G(s) ：判断给定的状态s是否为终止状态</li><li>收益函数U(p)  ：在终止状态p的收益</li><li>目标是找到一个策略：状态到动作的映射</li></ul><h5 id="极大极小搜索"><a href="#极大极小搜索" class="headerlink" title="极大极小搜索"></a>极大极小搜索</h5><p>基本思想：使用一个收益评估函数v(p) 对给定的中间节点p 进行评估，并通过搜索找到使收益评估函数最大（或最小）的动作</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(节点p 是终止节点)            //收益函数中的第一种情况</span><br><span class="line">       返回节点p 的收益函数𝑈(𝑝)</span><br><span class="line"><span class="keyword">if</span>(节点p 是极大方)               //收益函数中的第二种情况</span><br><span class="line">       𝑣 :=−∞</span><br><span class="line">       <span class="keyword">for</span> x <span class="keyword">in</span> 子节点集合</span><br><span class="line">            𝑣 :=<span class="built_in">max</span>⁡(𝑣 , 极小极大搜索(𝑥 , 否))//递归计算极小方节点的收益函数</span><br><span class="line">       返回v</span><br><span class="line"><span class="keyword">else</span>                                      //收益函数中的第三种情况</span><br><span class="line">        𝑣 :=+∞</span><br><span class="line">       <span class="keyword">for</span> x <span class="keyword">in</span> 子节点集合</span><br><span class="line">               𝑣 :=<span class="built_in">min</span>⁡(𝑣 , 极小极大搜索(𝑥 , 是))       //递归计算极大方节点的收益函数</span><br><span class="line">      返回v</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>优点： 能找到最优策略</p><p>缺点：需要展开整个搜索树</p><h5 id="alpha-beta-剪枝搜索"><a href="#alpha-beta-剪枝搜索" class="headerlink" title="alpha-beta 剪枝搜索"></a>alpha-beta 剪枝搜索</h5><p>Alpha-Beta 剪枝搜索通过避免不必要的节点搜索来提高算法的运行效率，是对极大极小搜索算法的优化。</p><p><strong>基本思想</strong>：如果当前节点已知对手存在一个策略是自己获得的收益少于之前某个节点能够获得的收益，那玩家一定不会选择当前节点，故无需继续搜索当前节点的剩余子节点。</p><p>引入alpha,beta 两个变量</p><p>alpha: 表示到目前为止的路径上发现的max玩家当前的最优值</p><p>beta：表示到目前为止的路径上发现的min玩家当前的最优值</p><p>如果在某一个节点有alpha&gt;&#x3D;beta, 则说明该玩家当前的最优策略Beta 劣于之前已有的最优策略alpha，故无需搜索当前节点的剩余子节点，可以进行剪枝操作（象棋举例：已知当自己下a 时对面吃掉自己的马是最坏的情况，但当自己下b 时会知道对面最少可以吃掉自己的车，所以这个时候就知道没有必要再去思考下b 还会带来的更坏的后果了，因为此时已经比下a的情况更糟糕了）</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081519848.png" alt="image-20231208151901768"></p><p>初始化： alpha &#x3D; -inf , beta &#x3D;  inf </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(节点 p 是终止节点)          //收益函数中的第一种情况</span><br><span class="line">        返回 节点p 的收益函数 𝑈(𝑝)</span><br><span class="line"><span class="keyword">if</span>(节点 p 是极大方)             //收益函数中的第二种情况</span><br><span class="line">        𝑣 :=−∞ </span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> 节点 p 的子节点集合</span><br><span class="line">         𝑣 :=<span class="built_in">max</span>⁡(𝑣,  AlphaBeta搜索(𝑥 ,𝛼,𝛽, 否))//递归计算极小方节点的收益函数</span><br><span class="line">   𝛼:=<span class="built_in">max</span>⁡(𝛼,𝑣)</span><br><span class="line">  <span class="keyword">if</span> (𝛼≥𝛽)              //剪枝, 降低搜索量</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        返回𝑣</span><br><span class="line"><span class="keyword">else</span>                   //收益函数中的第三种情况: 节点 p 是极小方</span><br><span class="line">        𝑣 :=+∞ ;</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> 节点 p 的子节点集合</span><br><span class="line">                𝑣 ≔<span class="built_in">min</span>⁡(𝑣,  AlphBeta搜索(𝑥, 𝛼, 𝛽, 是))//递归计算极小方节点的收益函数</span><br><span class="line">                𝛽:=<span class="built_in">min</span>⁡(𝛽,  𝑣)</span><br><span class="line">                <span class="keyword">if</span> (𝛼≥𝛽)      //剪枝, 降低搜索量</span><br><span class="line">                         <span class="keyword">break</span></span><br><span class="line">        返回𝑣</span><br></pre></td></tr></table></figure><h5 id="蒙特卡洛树搜索（MTCS）"><a href="#蒙特卡洛树搜索（MTCS）" class="headerlink" title="蒙特卡洛树搜索（MTCS）"></a>蒙特卡洛树搜索（MTCS）</h5><p>一种<strong>概率和启发式驱动</strong>的搜索算法，将经典的树搜索实现与<strong>强</strong>化学<strong>习的机器学习</strong>原理相结合</p><p>树搜索中存在当前最佳动作实际上不是最佳动作的可能性</p><p>“**探索-**开发权衡”策略：学习阶段通过定期评估其他备选方案，而不是当前感知的最佳策略</p><p><strong>探索扩展树的宽度</strong>，有助于确保 MCTS 不会忽略任何可能更好的路径，大量重复时变得低效</p><p><strong>开发扩展树的深度</strong>，坚持具有最大估计值的单一路径</p><h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><ul><li>盲目搜索，没有利用问题定义本身之外的知识，而是根据事先对比好的某种固定排序，依次调用动作，以探求得到目标。</li><li>启发式搜索，利用问题定义本身之外的知识来引导搜索，主要通过访问启发函数来估计每个节点到目标点的代价或损耗</li><li>局部搜索，用在当前解的领域内来寻找更优解</li><li>对抗搜索，出现在多个智能体的对抗性博弈中，在其它智能体通过搜索寻找它们的最优解的情况下寻找最优策略</li></ul><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h3><p>本质是上根据数据中的例子学习<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081606591.png" alt="image-20231208160606508" style="zoom:70%;" /></p><p>通过损失函数来评价结果的好坏</p><h4 id="过拟合与欠拟合"><a href="#过拟合与欠拟合" class="headerlink" title="过拟合与欠拟合"></a>过拟合与欠拟合</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081609416.png" alt="image-20231208160930307"></p><p><strong>过拟合</strong>：函数表达能力强，使得模型在训练集上损失很少，但在测试集上损失很大泛化能力差</p><p><strong>欠拟合</strong>：函数表达能力弱，使得模型在训练集和测试集上损失都很大。</p><h3 id="无监督学习与半监督学习"><a href="#无监督学习与半监督学习" class="headerlink" title="无监督学习与半监督学习"></a>无监督学习与半监督学习</h3><h4 id="一个经典算法-k-means-算法"><a href="#一个经典算法-k-means-算法" class="headerlink" title="一个经典算法 k means 算法"></a>一个经典算法 k means 算法</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">确定好𝑘</span><br><span class="line">随机选择𝑘 个中心</span><br><span class="line">将每个点与离它最近的中心相连（归属于此聚类）</span><br><span class="line">将每个聚类的点求平均，算作新的中心</span><br><span class="line">重复第<span class="number">3</span>&amp;<span class="number">4</span>步，直到收敛</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="谱聚类"><a href="#谱聚类" class="headerlink" title="谱聚类"></a>谱聚类</h4><p>邻接矩阵A：第(i,j) 位置存放的是i,j 之间的相似度</p><p>对角度数矩阵D：第(i,i)位置存放的是所有与i相连的边权和。非对角位置为0</p><p>拉普拉斯矩阵定义为 ： L &#x3D; D - A</p><p>优化和泛化的区别：优化是寻找损失函数最小的f的过程，泛化是使f 在没有见过的数据也有很好的表现能力。总的来说，优化是使f 在见过的数据表现更好，泛化是使f 在没有见过的数据上表现也很好</p><h2 id="线性回归方法"><a href="#线性回归方法" class="headerlink" title="线性回归方法"></a>线性回归方法</h2><p>特点：解释性强，简单，泛化能力稳定</p><p>应用领域：经济学，社会领域</p><h3 id="优化方式"><a href="#优化方式" class="headerlink" title="优化方式"></a>优化方式</h3><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><h3 id="二分类"><a href="#二分类" class="headerlink" title="二分类"></a>二分类</h3><p>激活函数使用：Sigmoid 函数</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312061559329.png" alt="123" style="zoom:67%;" />g(z)&#x3D;1&#x2F;(1+e^(-z) )∈(0,1)</p><h3 id="多分类"><a href="#多分类" class="headerlink" title="多分类"></a>多分类</h3><p>激活函数使用： softmanx函数</p><p>y<del>i</del>&#x3D;e^(u_i)^&#x2F;(∑<del>(j&#x3D;1)</del>^k^e(u<del>j</del> ) )≥0</p><p>∑<del>(i&#x3D;1)</del>^k^y<del>i</del> &#x3D;1</p><h3 id="岭回归"><a href="#岭回归" class="headerlink" title="岭回归"></a>岭回归</h3><p>基本思想：针对普通的损失函数1&#x2F;2N ∑<del>i</del>(w^T x<del>i</del>-y<del>i</del> )^2^ ,希望在使它结果最小的同时，w^2^的大小也能够限制，即满足w^2^ &lt;&#x3D; C ,这样可以限制解的空间，帮助解决过拟合问题。（L2 正则，当超参数λ越大，约束越强）</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092110022.png" alt="image-20231209211023902"></p><h3 id="套索回归"><a href="#套索回归" class="headerlink" title="套索回归"></a>套索回归</h3><p>基本思想：在优化1&#x2F;2N ∑<del>i</del>(w^T x<del>i</del>-y<del>i</del> )^2^的同时，满足 |w| &lt;&#x3D; c,即希望解比较稀疏（因为现实中存在很多特征是无用的），其非0 项不超过c个，在实际中，往往使用 |w| &lt;&#x3D; c 作为替代品。（L1正则）</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092112081.png" alt="image-20231209211225034"></p><h3 id="支持向量机方法（SVM）"><a href="#支持向量机方法（SVM）" class="headerlink" title="支持向量机方法（SVM）"></a>支持向量机方法（SVM）</h3><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092116628.png" alt="image-20231209211600567" style="zoom: 33%;" /><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092131513.png" alt="image-20231209213149466"> 当 a<del>i</del> 不等于 0 的数据才会被称为支持向量，这些数据点其实就是距离分割平面最近的那些点，所以比较少。支持向量机就相当于是由支持向量计算分隔方案的算法。</p><p> 支持向量机解的稀疏性: 训练完成后, 大部分的训练样本都不需保留, 最终模型仅与支持向量有关</p><p>对于数据不是线性可分的，可以把约束放松</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312092141971.png" alt="image-20231209214152921"></p><p>对于<strong>不存在一个能正确划分两类样本的超平面</strong>的方案：</p><p><strong>将样本从原始空间映射到一个</strong>更高维的特征空间**,** <strong>使得样本在这个特征空间内线性可分</strong>.</p><p>如果最后算出有 r 个 a<del>i</del> ≠ 0 ，那么只需要计算 r 次核函数的内积操作，就可以判断出点 x 的类别。</p><h2 id="决策树模型"><a href="#决策树模型" class="headerlink" title="决策树模型"></a>决策树模型</h2><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>一个树结构的每个<strong>中间节点对数据的某一个特征进行判断</strong>，根据判断结果的不同指向相应的子节点。<strong>每一个叶子节点，代表的是对符合所有从根节点到该叶子节点路径上判断条件的数据给出的一个预测值</strong>。这种树称为决策树</p><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><ol><li>初始化一个根节点，对应所有的训练数据</li><li>选择一个特征，设置一个分割条件</li><li>依据该条件构造根的两个叶子，每个叶子对应一部分数据</li><li>重复以上步骤，直到到达一定的终止条件</li></ol><h5 id="如何选择最优划分属性"><a href="#如何选择最优划分属性" class="headerlink" title="如何选择最优划分属性"></a>如何选择最优划分属性</h5><p>一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，结点的纯度越高越好</p><h5 id="属性划分方法"><a href="#属性划分方法" class="headerlink" title="属性划分方法"></a>属性划分方法</h5><h6 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h6><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051339414.png" alt="image-20231205133948308">信息增益越大，则意味着使用属性 a 来进行划分所获得的纯度提升越大</p><p>示例：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051357754.png" alt="image-20231205135711675"><em><em>该数据集包含17个训练样本，</em>|y|</em><strong>&#x3D;2，其中正例占</strong>p**<del>1</del>&#x3D;8&#x2F;17 ，反例占<strong>p<del>2</del></strong>&#x3D;9&#x2F;17 ，计算得到根结点的信息熵为:<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051357951.png" alt="image-20231205135749888"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051406814.png" alt="image-20231205140658747"></p><p>存在的问题：信息增益对可取数目较多的属性有所偏好（因为可取的数目越多，划分的就越细，子集合也越多，子集也就更纯，但这样的决策树不具有泛化能力）</p><h6 id="增益率"><a href="#增益率" class="headerlink" title="增益率"></a>增益率</h6><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051447591.png" alt="image-20231205144722527"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051447440.png" alt="image-20231205144734402"></p><p>IV(a) 称为属性 a 的“固有值” ，属性 a 的可能取值数目越多，那么 IV(a)通常越大</p><p>存在的问题：对可取值数目较少的属性有所偏好</p><h6 id="基尼指数"><a href="#基尼指数" class="headerlink" title="基尼指数"></a>基尼指数</h6><p>数据集D的纯度可用“基尼值”来度量</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051508825.png" alt="image-20231205150827763">Gini(D) 越小，数据集 D 的纯度越高</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312051509017.png" alt="image-20231205150946971">选择使划分后基尼指数最小的属性作为最优划分属性</p><p>在ID<del>3</del>决策树中以信息增益作为准则来划分属性</p><p>在 C4.5 中使用的是先从候选划分属性中找出信息增益高于平均水平的属性，再从中选取增益率最高的</p><p>CART采用“基尼指数”来选择划分属性（减少对数运算）</p><h4 id="过拟合处理方法：剪枝"><a href="#过拟合处理方法：剪枝" class="headerlink" title="过拟合处理方法：剪枝"></a>过拟合处理方法：剪枝</h4><h5 id="预剪枝"><a href="#预剪枝" class="headerlink" title="预剪枝"></a>预剪枝</h5><p>在决策树的训练过程中加入限制条件，避免违反这些条件的分割</p><h6 id="准则"><a href="#准则" class="headerlink" title="准则"></a>准则</h6><ul><li>限制树的最大深度</li><li>限制树的最大叶子数目</li><li>限制每片叶子最少的样本数</li><li>…</li></ul><h5 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h5><p>先训练一个规模足够大的决策树，然后再删去多余树的分支</p><h6 id="准则-1"><a href="#准则-1" class="headerlink" title="准则"></a>准则</h6><ul><li>该子树没能使验证集上的误差有所减少</li><li>该子树不包含有足够大分割增益的分割</li><li>…</li></ul><h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>模型相对简单，具有较好的解释性，但是预测效果比不上更高级的模型</p><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h2><p>集成学习思想：是集合一系列弱模型的预测结果，从而实现更稳定，表现更好的模型</p><h4 id="两种集成方式"><a href="#两种集成方式" class="headerlink" title="两种集成方式"></a>两种集成方式</h4><h5 id="平行的集成方式"><a href="#平行的集成方式" class="headerlink" title="平行的集成方式"></a>平行的集成方式</h5><p>引导聚集方法(Bagging（ bootstrap aggregating）) </p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312021934718.png" alt="image-20231202193428587"></p><h6 id="例子：随机森林"><a href="#例子：随机森林" class="headerlink" title="例子：随机森林"></a>例子：随机森林</h6><p>训练多个决策树，为避免在训练中多个决策树给出相同的预测，随机森林在训练每个决策树时会引入一定的随机性</p><p>特点：</p><ul><li>训练每个决策树时，随机选取部分训练数据进行训练（子树之间独立）</li><li>在每次分割叶子节点时，随机选取特征的一个子集，从该子集中选取最优的分割条件</li></ul><p>预测结果选择：</p><ul><li>回归问题：预测输出为所有决策树预测的<strong>均值</strong></li><li>分类问题：对所有决策树的预测类别进行投票，<strong>得票最高的类别</strong>作为输出结果</li></ul><h6 id="关于随机性的探讨"><a href="#关于随机性的探讨" class="headerlink" title="关于随机性的探讨"></a>关于随机性的探讨</h6><ul><li>特征&#x2F;数据集采样会降低单个决策树的效果，但这样能增加不同决策树之间的独立性与差异性</li><li>将许多这样的决策树以这种方式组合起来，通常能够得到比不引入随机性更好的单个决策树效果。差异较大的模型互相弥补了各自的短板</li></ul><p><strong>关于为什么要采取有放回的采样</strong></p><ul><li><strong>引入多样性：</strong> 允许同一样本在不同的子集中出现，这导致了每个决策树都是在略有不同的数据子集上训练的。这增加了每个决策树的多样性，有助于防止过拟合。如果使用无放回抽样，每个子集的样本都是独立的，可能导致每个决策树过度拟合于特定的子集。</li><li><strong>减小方差：</strong> 由于每个决策树的训练集都是通过有放回抽样生成的，因此每个树都是在略微不同的数据集上训练的。当多个决策树组成随机森林时，它们的预测会取平均值，从而减小了预测的方差，提高了整体模型的稳定性。</li><li><strong>处理大量特征：</strong> 随机森林通常应用于高维数据，有放回抽样可以使每个决策树使用不同的特征子集进行训练。这有助于每个树专注于不同的特征，提高了整体模型对于特征的利用效率。</li></ul><h5 id="串行的集成方式"><a href="#串行的集成方式" class="headerlink" title="串行的集成方式"></a>串行的集成方式</h5><p>提升算法 (Boosting)</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312021935533.png" alt="image-20231202193502462"></p><p>基础模型是一个一个训练的，后一个的训练依赖以前基础模型的训练结果</p><h6 id="例子：梯度提升决策树（GBDT）"><a href="#例子：梯度提升决策树（GBDT）" class="headerlink" title="例子：梯度提升决策树（GBDT）"></a>例子：梯度提升决策树（GBDT）</h6><p>基本思想：不断训练新的决策树，以弥补已经训练好的决策树的误差</p><p>和随机森林的区别：各个子模型之间存在更强的依赖关系</p><h6 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h6><ul><li>新子树拟合已有子模型的结果相对于数据标签的残差或负梯度，子树间不独立</li><li>多颗子树不断提升集成模型总体的效果</li><li>目前针对表格数据最有竞争力模型之一，使用非常广泛</li></ul><h4 id="理解集成学习的优势"><a href="#理解集成学习的优势" class="headerlink" title="理解集成学习的优势"></a>理解集成学习的优势</h4><ul><li>特征会存在冗余性</li><li>可以从多个视图挖掘信息</li><li>集成学习中的每个基础模型可以充分挖掘分给它的那部分特征，然后一起整合预测结果</li></ul><h4 id="bagging-bootstrap-aggregating-和boosting-的区别"><a href="#bagging-bootstrap-aggregating-和boosting-的区别" class="headerlink" title="bagging(bootstrap aggregating) 和boosting 的区别"></a>bagging(bootstrap aggregating) 和boosting 的区别</h4><ul><li>bagging是将训练样本从数据集中多次抽取，构建多个弱学习器，每个学习器给与的权重是一样的，而boosting 是在训练期间迭代构建强学习器，对于误差小的学习器给与更大的权重</li><li>bagging随机抽取多组样本集合，分别训练不同的模型，采取并行模式，多个模型同时运行，对于回归任务输出结果采取平均值，对于分类任务采取投票</li><li>boosting 使用多个模型，采用串行模式，每次迭代都对上一次的模型进行改进。boosting 会根据错误率不断调整样例的权重，错误率越大则权重越大</li></ul><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><p>选择relu 不选择sigmoid ：sigmoid 函数在远离0 点的时候导数非常小，影响优化,relu 的梯度十分好求，对于优化是十分方便的</p><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><h2 id="自然语言处理"><a href="#自然语言处理" class="headerlink" title="自然语言处理"></a>自然语言处理</h2><h3 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h3><p>概念：用来评估一个句子或短语有多“像”是一个自然语言的工具。如果一个句子或短语更符合自然语言的表达方式，那么该句子或短语在语言模型的打分就应该更高；反之，则更低。</p><p>语言模型可以用到许多实用的场景，如文本纠错、翻译、语言生成等</p><h4 id="概率模型：n-gram-模型"><a href="#概率模型：n-gram-模型" class="headerlink" title="概率模型：n-gram 模型"></a>概率模型：n-gram 模型</h4><p>用字符序列代表句子，该字符序列的概率越大，则该序列更像自然语言。通常采用链式法则计算概率：P(c1c2c3)&#x3D;p(c1)*p(c2|c1)*p(c3|c1c2)</p><p>链式法则的优点：</p><ul><li>条件独立性</li><li>无后效性</li></ul><p>但这存在一个问题，对于很长的句子，越靠后的词的条件概率就会非常复杂，它对应的子序列在一个文章出现的概率也就越低，需要搜集更多的文章才能准确估计这个子序列的出现频率。所以一般采用n-gram 模型来解决这个问题。</p><p><strong>目的：简化条件概率计算</strong></p><p>近似假设 P(c<del>i</del>│c<del>1</del>…c<del>i-1</del> )≈P(c<del>i</del>|c<del>i-n+1</del>…c<del>i-1</del>)</p><p><strong>条件概率仅依赖前</strong>n-1<strong>个字符</strong></p><p><strong>对于固定</strong>n&#x3D;k<strong>，称之为</strong>k**-gram**模型</p><p>k&#x3D;1**: unigram model **    P(c<del>1</del>…c<del>N</del> )&#x3D;∏<del>i</del>P(c<del>i</del>)， 即每一个字相互独立，又称背包模型，bag-of-words）</p><p><strong>举例：</strong>P(清华大学)≈P(清)P(华)P(大)P(学)</p><p>k&#x3D;2**: Markov model** （马尔可夫模型）, bi-gram model</p><p>k<strong>越大，</strong>n**-gram**方法越精确</p><p><strong>词序列随着</strong>k指数变多，需要更多的语料才能准确估计概率</p><h4 id="最大似然估计MLE"><a href="#最大似然估计MLE" class="headerlink" title="最大似然估计MLE"></a>最大似然估计MLE</h4><p>用语料中的频率来近似概率。显然，搜集的语料越多，得到的概率就会越准确。从统计学角度可以证明，当语料有限时，是最“精准”的概率估计方式，这种方式就叫作最大似然估计。</p><h4 id="困惑度-preplexity"><a href="#困惑度-preplexity" class="headerlink" title="困惑度(preplexity)"></a>困惑度(preplexity)</h4><p>困惑度越低的语言模型生成语言的句子质量越高</p><p>PP(c<del>1</del>…c<del>N</del>)&#x3D;P(c<del>1</del>…c<del>N</del> )^(-1&#x2F;N)^</p><p>对数困惑度</p><p>log⁡PP(c<del>1</del>…c<del>N</del>)&#x3D;-1&#x2F;N ∑<del>i</del>log⁡P(c<del>i</del>|c<del>1</del>…c<del>i-1</del>) </p><h4 id="字模型和词模型"><a href="#字模型和词模型" class="headerlink" title="字模型和词模型"></a>字模型和词模型</h4><p><strong>字模型：</strong></p><ul><li><strong>常见汉字约2500</strong>字；建模与计算简单</li><li><strong>相对不精确，需要比较复杂的模型</strong></li></ul><p><strong>词模型：</strong></p><ul><li><strong>相对精确</strong></li><li><strong>词量巨大（海量专业名词），新词不断产生，需要分词</strong></li></ul><h4 id="中文和英文的差别"><a href="#中文和英文的差别" class="headerlink" title="中文和英文的差别"></a>中文和英文的差别</h4><p>英文更关注的是词性的区别，没有分词困扰</p><p>英文和中文的主要区别：</p><ul><li>英文中有大量的特定名词</li><li>英文中存在缩写与连写</li><li>英文中有不同的词性变换</li></ul><h3 id="向量语义"><a href="#向量语义" class="headerlink" title="向量语义"></a>向量语义</h3><h5 id="分布假设"><a href="#分布假设" class="headerlink" title="分布假设"></a>分布假设</h5><p><strong>单纯基于频率计算的模型忽略了词的语义信息</strong>。所以人们想到了分布假设。分布假设认为，两个词的词义越相似，那么它们在自然语言中出现的分布会越相近。以青菜白菜举例，二者都会经常出现在有关吃饭、菜谱等有关话题的句子中。</p><p>从分布假设角度来说，如果要表达一个词的意义，可以利用上下文的分布作为该词的特征。</p><h5 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h5><p>用向量（一些实数）来表达一个词的语义</p><p>近义词则对应向量空间距离近</p><h5 id="相似度"><a href="#相似度" class="headerlink" title="相似度"></a>相似度</h5><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312081651753.png" alt="image-20231208165126665" style="zoom:50%;" /><p>词义相近的词则夹角小（相似度大）</p><p>词义差距大的词则夹角大（相似度低）</p><h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>思路：采用机器学习方式，把词向量作为参数进行优化求解</p><h4 id="连续词袋模型CBOW"><a href="#连续词袋模型CBOW" class="headerlink" title="连续词袋模型CBOW"></a>连续词袋模型CBOW</h4><p>给定词的上下文，推断该词出现的概率</p><p>P(w│c<del>(-k)</del>…c<del>k</del> )&#x3D;∏<del>1≤|i|≤k</del> P(w|c<del>i</del>) 将联合分布分解为每一个上下文词C<del>i</del>与当前词w的条件概率的乘积。在这种独立假设的情况下，上下文中的每一个词的先后关系并不被模型考虑。</p><p>这种忽略词出现顺序只在乎是否出现的方法称为词袋模型。</p><p>对于CBOW来说，给定一个上下文，输出对于整个词表所有词的概率分布，我们希望对于候选词的概率要大一点。</p><p>余弦相似度<strong>：</strong>cos⁡&lt;w,c<del>i</del>&gt; ≈  w^T^ c<del>i</del></p><p>P(+│w,c<del>i</del> )：特定词适合和上下文一起出现的概率</p><p>P(-│w,c<del>i</del>): 其它词不适合和上下文一起出现的概率</p><p>P(-│w,c<del>i</del>)+P(+│w,c<del>i</del> )&#x3D;1</p><p>P(+│w,c<del>i</del> )&#x3D;σ(w^T^c<del>i</del> )&#x3D;1&#x2F;(1+e^ (-w^T^ c<del>i</del> ) )</p><p>P(-│w,c<del>i</del> )&#x3D;1-σ(w^T^ c<del>i</del>)&#x3D;e^ (-w^T^ c<del>i</del> ) &#x2F;(1+e^ (-w^T^ c<del>i</del> ) )</p><p>最后的结果：P(+│w,c<del>(-k)</del>…c<del>k</del> )&#x3D;∏_(1≤|i|≤k )P(+|w,c<del>i</del>)</p><p><strong>一般为了数值稳定，优化对数概率</strong></p><p>L(w,c)&#x3D;∑_((w,c<del>i</del> )∈D^+) log⁡P(+|w,c_i) </p><h4 id="跳字模型skip-gram"><a href="#跳字模型skip-gram" class="headerlink" title="跳字模型skip-gram"></a>跳字模型skip-gram</h4><p>给定一个词，推断其上下文词出现的概率，计算P(c<del>-k</del>…c<del>k</del>|w),也采用独立假设。</p><p>word2vec 只考虑得到的词向量，并不在意得到的结果，词向量是word2vec的参数。需要注意到，skip-gram 比CBOW 更不容易过拟合。</p><h3 id="基于神经网络的语言模型"><a href="#基于神经网络的语言模型" class="headerlink" title="基于神经网络的语言模型"></a>基于神经网络的语言模型</h3><p>能够在计算条件概率时将每个词的语义信息考虑进去。</p><p><strong>采用神经网络表示条件概率</strong>P(w|w<del>(i-1)</del>)</p><h4 id="基于神经网络的bigram模型"><a href="#基于神经网络的bigram模型" class="headerlink" title="基于神经网络的bigram模型"></a>基于神经网络的bigram模型</h4><p>找到一个函数，给定一个词表L&#x3D;{w^1^,w^2^,w^3^,…,w^n^},以及当前词w<del>i-1</del>作为输入，输出一个长度为 v 的向量 o &#x3D; (o<del>1</del>,o<del>2</del>,…,o<del>n</del>),其中o<del>i</del>,表示词表中第i 个词出现在w<del>i-1</del>之后的概率。</p><p> 损失函数</p><p>L(θ;w<del>(i-1)</del>,w<del>i</del> )&#x3D;-log⁡P(w<del>i</del> |w<del>(i-1)</del>)&#x3D;log y<del>wi</del>&#x3D;log⁡(∑<del>(k∈L)</del> exp⁡(β<del>k</del> ) )-β<del>wi</del></p><p><strong>为什么此时不需要构造负例进行训练了？</strong></p><p><strong>多分类问题的</strong>softmax<strong>输出自然包含了负例</strong></p><h4 id="基于神经网络的ngram-模型"><a href="#基于神经网络的ngram-模型" class="headerlink" title="基于神经网络的ngram 模型"></a>基于神经网络的ngram 模型</h4><p>与bigram 模型的不同之处在于输入的不在是一个词，而是n 个词拼接在一起，其余部分相同。<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101653052.png" alt="image-20231210165312862" style="zoom:40%;" /></p><h4 id="基于LSTM-的语言模型"><a href="#基于LSTM-的语言模型" class="headerlink" title="基于LSTM 的语言模型"></a>基于LSTM 的语言模型</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101656355.png" alt="image-20231210165625275"></p><h3 id="基于神经网络的机器翻译"><a href="#基于神经网络的机器翻译" class="headerlink" title="基于神经网络的机器翻译"></a>基于神经网络的机器翻译</h3><p><strong>翻译问题本质上寻找一个函数</strong>f<strong>，给定某种语言的句子</strong>X<strong>，输出另一种语言中对应的句子</strong></p><h4 id="基于LSTM的seq2seq模型"><a href="#基于LSTM的seq2seq模型" class="headerlink" title="基于LSTM的seq2seq模型"></a>基于LSTM的seq2seq模型</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101659134.png" alt="image-20231210165922032"></p><p>编码器将输入序列 x 编码为一个特征向量 h ,再通过解码器将 h 转化为完整的目标句子 y </p><h4 id="beam-search"><a href="#beam-search" class="headerlink" title="beam search"></a>beam search</h4><p><strong>保留</strong>k<strong>个贪心候选序列</strong></p><p>对于每次时间步解码器的输出选择前k 个概率最大的词，然后作为下一个时间步的输入之一</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312101704128.png" alt="image-20231210170400980"></p><h4 id="基于注意力机制的seq2seq模型"><a href="#基于注意力机制的seq2seq模型" class="headerlink" title="基于注意力机制的seq2seq模型"></a>基于注意力机制的seq2seq模型</h4><p>LSTM模型的缺点：</p><ul><li><p>对于固定维度的句子，LSTM模型都只会将其映射到一个固定维度的特征向量 h ,但这个固定维度是模型的瓶颈，会限制模型的表达能力</p></li><li><p>输入词和输出词之间是有对应关系的，LSTM不能表达出这这关系</p></li></ul><p>加上注意力机制，输出状态会更加和它相似的输入状态，对于相似的注意力更高，特征向量是由自身对于编码器的不同时间步的输出的不同权重和得到的，所以每一个输出词都有自己对应的特征向量h</p><h4 id="Transformer-模型"><a href="#Transformer-模型" class="headerlink" title="Transformer 模型"></a>Transformer 模型</h4><p>对于输入序列的每一个词X<del>i</del>,都有对应的 k<del>i</del> , q<del>i</del> ,v<del>i</del> ,代表的意思是关键字，询问和值，使用这三个变量是为了提取出输入向量x<del>i</del> 中的不同信息。</p><p>k<del>i</del>,q<del>i</del>,v<del>i</del>&#x3D;[W<del>k</del> x<del>i</del>,W<del>q</del> x<del>i</del>,W<del>v</del> x<del>i</del>]</p><p>α<del>j</del>&#x3D;softmax(β<del>j</del>&#x2F;√d)     β<del>j</del>&#x3D;q<del>i</del>^T^ k<del>j</del></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312111425410.png" alt="image-20231211142548287"></p><p>输出的是值向量的加权求和</p><h5 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h5><p><strong>计算</strong>m<strong>个维度为</strong>d&#x2F;m<strong>的独立自注意力分布和自注意力特征</strong></p><p>α<del>j</del>^l^&#x3D;softmax((β<del>j</del>^l^)&#x2F;√(d&#x2F;m)); β<del>j</del>^l^&#x3D;q<del>i</del>^l^^T  k<del>j</del>^l^;h<del>SA</del>^l^&#x3D;∑<del>j</del> α<del>j</del>^l^ v<del>j</del>^l^</p><p><strong>合并得到最终的维度为</strong>d<strong>的自注意力特征</strong>h_SA&#x3D;concat(h<del>SA</del>^1^,…,h<del>SA</del>^m^)</p><p><strong>多头注意力机制与原自注意力机制</strong>计算量相同</p><p><strong>每个独立头维度降低，表达能力相对减弱</strong></p><p><strong>保证整体不消耗额外计算</strong></p><p><strong>可以完整使用矩阵运算完成</strong></p><p><strong>自注意力机制本质上为线性运算</strong>，所以还需要添加非线性层。</p><h5 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h5><p><strong>自注意力机制有顺序不变性</strong>，所以需要添加位置编码来提供位置信息</p><p>p<del>i</del><strong>表示</strong>x<del>i</del><strong>所处的位置信息，以</strong>[x<del>i</del>,p<del>i</del>]<strong>作为自注意力模块的输入</strong></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312111453789.png" alt="image-20231211145359645"></p><h3 id="语言模型预训练"><a href="#语言模型预训练" class="headerlink" title="语言模型预训练"></a>语言模型预训练</h3><h5 id="GPT-generative-pretained-Transformer-与BERT-bidirectional-encoder-representation-from-transformer-的区别和联系"><a href="#GPT-generative-pretained-Transformer-与BERT-bidirectional-encoder-representation-from-transformer-的区别和联系" class="headerlink" title="GPT (generative pretained Transformer )与BERT(bidirectional encoder representation from transformer)的区别和联系"></a>GPT (generative pretained Transformer )与BERT(bidirectional encoder representation from transformer)的区别和联系</h5><p>联系</p><ul><li>二者都使用了transformer架构</li><li>都采用预训练的策略，在大规模文本语料库上进行预训练，然后在特定任务上进行微调。</li></ul><p>区别</p><ul><li>GPT采用的是自回归的预训练模型。模型在预训练阶段通过预测下一个词的方式学习语言表示</li><li>bert 采用的是掩码语言模型的预训练目标。在输入序列中，随机掩盖一些词，并要求模型预测这些被掩盖的词</li><li>GPT采用的是单侧从左向右预测的方式，bert通过掩盖部分输入词，实现双向上下文理解，使得模型在预测缺失词时能够同时考虑左右两侧的信息</li><li>GPT是生成是模型，可以生成连续的文本序列，bert主要用于特征提取，通常在预训练后将其输出用于下游任务的任务特定模型</li></ul>]]></content>
      
      
      <categories>
          
          <category> 大学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像分割论文学习</title>
      <link href="/2023/11/20/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/11/20/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h6 id="从暑假开始阅读论文的，大多数是和医学图像分割相关的，在这里做一个记录和总结。"><a href="#从暑假开始阅读论文的，大多数是和医学图像分割相关的，在这里做一个记录和总结。" class="headerlink" title="从暑假开始阅读论文的，大多数是和医学图像分割相关的，在这里做一个记录和总结。"></a>从暑假开始阅读论文的，大多数是和医学图像分割相关的，在这里做一个记录和总结。</h6><h2 id="CE-Net（CVPR）"><a href="#CE-Net（CVPR）" class="headerlink" title="CE-Net（CVPR）"></a><strong>CE-Net</strong>（CVPR）</h2><p>[<a href="https://arxiv.org/abs/1903.02740">1903.02740] CE-Net: Context Encoder Network for 2D Medical Image Segmentation (arxiv.org)</a></p><p>网络框架<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/CE-Net.png"></p><h2 id="SLEX-Net-JBHI"><a href="#SLEX-Net-JBHI" class="headerlink" title="SLEX-Net(JBHI)"></a>SLEX-Net(JBHI)</h2><p><a href="https://ieeexplore.ieee.org/document/9511297">Hematoma Expansion Context Guided Intracranial Hemorrhage Segmentation and Uncertainty Estimation | IEEE Journals &amp; Magazine | IEEE Xplore</a></p><h2 id="UNet"><a href="#UNet" class="headerlink" title="UNet++"></a>UNet++</h2><p>[<a href="https://arxiv.org/abs/1807.10165">1807.10165] UNet++: A Nested U-Net Architecture for Medical Image Segmentation (arxiv.org)</a></p><h2 id="TransUNet（CVPR）"><a href="#TransUNet（CVPR）" class="headerlink" title="TransUNet（CVPR）"></a>TransUNet（CVPR）</h2><p>[<a href="https://arxiv.org/abs/2102.04306v1">2102.04306v1] TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation (arxiv.org)</a></p><p>网络框架<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/TransUNet.png"/></p><h3 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h3><ol><li>CNN和transformer 相结合，其中，先采用CNN进行卷积操作，得到图像浅层的特征，再输入到transformer模块中，transformer 模块（待补充，这部分存疑）</li></ol><h2 id="S4CVnet-ECCV"><a href="#S4CVnet-ECCV" class="headerlink" title="S4CVnet(ECCV)"></a><strong>S4CVnet</strong>(ECCV)</h2><p><a href="https://link.springer.com/chapter/10.1007/978-3-031-25082-8_28">When CNN Meet with ViT: Towards Semi-supervised Learning for Multi-class Medical Image Semantic Segmentation | SpringerLink</a></p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/S4CVnet.png"><h2 id="AAS-DCL-FSS-ECCV-小样本"><a href="#AAS-DCL-FSS-ECCV-小样本" class="headerlink" title="AAS-DCL-FSS(ECCV 小样本)"></a>AAS-DCL-FSS(ECCV 小样本)</h2><p>地址：<a href="https://link.springer.com/chapter/10.1007/978-3-031-20044-1_24">Dual Contrastive Learning with Anatomical Auxiliary Supervision for Few-Shot Medical Image Segmentation | SpringerLink</a></p><p>code:  <a href="https://github.com/cvszusparkle/AAS-DCL_FSS">https://github.com/cvszusparkle/AAS-DCL_FSS</a></p><p>网络框架<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AAS-DCL-FSS.png"></p><h3 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h3><ol><li>采用小样本学习</li><li>提出双对比学习分割模型（原型和上下文两个角度），利用没有目标类的医学图像的解剖辅助信息来提高特征的可辨别性和数据的利用率</li><li>设计约束迭代预测模型来优化查询图像的分割</li></ol><h3 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h3><h4 id="对于无标签的图像的处理"><a href="#对于无标签的图像的处理" class="headerlink" title="对于无标签的图像的处理"></a>对于无标签的图像的处理</h4><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS1.png"><p>大概过程：从所有的无标签的图像中随机抽取K个图像，对于挑选的每一个图像，从它们各自对应的超像素中选一个类别进行二值化作为它们的伪标签；同时，提取这K个图像的特征，最后将特征信息和伪标签一起送入到对比学习模块。</p><h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS_decoder.png" alt="image-20231121184433841"></p><p>输入：一个支持查询对，支持集是图像和标签，查询集是图像；</p><p>如上图所示，有五层卷积层（提取不同尺度的特征），其中前四层的具体模块如下:</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS2.png" alt="image-20231121185650736"></p><p>最后一层相比于前四层，少了重复的一段</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS5.png" alt="image-20231121185919786"></p><p>在查询分支，还增有sSE模块，<strong>首先，叠加支持特征中的各个特征图的重要信息，得到一个挤压过的特征图，再通过sigmoid激活，得到的权重矩阵与查询特征进行点乘，最后得到了校准过的查询特征</strong>，具体内容可以查看这篇博客：[【精选】<a href="https://blog.csdn.net/qq_38932073/article/details/115869172">小样本医学图像]‘Squeeze &amp; Excite’ Guided Few-Shot Segmentation of Volumetric Images_医学小样本三维分割_XL_Dylan的博客-CSDN博客</a></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS6.png" alt="image-20231121191540524"></p><h4 id="双支路对比学习"><a href="#双支路对比学习" class="headerlink" title="双支路对比学习"></a><strong>双支路对比学习</strong></h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS7.png" alt="image-20231121200052095"></p><p>双对比学习模块</p><p>b:语义对比学习</p><p>c:事先嵌入</p><p>d:原型对比学习</p><h5 id="语义对比学习"><a href="#语义对比学习" class="headerlink" title="语义对比学习"></a>语义对比学习</h5><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS8.png" alt="image-20231121200440391"></p><p>对于无标签的切片特征图和支持集的切片特征图处理过程：首先通过ssE模块，使不同通道上的信息进行交互，再通过平均池化和重塑，最终得到无标签切片和支持集的向量。</p><p>而对于查询集，则比上面二者多了一步PE模块。最终得到的是查询集的特征向量，通过<strong>Info NCE loss</strong>，要使得查询向量与支持向量尽可能相似，与无标签的向量尽可能不相似。损失函数公式如下：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS9.png" alt="image-20231122133146643"></p><h5 id="事先嵌入（PE）模块"><a href="#事先嵌入（PE）模块" class="headerlink" title="事先嵌入（PE）模块"></a>事先嵌入（PE）模块</h5><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS10.png" style="height:300px;width=50px"><p>M：将支持特征与查询特征中的前景特征通过余弦相似度算法得到二者之间的相似度再加上归一化操作生成的相似度矩阵（即计算查询特征与支持特征标记区域的相似度）</p><p>P^s：聚合支持特征图前景区域和背景区域的信息。</p><p>最后将M、扩展过的p^s和查询特征f^q进行连接再卷积，得到前景信息增强过的查询特征f^q&#96;</p><h5 id="原型对比学习"><a href="#原型对比学习" class="headerlink" title="原型对比学习"></a>原型对比学习</h5><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS11.png" alt="image-20231122135319262"></p><p>有两个部分，cpcl &amp;ppcl </p><h6 id="Class-level-Prototypical-Contrastive-Learning"><a href="#Class-level-Prototypical-Contrastive-Learning" class="headerlink" title="Class-level Prototypical Contrastive Learning"></a>Class-level Prototypical Contrastive Learning</h6><p>与PE模块中得到P^s的步骤相同，聚合支持集和无标签集各自的前后区域信息，然后查询集通过PE模块加强前景信息后，通过自适应平均池化调整好形状，再进行Info NCE loss 计算，其中支持集是正样本，无标签集是父样本。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS12.png" alt="image-20231122140944889"></p><h6 id="Patch-level-Prototypical-Contrastive-Learning"><a href="#Patch-level-Prototypical-Contrastive-Learning" class="headerlink" title="Patch-level Prototypical Contrastive Learning"></a>Patch-level Prototypical Contrastive Learning</h6><p>提出PPCL有两个原因:一是由于MAP获得的类级原型会过滤掉目标类周围的其他背景信息，这些背景信息可能被用作负样本来增强目标类周围语义的判别性,二因为对比学习在很大程度上需要大量的有效学习负样本，只考虑类级原型是不够的。</p><p>没读懂论文这一部分的具体实现。大概意思是通过设置阈值来划分支持集，将低于阈值的支持集的patch 聚合的特征向量作为负样本？<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS13.png" alt="image-20231122143827609"></p><h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS14.png" alt="image-20231122151434350"></p><h4 id="Constrained-Iterative-Prediction"><a href="#Constrained-Iterative-Prediction" class="headerlink" title="Constrained Iterative Prediction"></a>Constrained Iterative Prediction</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/AASDCLFSS15.png" alt="image-20231122160202560"></p><p>优化分割结果</p><p>大致过程：首先利用通用分类器的结果作为初始预测掩码，将其与解码器尾部的查询特征融合并发送到PE模块，在PE模块后通过一系列卷积运算获得新的分割结果。PE的应用是利用支持特征的引导信息来促进预测查询掩码的融合。新的查询预测将通过SCC中的以下约束来确定更新。</p><p>代码目前还没有公布训练和测试代码，只公布了模型代码</p><h2 id="CANet-CVPR-2019-小样本"><a href="#CANet-CVPR-2019-小样本" class="headerlink" title="CANet(CVPR 2019 小样本)"></a>CANet(CVPR 2019 小样本)</h2><p>[<a href="https://arxiv.org/abs/1903.02351">1903.02351] CANet: Class-Agnostic Segmentation Networks with Iterative Refinement and Attentive Few-Shot Learning (arxiv.org)</a></p><p>代码：<a href="https://github.com/icoz69/CaNet">https://github.com/icoz69/CaNet</a></p><h3 id="网络框架"><a href="#网络框架" class="headerlink" title="网络框架"></a>网络框架</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/CANet1.png" alt="image-20231124103524420"></p><h4 id="Dense-Comparison-Module"><a href="#Dense-Comparison-Module" class="headerlink" title="Dense Comparison Module"></a>Dense Comparison Module</h4><p>由两部分组成：表征特征提取部分（Feature Extractor）和特征对比部分（Dense Comparision）</p><h5 id="Feature-Extractor"><a href="#Feature-Extractor" class="headerlink" title="Feature Extractor"></a>Feature Extractor</h5><p>目的：从CNN中获取不同层次的特征表示</p><p>使用ResNet-50 作为特征提取器的backbone,backbone 在Imagenet 上进行预训练</p><p>通常，较低层的特征信息与浅层的信息相关，比如边缘和颜色，对于较高层的特征则与对象及的概率相关，比如对象类别。</p><p>模型需要适应未知的类，模型在训练过程中可能学习不到未知类别对应的特征，但在中间层的特征中，这些特征可能构成未知类的部分特征。</p><p>ResNet 分为四个块，对应四个不同的表示级别，模型选择由block2和block3生成的特征进行特征比较，舍去block3 之后的模块。在block2后的层使用空洞卷积来保持特征图的空间分辨率。</p><p>支持分支和查询分支都使用相同的特征提取器，在训练过程中，将ResNet 的权重保持固定</p><h5 id="Dense-Comparision"><a href="#Dense-Comparision" class="headerlink" title="Dense Comparision"></a>Dense Comparision</h5><p>使用前景区域的全局平均池化特征映射压缩为特征向量。从支持集获得全局特征向量后，将该向量与查询分支生成的特征图中所有的空间位置连接起来（是为了进行对比），然后通过卷积块来进行对比</p><h4 id="Iterative-Optimization-Module"><a href="#Iterative-Optimization-Module" class="headerlink" title="Iterative Optimization Module"></a>Iterative Optimization Module</h4><p>同一类别内的外观会存在差异，模型无法准确分割图像中的整个物体。最初始的预测是关于物体大致位置的重要线索，所以提出一个迭代优化模块，对预测结果进行迭代优化。</p><p>模块的输入是密集比较模块生成的特征图和上次迭代的预测掩码，通过拼接的方式会导致特征分布不匹配（因为第一次的时候没有预测掩码），采取的是采用残差形式的合并。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/CAnet2.png" alt="image-20231124112054813"></p><p>其中，x为密集比较模块的输出特征；y<del>t-1</del> 为上一步迭代得到的预测掩码，M<del>t</del> 为残差块的输出。使用ASPP（空间金字塔模块） 来捕获多尺度信息</p><h4 id="Attention-Mechanism-for-k-shot-Segmentation"><a href="#Attention-Mechanism-for-k-shot-Segmentation" class="headerlink" title="Attention Mechanism for k-shot Segmentation"></a>Attention Mechanism for k-shot Segmentation</h4>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论文学习</title>
      <link href="/2023/08/22/%E8%84%91%E5%87%BA%E8%A1%80%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/08/22/%E8%84%91%E5%87%BA%E8%A1%80%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="Attention-U-Net"><a href="#Attention-U-Net" class="headerlink" title="Attention U-Net:"></a>Attention U-Net:</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ul><li>提出新的关住门（AG）模型，</li></ul><h1 id="IHA-Unet"><a href="#IHA-Unet" class="headerlink" title="IHA-Unet"></a>IHA-Unet</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ul><li><u>（First of all,as the complexity of brain structure, the feature of ICH is very similar to that of skull, which brings interference to  both feature extraction and segmentation.）</u>ICH 的特征与颅骨的特征非常相似，这会给特征的提取和分割带来干扰。</li><li><u>(Additionally, the location and size of cerebral hemmorrhage lesions on CT images are varible,which further reduces the accuracy of segmentation, for example, the erea of intraventricular hemorrhage is long strip and slightly low density, while the one of brainstem hemorrhage is very smal</u>l.) 脑出血病灶在CT图像上的位置和大小不一，这引入降低了分割的准确性，比如脑室内出血区域呈长条形，密度略低，而脑干出血区域很小。并且，ICH种类繁多，很难构建一个能处理不同规模ICH的深度学习模型。</li><li><u>Moreover, due to continuous convolutions, a puny cerebral hematoma(early ICH) tends to be lightly lost and difficult to recover</u>.(持续的卷积，微小的脑血肿容易丢失且难以恢复)</li><li><u>there is a large imbalance between foreground and background pixels, which further reduces the accuracy of segmentation</u>.(前景和背景像素之间存在较大的不平衡，这也会降低分割的准确性)，在作者使用的公开数据集中，大部分的血肿面积占整个图像的比例不超过3%。</li><li><u>In order to address the aforementioned issues, in this paper, a novel end-to-end model is developed, which can work well on tiny intracerebral hemorrhage segmentation</u>.(为了解决上述问题，本文提出了一种新颖的端到端脑出血分割模型，该模型能够很好低进行微小脑出血分割)</li><li><strong><u>In response to the loss of information on tiny hemaomas, we introduce the Residual Hybrid Atrous Convolution(RUAC) strategy to integrate atrous convolution.</u> (针对微小血肿信息丢失问题，引入残差混合亚光卷积对亚光卷积进行积分)</strong> ，<strong>Generally speaking ,in the shallower layers, the larger the receptive field is, the sparser the convolution kernel is , it cannot cover critical local information and consequently a great part of the detailed information diminishes, which is very detrimental for tiny intracerebral hemorrhage segmentation.(在较浅的层中，感受野越大，卷积核越稀疏，无法覆盖关键的局部信息，从而导致很一部分详细信息衰减，这对微小脑出血分割是十分不利的。)</strong>  <strong>Thus, by configuring RHAC modules with different atrous rates at different encoder stages, multi-scale contextual information can be aggregted without loss of resolution.(因此，通过在不同的编码区阶段配置具有不同属性率的RHAC模块，可以在不损失分辨率的情况下聚合多尺度上下文信息)</strong> <strong>Specifically,for shallow stages, we set  a smaller reception field,compared with deep ones.(具体来说，在浅层阶段，设置了一个较小的接受场，而不是在深层阶段）</strong> <strong>Besides, the multi-object optimization function is utilized as the loss during training process to handle the imbalance problem of the imbalance problem of the foreground and background pixels.(在训练过程中，利用多目标优化函数作为损失来处理前景和背景像素的不平衡问题) Furthermore, in order to facilitate better segmentation performance of intracranial hemorrhage, we also employ intermediate supervision with a lightweight heed.(为了更好地分割颅内出血，还采用了轻量级头部的中间监督)</strong></li></ul><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/image-20230824164544764.png"/><p><strong>We employ the modified U-shape structure to improve segmentation performance, where pretained ResNet-34 is exploited as the encoder backbone.</strong> </p><p><strong>Before information fusion, Residual Hybrid Atros Convolution(RHAC) strategy is adopted to gather multiscale context information of the objects.</strong></p><p><strong>In addition to the first stage of the encoder, RHAC modules with different configurations are placed at each stage of the encoder as the network deepens.</strong></p><p><strong>Then, a spatial context information extractor is utilized to capture spatial information of local and global features.</strong></p><p><strong>UItimately, the final result is acquired by inserting an attention mechanism with intermediate supervision in the decoding stage to recover the resolution of feature map.</strong></p><p>IHA-Net 结构：采用改进的U形结构来提高分割性能，利用预训练的ResNet-34 作为编码器主干。在信息融合之前，采用残差混合属性卷积策略收集目标的多尺度上下文信息，除了编码器的第一级外，随着网络的加深，在编码器的每一级都放置了不同配置的RHAC模块，利用空间上下文信息提取器捕获局部和全局特征的空间信息。最后，通过在解码阶段插入具有中间监督的注意力机制来恢复特征映射的分辨率，从而获得最终结果。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202308241750688.png" alt="image-20230824175059643">     血肿大小的分布，其中每个血肿在每组中的比例相对平衡。横轴表示血肿区域与图像区域的比值。纵轴表示一定大小的血肿样本与整个样本的比例</p><h3 id="x3D-x3D-A-Residual-Hybrid-Atrous-Convolution-module-x3D-x3D"><a href="#x3D-x3D-A-Residual-Hybrid-Atrous-Convolution-module-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;A. Residual Hybrid Atrous Convolution module&#x3D;&#x3D;"></a>&#x3D;&#x3D;A. Residual Hybrid Atrous Convolution module&#x3D;&#x3D;</h3><p><strong>Since the cerebral hemorrhage data used for training and test many come from the patients with different types of ICH at different stages, there may be different scales of ICH regions.(来自不同类型、不同阶段的脑出血患者，脑出血区域可能有不同的尺度)</strong> **Considering that the continuous stacked down-sampling in the encoder  stages will lead to a severe loss of relevant detailed information, small object information cannot be reconstructed.(考虑到编码器阶段的连续叠加下采样会导致相关详细信息的严重丢失，小目标信息无法重构)**，Hence, how to dwindle the loss of crucial information on small objects has become an important issue we need to address.To mitigate the influe-nce of the above problems, we propose the Residual Hybrid Atrous Convolution(-RHAC) module to further capture multi-scale context features for intracanial he-morrhage of different sizes.<strong>RHAC block is integrated by several hybrid atrous co-nvolutional layers, max-pooling layers and residual connections. （RHAC块由多个混合属性卷积层、最大池化层和残差连接组成）</strong> </p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202308241837033.png" alt="image-20230824183747982"></p><p>four parallel branches ,three of which are equipped with convolutions with diferent diated ratios, and one branch with the max-pooling layer.A 1x1 convolution is added after convolution of each branch to realize integration of information, and then the rectified linear unit(ReLU) is used as the activation function. Moreover, we utilize residual connections to optimize network.</p><p>​With the depth of networks increasing, the resolution of feature maps gradually decreases, resulting in information contained in the shallow network sparser.Therefore, it is necessary to use a larger receptive fielf to aggregate spatial  information.**Compared with conventional convolution, the noteworthy of atrous convolution is the expansion of the receptive field without alteringf feature maps resolution, which is helpful to reduce the loss of semantic information to some extent.(在不改变特征图分辨率的情况下扩展了感受野，这在一定程度上有助于减少语义信息的损失)**，However, only using the convolution with large dilated rate is not benefiical to the segmentation of small objects.(仅使用膨胀率较大的卷积不利于小目标的分割)，In order to reduce the loss of the internal data and spatial hierarchical information of small objects, we configure RHAC modules with different atrous rates  in the different stages of encoders.<strong>To be specific,a relatively small receptive field is formed by configuring a smaller atrous rate in the shallow layer, and a larger atrous rateis configured in the deep layer to yield a relatively large receptive  field.(浅层配置较小的属性率形成相对较小的感受野，深层配置较大的属性率形成相对较大的感受野)<strong>，</strong>In this way, the information of small targets in the shallow layer is less likely to be lost.RHAC block ,more abstract features of different sizes for hemorrhage regions are extracted and multi-scale context information is fully captured, enhancing local information prediction.(提取出血区域更多不同大小的抽象特征，充分捕获多尺度上下文信息，增强局部信息预测)</strong> </p><h3 id="x3D-x3D-B-Spatial-Context-Information-Extractor-x3D-x3D"><a href="#x3D-x3D-B-Spatial-Context-Information-Extractor-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;B. Spatial Context Information Extractor&#x3D;&#x3D;"></a>&#x3D;&#x3D;B. Spatial Context Information Extractor&#x3D;&#x3D;</h3><p><strong>From above modules, more semantic information of shallow features is introduced. To further embed more spatial inforamtion in the deep features, we utilize like spatial pyramid pooling for accurate ICH region segmentation from the CE-Net model .(从以上模块中引入了更多浅层特征的语义信息，为了进一步在浅层特征中嵌入更多的空间信息，利用空间金字塔池从CE-Net 模型中精准分割ICH 区域)</strong> <strong>The same feature map is pooled in parallel with different sizes, so that the feature map of different sizes can be obtained. Then low-resolution feature map is upsampled directly via bilinear interpolation to obtain a feature map with the same resolution as the original feature map. Finally, the different levels of features are concatenated together as a final pyramid pooling global feature.(对相同的特征图进行不同大小的并行池化，得到不同大小的特征图，对低分辨率特征图直接进行双线性插值上采样，得到与原特征图分辨率相同的特征图，最后，将不同级别的特征连接在一起，形成最终的金字塔全局特征)</strong> 为了减少参数，池化后进行1x1 卷积，用于捕获空间上下文信息的感受野大小分别为2x2,3x3,5x5,6x6.<strong>This module enables the fusion of local and global features, enriching the expression capability of feature maps and facilitating the segmentation of hematoma regions with  different sizes in images.该模块实现了局部和全局特征的融合，丰富了特征图的表达能力，便于对图像中不同大小的血肿区域进行分割。</strong></p><h3 id="x3D-x3D-C-Attention-Mechanism-x3D-x3D"><a href="#x3D-x3D-C-Attention-Mechanism-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;C. Attention Mechanism&#x3D;&#x3D;"></a>&#x3D;&#x3D;C. Attention Mechanism&#x3D;&#x3D;</h3><p>Apart from designing various networks architectures,the mechanism named attention, which is motivated by the fact that human recognize what we see not by a full scene but firstly focus on the most important objects has widely been applied in CNN architectures and obtained splendid performance.**As a simple but effective structure that improves the representational power and attention of relevant features by suppressing non-important features, among these attention mechanisms, channel-wise attention and spatial-wise attention inherit the capability of capturing inter relationship to denote which part of features is more considered by CNN.(作为一种简单而有效的结构，通过抑制非重要特征来提高相关特征的表征能力和关注度，在这些注意机制中，通道型注意和空间型注意继承了捕捉相互关系的能力，以表示CNN更关注哪一部分特征)**，Likewise,these two attention mechanisms attempt to emphasize input features by separately placing channel-wise and spatial-wise attention modules where the attention output is rescored to input feature with element-wise multiplication. </p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202308242217452.png" alt="image-20230824221720373"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202308242220455.png" alt="image-20230824222015418"></p><h3 id="x3D-x3D-D-Intermediate-Supervision-x3D-x3D"><a href="#x3D-x3D-D-Intermediate-Supervision-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;D. Intermediate Supervision&#x3D;&#x3D;"></a>&#x3D;&#x3D;D. Intermediate Supervision&#x3D;&#x3D;</h3><p>When training the deep network, with the update of the model parameters using the gradient descent method, the loss of the output layer is greatly reduced by multiple layers, namely, the gradient vanishing occurs.** To solve this problem, we calculate the output loss at the soecified decoder stage, and sum up these loss values when updating the weights.(计算指定解码阶段的输出损失，并在更新权值时将这些损失值相加)**，This method guarantees the normal update of the parameters and suppresses the gradient vanishing problem.<strong>Among them, the calculated losses in these network layers become auxiliary losses.The purpose of intermediate supervision is to allow more adequate training of the shallow layers to avoid gradient disappearance and slow convergence.(这些网络层的计算损耗成为辅助损耗，中间监督的目的是额为了对浅层进行更充分得到训练，以避免梯度消失和收敛缓慢)</strong> <strong>By using additional intermediate supervision signals to help intensify tiny hematoma features and prevent information loss in deep layers(通过使用额外的中间监督信号来帮助强化微小的血肿特征，防止深层的信息丢失)</strong></p><h3 id="x3D-x3D-E-Multi-object-function-for-joint-optimization-x3D-x3D"><a href="#x3D-x3D-E-Multi-object-function-for-joint-optimization-x3D-x3D" class="headerlink" title="&#x3D;&#x3D;E. Multi-object function for joint optimization&#x3D;&#x3D;"></a>&#x3D;&#x3D;E. Multi-object function for joint optimization&#x3D;&#x3D;</h3><p><strong>in our dataset, due to the proportion of hematomas in the images is much smaller than the background, the positive and negative classes of samples are extremely unbalanced.In this situation, the ordinarily used pixel-by-pixel cross-entropy loss function may give rise the training procedure dominated by the negative class, thereby  reducing the effectiveness of the network.（血肿比例远远小于背景，正负类样本极不平衡）使用逐像素交叉熵损失函数可能会产生负类主导的训练过程，从而降低网络的有效性）</strong>，The focal object optimization function was originally used in the field of object detection to solve the sample imbalance problem by <strong>reducing the weights  of a large number of negative samples in the training process.</strong></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202308251246319.png" alt="image-20230825124612217"></p><h1 id="Directional-Connectivity-based-Segmentation-of-Medical-Images"><a href="#Directional-Connectivity-based-Segmentation-of-Medical-Images" class="headerlink" title="Directional Connectivity-based Segmentation of Medical Images"></a>Directional Connectivity-based Segmentation of Medical Images</h1><h2 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h2><p>提出观点：生物标记物分割中的解剖一致性对许多医学图像分析任务至关重要，实现解剖一致性的一个有用的方式是将像素连通性纳入像素间关系模型；以往的连通性建模工作忽略了潜在空间中丰富的通道方向信息，作者证明了从共享潜在空间中有效地纠缠方向子空间可以显著增强基于连接的网络中的特征表示；作者提出一种用于分割的方向连接建模方案，该方案可以解耦、跟踪和利用整个网络的方向信息。代码：<strong><a href="https://github.com/Zyun-Y/DconnNet">https://github.com/Zyun-Y/DconnNet</a></strong></p><h2 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h2><p>图像中的解剖一致性可以用拓扑属性来表示，例如像素连通性和邻接性。基于图的方法通过直接建模像素或区域之间的相互信息，长期以来被用于纠正拓扑和几何误差，但这些技术通常依赖于人工定义的先验，不易推广到各种各样的应用中。</p><p>​典型的分割网络将问题建模为纯像素分类任务，并使用分割掩码作为唯一的标签。但是，这种逐像素建模方案是次优的，因为它没有直接利用像素间关系和几何属性。因此，这些模型在预测中可能导致低空间相干性（即对具有相似空间特征的相邻像素的预测不一致）。特别是，当应用与高噪声&#x2F;伪像的医疗数据时，较低的空间一致性可能导致拓扑问题。长期以来，像素连通性的概念被用于确保数字图像中分离和连通性的基本拓扑对偶性。使用连通性掩码作为训练标签有几个优点：<strong>在问题建模方面，使用连通性掩码本质上将问题从逐像素分类更改为连通性预测，从而对感兴趣的像素之间的拓扑表示进行建模和增强；在标签表示方面，连接掩码在三个方面提供了更多的信息：首先，连接掩码在像素的连接之间存储分类信息，并且具有像素间关系感知；其次，稀疏表示边缘像素；第三，它包含了丰富的定向信息渠道。因此，用连通性掩模训练的网络在潜空间中既有分类特征（通过连通性来体现），也有方向性特征，每一种特征都形成了特定的次替空间。</strong></p><p>在以往的研究中，这两组特征是通过共享的网络路径同时学习的，这可能导致潜在空间高度耦合，并引入冗余。另外，从共享潜在空间中有效分离有意义的子空间已被证明可以有效地解释特征之间的依赖性&#x2F;独立性。</p><p><strong>受潜在空间解纠缠思想启发，作者提出一种新的基于方向连通性的分割网络，从共享潜在空间中解纠缠方向子空间，并利用提取的方向特征来增强整体数据表示。解缠过程是由一种称为子路径方向激励(SDE)的基于子路径切片的模块进行的。利用具有两个自顶向下交互解码流的交互式特征空间解码器(IFD)，以从粗到精的方式应用基于方向的特征增强。最后，我们提出了一种新的基于标签大小分布的加权方案，缓解了医疗数据集中常见的数据不平衡问题。通过对不同公共医学图像分析基准的实验，证明了DconnNet相对于其他最先进方法的优越性。</strong></p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>像素连通性</strong></p><p>在拓扑学中，像素连通性描述了相邻像素之间的相互关系。在深度学习中的图像分割中，基于连通性的分割网络使用连通性掩码作为标签，定义为8通道掩码，每个通道表示原始图像上的一个像素是否是与其相邻像素在特定方向上属于同一兴趣类。</p><p><strong>潜在空间</strong></p><p><strong>在这项工作中，我们利用连通性掩模的固有特性，提出了一种简单而有效的基于子路径切片的方法来从共享潜在空间中解纠缠方向子空间，随后用T-SNE可视化演示了我们解纠缠过程的有效性</strong></p><h2 id="方法-1"><a href="#方法-1" class="headerlink" title="方法"></a>方法</h2><p>由于不同像素类别和方向之间的连通性，基于连通性的网络潜在空间中存在两组特征:分类方向。每一组特征在潜在空间中形成其特定的子空间。在单路径连接网络中，这两个子空间是高度耦合的(图2)，从而产生低判别特征。我们证明了有效的解纠缠和方向空间的有效利用可以增强连通性模型中的整体特征表示。</p><p>在连接掩码中，不同的通道表示像素连接的不同方向。因此，随着网络的深入，它自然会在通道之间存储定向信息。基于这一特性，可以通过通道操作捕获和操纵方向特征。具体来说，我们提出SDE从潜在空间中解出通道方向特征，然后提出IFD提取不同层的方向嵌入，并使用它们以自关注的方式增强整体特征表示。</p><p>DconnNet的总体结构如图所示。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>MT-UNet论文学习</title>
      <link href="/2023/07/31/MT-UNet%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/07/31/MT-UNet%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li><p>指出U-Net在医学图像分割任务有巨大成功，但模型缺乏远程依赖能力</p></li><li><p>transformer 固有的self-attention模块能够拥有捕获远程相关性的能力，但transformer 通常依赖于大规模的预训练，并且有很高的计算复杂度，并且，self-attention 只能对单个样本的自注意力建模，忽略了整个数据集的潜在相关性</p></li><li><p>提出一个新的transformer模块MTM，用于同时进行样本内部和样本之间注意力学习；MTM首先通过精心设计的局部-全局高斯加自注意（LGG-SA）有效地计算注意力，然后通过外部注意EA挖局样本之间的相互联系</p></li><li><p>在MTM基础上，构建一个U型混合tansformer的MT-UNet模型，用于医学图像的精准分割</p></li><li><p>代码：<a href="https://github.com/Dootmaan/MT-UNet">https://github.com/Dootmaan/MT-UNet</a></p></li></ol><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>        U-Net面临所有CNN都面临的问题：缺乏建模远程相关性的能力，主要是因为卷积运算的固有局部性。许多研究尝试用Transformer来解决这个问题。self-attention 是transformer 的关键组成部分，它可以对输入的tokens之间互相进行计算，所以transformer 可以处理远程依赖关系。虽然有一些工作取得了令人满意的结果，但transformer 通常依赖于大规模的预训练，对于给使用它带来了不便。并且，SA的计算复杂度为二次的，会降低图像的处理速度。同时，SA有忽略样本之间相关性的局限性，这有很大的提升空间。</p><p>        为了获取更好的局部感知和更低的计算成本，作者重新设计了SA，再与外部注意EA集成，同时管理样本内和样本间的相关性。</p><p>        作者提出在细粒度的局部上下文中执行局部SA，而全局SA仅在粗粒度的全局上下文中执行，因为在大多数视觉任务中，近区域之间的视觉依赖性比远区域之间的视觉依赖性强。局部上下文中的信息通常更加相关，因此需要更细致地处理。而全局上下文中的信息则相对不那么相关，因此可以使用粗粒度来处理。这样可以提高计算效率，同时保留重要的信息。例如，在一张图片中，图像中心的物体与周围物体之间的关系通常比与边缘物体之间的关系更为重要。（细粒度和粗粒度是指处理信息时所采用的精细程度。细粒度意味着对信息进行更细致、更详细的处理，而粗粒度则意味着对信息进行更粗略、更概括的处理。例如，在图像处理中，细粒度可能意味着对每个像素进行处理，而粗粒度则可能意味着对图像的整个区域进行处理。）</p><p>        计算全局注意图时，作者使用轴向注意来减少计算量，并进一步引入可学习的高斯矩阵来增强附近标记的权重。</p><p>        transformer 需要大规模预训练的主要原因在于它对问题的结构没有先验知识。所以作者在设计MT-UNet 时，使用convolution stem 作为浅层的特征提取器，为分割任务设置结构先验。</p><p>        <strong>总结：设计MTM用于同时进行样本间和样本内部亲和力的学习；提出LGG-SA，在细粒部的局部上下文和粗粒度的局部上下文依次执行SA，还引入可学习的高斯矩阵来强调每个查询周围的附近区域；构建了一个用于医学图像分割的混合Transformer UNet 。</strong></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="整体结构设计"><a href="#整体结构设计" class="headerlink" title="整体结构设计"></a>整体结构设计</h3><p>        网络基于编码器-解码器结构，在解码时使用跳跃连接来保持低级特征。MTM仅用在空间尺寸较小的深层，用于减少计算成本，上层仍然使用经典的卷积运算（希望关注初始层上的局部关系，它们包含了更多高分辨率的细节，）；在模型之前引入一些卷积操作，对于相对较小的医学图像数据集很有帮助； 对于所有的transformer模块，都遵循一个2步卷积&#x2F;反卷积内核来实现下采样&#x2F;上采样以及通道扩展&#x2F;压缩。</p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/MT-UNet.png" title="" alt="" width="446"><h4 id="Mixed-Transformer-Module"><a href="#Mixed-Transformer-Module" class="headerlink" title="Mixed Transformer Module"></a>Mixed Transformer Module</h4><p>        MTM由LGG-SA和EA组成，LGG-SA用于模拟不同粒度的短期和长期依赖关系，EA用于挖掘样本间相关性。该模块具有更好的视觉任务性能和更低的时间复杂度，可以取代原有的Transformer编码器。</p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/MT-UNet_mixtrans.png" title="" alt="" width="392"><h5 id="Local-Global-Gaussion-Weighted-Self-Attention"><a href="#Local-Global-Gaussion-Weighted-Self-Attention" class="headerlink" title="Local-Global Gaussion-Weighted Self-Attention"></a>Local-Global Gaussion-Weighted Self-Attention</h5><p>        LGG-SA 完美体现了集中计算的思想。与传统的SA对于所有的token给与同等的关注不同，LGG-SA 通过使用Local-Global 策略和高斯掩码，可以更多地关注附近区域。LGG-SA可以提高模型性能和节省计算资源。</p><img title="" src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/mixtrans_attention.png" alt="" width="524"><h6 id="Local-Global-Self-Attention"><a href="#Local-Global-Self-Attention" class="headerlink" title="Local-Global Self-Attention"></a>Local-Global Self-Attention</h6><p>        SA的目的在于捕获输入序列中所有实体之间的相互联系。在计算机视觉中，近区域之间的相关性往往比远区域更重要，在计算注意力时，不需要为更远的区域付出同样的代价。提出Local-Global Self-Attention，<strong>LocalSA计算每个window内的注意力，然后将每个window 内的token 聚合为一个全局token，表示窗口的主要信息</strong>（对于聚合函数，作者尝试了stride convolution、Max Pooling等方法，其中轻量级动态卷积(Lightweight Dynamic convolution, LDConv)表现最好。在对整个特征映射进行下采样后，可以以更少的开销执行全局SA。）输入的特征映射为H*W *C,将窗口大小设置为P，整个过程如下图： </p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/localSA.png" title="" alt="" width="412"><h6 id="Gaussian-Weighted-Aixal-Attention"><a href="#Gaussian-Weighted-Aixal-Attention" class="headerlink" title="Gaussian-Weighted Aixal Attention"></a>Gaussian-Weighted Aixal Attention</h6><p>        与使用原始SA的LSA不同，对于GSA，使用了GWAA，G-WAA 通过一个可学习的高斯矩阵来增强每个查询对于附件标记的感知，同时使用的轴向注意具有较低的时间复杂度。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/MT-UNet%20%E5%85%AC%E5%BC%8F1.png"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/MT-UNet%20%E5%85%AC%E5%BC%8F2.png"></p><h5 id="External-Attention"><a href="#External-Attention" class="headerlink" title="External Attention"></a>External Attention</h5><p>        提出外部注意来解决SA无法利用不同样本之间的关系的问题，与self-attention 使用每个样本自己的线性变换来计算注意力分数不同，<strong>EA中，所有的样本共享两个记忆单元MK 和MV，用来描述整个数据集的最基本的信息。</strong> 另外，对Q使用额外的线性映射来扩大通道，提高该模块的表示学习能力。</p><p>由于EA的时间复杂度为O(n)，因此MT-UNet的总体时间复杂度保持为O(n√n)。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TranUNet学习</title>
      <link href="/2023/07/29/TranUNet%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/07/29/TranUNet%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><ol><li><p><strong>由于卷积运算的固有局部性，U-Net 通常在显示建模长期依赖关系方面表现出局限性</strong>（卷积运算的固有局部性：卷积运算只能捕捉到输入数据中相邻区域之间的关系，这个区域大小取决于卷积核大小，这种局部性使得卷积神经网络能够很好地处理图像等具有局部相关性的数据，但也限制了它在处理远程依赖关系方面的能力。比如，一个像素的标签可能和它周围很远的像素有关，但卷积神经网络可能难以捕捉到这种远程依赖关系；虽然U-Net通过层次的堆叠来扩大感受野，但U-Net 仍然可能在长期依赖关系方面表现出局部性）</p></li><li><p><strong>Transformer具有全局的自注意力机制，但由于缺乏低层次细节，可能导致定位能力有限</strong>（低层次细节捕捉：可以通过卷积神经网络的下采样来扩大感受野，从而来捕捉到图像中的纹理、边缘和颜色变化等信息；定位能力有限：模型不能捕捉到图像的边缘信息，它很难准确地划分不同的物体）</p></li><li><p>提出了TransUNet，它兼有transformer和U-Net的优点，作为医学图像分割的强大替代方案。<strong>Transformer对来自卷积神经网络(CNN)特征映射的标记化图像补丁进行编码，作为提取全局上下文的输入序列；解码器对编码特征进行采样，然后将其与高分辨率CNN特征图相结合，以实现精确的定位</strong>。</p></li><li><p><strong>Transformer可以作为医学图像分割任务的强编码器，结合U-Net通过恢复局部空间信息来增强更精细的细节</strong>。</p></li></ol><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><ol><li><p>经过transformer得到特征直接进行上采样得到的结果令人不满意（原因：transformer 将输入的图像作为一维的序列，所有的阶段都在专注于获取全局上下文信息，缺乏对于详细定位信息的低分辨率特征，直接上采样到全分辨率不能有效恢复这些信息，导致分割结果粗糙）</p></li><li><p>提出了医学图像分割框架TransUNet，从序列到序列预测的角度建立了自关注机制。为了弥补transformer带来的特征分辨率损失，TransUNet 采用CNN-transformer混合架构，结合CNN特征的详细高分辨率空间信息和Transformer的全局上下文。</p></li><li><p>受U-Net结构启发，将transformer得到的self-attention 特征上采样后与从编码路径得到的不同高分辨率CNN特征相结合，从而实现精准定位。与基于CNN的self-attention方法相比，基于transformer的架构可以更好地利用self-attention。另外，更密集地结合低级特征通常会导致更好的分割精度。</p></li></ol><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><ol><li><p>将CNN与自注意机制结合</p></li><li><p>TransUNet是第一个基于transform的医学图像分割框架，它建立在非常成功的ViT之上</p></li></ol><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/TransUNet.png"></p><h3 id="代码学习"><a href="#代码学习" class="headerlink" title="代码学习"></a>代码学习</h3><h4 id="TransNet-网络设置"><a href="#TransNet-网络设置" class="headerlink" title="TransNet 网络设置"></a>TransNet 网络设置</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_transNet</span>(<span class="params">n_classes</span>):</span><br><span class="line">    img_size = <span class="number">512</span> <span class="comment">#设置图像大小</span></span><br><span class="line">    vit_patches_size = <span class="number">16</span> <span class="comment">#设置patch 个数</span></span><br><span class="line">    vit_name = <span class="string">&#x27;R50-ViT-B_16&#x27;</span></span><br><span class="line"></span><br><span class="line">    config_vit = CONFIGS_ViT_seg[vit_name] <span class="comment">#获取配置</span></span><br><span class="line">    config_vit.n_classes = n_classes</span><br><span class="line">    config_vit.n_skip = <span class="number">3</span></span><br><span class="line">    <span class="keyword">if</span> vit_name.find(<span class="string">&#x27;R50&#x27;</span>) != -<span class="number">1</span>: <span class="comment">#如果名称中包含R50</span></span><br><span class="line">        <span class="comment">#重新设置patch大小</span></span><br><span class="line">        config_vit.patches.grid = (<span class="built_in">int</span>(img_size / vit_patches_size), <span class="built_in">int</span>(img_size / vit_patches_size))</span><br><span class="line">    <span class="comment">#创建ViT_seg网络</span></span><br><span class="line">    net = ViT_seg(config_vit, img_size=img_size, num_classes=n_classes)</span><br><span class="line">    <span class="keyword">return</span> net</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="Resnet50-ViT-B-x2F-16-模型的配置"><a href="#Resnet50-ViT-B-x2F-16-模型的配置" class="headerlink" title="Resnet50 + ViT-B&#x2F;16 模型的配置"></a>Resnet50 + ViT-B&#x2F;16 模型的配置</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_r50_b16_config</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the Resnet50 + ViT-B/16 configuration.&quot;&quot;&quot;</span></span><br><span class="line">    config = get_b16_config()<span class="comment">#获取Vit模型的基本配置</span></span><br><span class="line">    config.patches.grid = (<span class="number">16</span>, <span class="number">16</span>)<span class="comment">#设置patch大小</span></span><br><span class="line">    config.resnet = ml_collections.ConfigDict()<span class="comment">#创建一个configdict</span></span><br><span class="line">    <span class="comment">##来存储resnet</span></span><br><span class="line">    config.resnet.num_layers = (<span class="number">3</span>, <span class="number">4</span>, <span class="number">9</span>)<span class="comment">#设置resnet模型层数</span></span><br><span class="line">    config.resnet.width_factor = <span class="number">1</span> <span class="comment">#设置resnent的宽度因子</span></span><br><span class="line"></span><br><span class="line">    config.classifier = <span class="string">&#x27;seg&#x27;</span> <span class="comment">#分割</span></span><br><span class="line">    <span class="comment">#设置预训练模型路径</span></span><br><span class="line">    config.pretrained_path = <span class="string">&#x27;../model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz&#x27;</span></span><br><span class="line">    config.decoder_channels = (<span class="number">256</span>, <span class="number">128</span>, <span class="number">64</span>, <span class="number">16</span>)<span class="comment">#设置解码器通道</span></span><br><span class="line">    config.skip_channels = [<span class="number">512</span>, <span class="number">256</span>, <span class="number">64</span>, <span class="number">16</span>]<span class="comment">#设置skip通道</span></span><br><span class="line">    config.n_classes = <span class="number">2</span> <span class="comment">#设置类别数</span></span><br><span class="line">    config.n_skip = <span class="number">3</span>  <span class="comment">#设置skip 数目</span></span><br><span class="line">    config.activation = <span class="string">&#x27;softmax&#x27;</span> <span class="comment">#设置激活函数</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> config <span class="comment">#返回配置结果</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="ViT-B-x2F-16模型配置"><a href="#ViT-B-x2F-16模型配置" class="headerlink" title="ViT-B&#x2F;16模型配置"></a>ViT-B&#x2F;16模型配置</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_b16_config</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Returns the ViT-B/16 configuration.&quot;&quot;&quot;</span></span><br><span class="line">    config = ml_collections.ConfigDict() <span class="comment">#创建一个CondigDict()</span></span><br><span class="line">    <span class="comment">#对象来存储信息</span></span><br><span class="line">    <span class="comment">#设置patch大小</span></span><br><span class="line">    config.patches = ml_collections.ConfigDict(&#123;<span class="string">&#x27;size&#x27;</span>: (<span class="number">16</span>, <span class="number">16</span>)&#125;)</span><br><span class="line">    <span class="comment">#设置隐藏层大小</span></span><br><span class="line">    config.hidden_size = <span class="number">768</span></span><br><span class="line">    <span class="comment">#创建一个新的CondigDict 对象来存储Transformer的设置</span></span><br><span class="line">    config.transformer = ml_collections.ConfigDict()</span><br><span class="line">    <span class="comment">#设置Transformer 的MLP维度</span></span><br><span class="line">    config.transformer.mlp_dim = <span class="number">3072</span></span><br><span class="line">    <span class="comment">#设置头数</span></span><br><span class="line">    config.transformer.num_heads = <span class="number">12</span></span><br><span class="line">    <span class="comment">#设置层数</span></span><br><span class="line">    config.transformer.num_layers = <span class="number">12</span></span><br><span class="line">    <span class="comment">#设置dropout率</span></span><br><span class="line">    config.transformer.attention_dropout_rate = <span class="number">0.0</span></span><br><span class="line">    config.transformer.dropout_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    config.classifier = <span class="string">&#x27;seg&#x27;</span></span><br><span class="line">    config.representation_size = <span class="literal">None</span></span><br><span class="line">    config.resnet_pretrained_path = <span class="literal">None</span></span><br><span class="line">    config.pretrained_path = <span class="string">&#x27;../model/vit_checkpoint/imagenet21k/ViT-B_16.npz&#x27;</span></span><br><span class="line">    config.patch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">    config.decoder_channels = (<span class="number">256</span>, <span class="number">128</span>, <span class="number">64</span>, <span class="number">16</span>)</span><br><span class="line">    config.n_classes = <span class="number">2</span></span><br><span class="line">    config.activation = <span class="string">&#x27;softmax&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> config</span><br></pre></td></tr></table></figure><h4 id="VisionTransformer"><a href="#VisionTransformer" class="headerlink" title="VisionTransformer"></a>VisionTransformer</h4><p><code>forward</code> 方法接受一个输入张量 <code>x</code>，并返回分割结果。如果输入张量的通道数为 1，则将其重复三次以模拟 RGB 图像。然后，将输入张量传递给 Transformer 编码器，得到编码结果、注意力权重和特征图。接着，将编码结果和特征图传递给解码器，得到解码结果。最后，将解码结果传递给分割头，得到分割结果。</p><p><code>load_from</code> 方法接受一个权重字典，并使用这些权重来初始化模型的权重。方法首先复制权重字典中的权重到模型的相应部分。然后，处理位置嵌入的权重。如果位置嵌入的大小与模型中位置嵌入的大小相同，则直接复制权重；否则，根据情况调整位置嵌入的大小并复制权重。接着，初始化编码器的权重。最后，如果模型包含混合模型，则初始化混合模型的权重。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">VisionTransformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, img_size=<span class="number">512</span>, num_classes=<span class="number">2</span>, zero_head=<span class="literal">False</span>, vis=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(VisionTransformer, self).__init__()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.zero_head = zero_head <span class="comment">#设置分类器权重初始化是否为0</span></span><br><span class="line">        self.classifier = config.classifier</span><br><span class="line">        self.transformer = Transformer(config, img_size, vis)</span><br><span class="line">        self.decoder = DecoderCup(config)</span><br><span class="line">        <span class="comment">#print(&quot;inchannel:&quot;,config[&#x27;decoder_channels&#x27;][-1])</span></span><br><span class="line">        self.segmentation_head = SegmentationHead(</span><br><span class="line">            in_channels=config[<span class="string">&#x27;decoder_channels&#x27;</span>][-<span class="number">1</span>],</span><br><span class="line">            out_channels=config[<span class="string">&#x27;n_classes&#x27;</span>],</span><br><span class="line">            kernel_size=<span class="number">3</span>,</span><br><span class="line">        )</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">if</span> x.size()[<span class="number">1</span>] == <span class="number">1</span>:<span class="comment">#输入张量通道数为1</span></span><br><span class="line">            x = x.repeat(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#将其重复三次</span></span><br><span class="line">        <span class="comment">#将张量传递给Transformer编码器，得到编码结果，注意力权重和特征图</span></span><br><span class="line">        x, attn_weights, features = self.transformer(x)  <span class="comment"># (B, n_patch, hidden)</span></span><br><span class="line">        x = self.decoder(x, features)<span class="comment">#将编码结果和特征图传给</span></span><br><span class="line">        <span class="comment">#解码器,得到解码结果</span></span><br><span class="line">        <span class="comment">#将解码结果传递给分割头，得到分割结果</span></span><br><span class="line">        logits = self.segmentation_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_from</span>(<span class="params">self, weights</span>):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line"></span><br><span class="line">            res_weight = weights</span><br><span class="line">            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[<span class="string">&quot;embedding/kernel&quot;</span>], conv=<span class="literal">True</span>))</span><br><span class="line">            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[<span class="string">&quot;embedding/bias&quot;</span>]))</span><br><span class="line"></span><br><span class="line">            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[<span class="string">&quot;Transformer/encoder_norm/scale&quot;</span>]))</span><br><span class="line">            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[<span class="string">&quot;Transformer/encoder_norm/bias&quot;</span>]))</span><br><span class="line"></span><br><span class="line">            posemb = np2th(weights[<span class="string">&quot;Transformer/posembed_input/pos_embedding&quot;</span>])</span><br><span class="line"></span><br><span class="line">            posemb_new = self.transformer.embeddings.position_embeddings</span><br><span class="line">            <span class="keyword">if</span> posemb.size() == posemb_new.size():</span><br><span class="line">                self.transformer.embeddings.position_embeddings.copy_(posemb)</span><br><span class="line">            <span class="keyword">elif</span> posemb.size()[<span class="number">1</span>]-<span class="number">1</span> == posemb_new.size()[<span class="number">1</span>]:</span><br><span class="line">                posemb = posemb[:, <span class="number">1</span>:]</span><br><span class="line">                self.transformer.embeddings.position_embeddings.copy_(posemb)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                logger.info(<span class="string">&quot;load_pretrained: resized variant: %s to %s&quot;</span> % (posemb.size(), posemb_new.size()))</span><br><span class="line">                ntok_new = posemb_new.size(<span class="number">1</span>)</span><br><span class="line">                <span class="keyword">if</span> self.classifier == <span class="string">&quot;seg&quot;</span>:</span><br><span class="line">                    _, posemb_grid = posemb[:, :<span class="number">1</span>], posemb[<span class="number">0</span>, <span class="number">1</span>:]</span><br><span class="line">                gs_old = <span class="built_in">int</span>(np.sqrt(<span class="built_in">len</span>(posemb_grid)))</span><br><span class="line">                gs_new = <span class="built_in">int</span>(np.sqrt(ntok_new))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;load_pretrained: grid-size from %s to %s&#x27;</span> % (gs_old, gs_new))</span><br><span class="line">                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -<span class="number">1</span>)</span><br><span class="line">                zoom = (gs_new / gs_old, gs_new / gs_old, <span class="number">1</span>)</span><br><span class="line">                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=<span class="number">1</span>)  <span class="comment"># th2np</span></span><br><span class="line">                posemb_grid = posemb_grid.reshape(<span class="number">1</span>, gs_new * gs_new, -<span class="number">1</span>)</span><br><span class="line">                posemb = posemb_grid</span><br><span class="line">                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Encoder whole</span></span><br><span class="line">            <span class="keyword">for</span> bname, block <span class="keyword">in</span> self.transformer.encoder.named_children():</span><br><span class="line">                <span class="keyword">for</span> uname, unit <span class="keyword">in</span> block.named_children():</span><br><span class="line">                    unit.load_from(weights, n_block=uname)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> self.transformer.embeddings.hybrid:</span><br><span class="line">                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(res_weight[<span class="string">&quot;conv_root/kernel&quot;</span>], conv=<span class="literal">True</span>))</span><br><span class="line">                gn_weight = np2th(res_weight[<span class="string">&quot;gn_root/scale&quot;</span>]).view(-<span class="number">1</span>)</span><br><span class="line">                gn_bias = np2th(res_weight[<span class="string">&quot;gn_root/bias&quot;</span>]).view(-<span class="number">1</span>)</span><br><span class="line">                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)</span><br><span class="line">                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> bname, block <span class="keyword">in</span> self.transformer.embeddings.hybrid_model.body.named_children():</span><br><span class="line">                    <span class="keyword">for</span> uname, unit <span class="keyword">in</span> block.named_children():</span><br><span class="line">                        unit.load_from(res_weight, n_block=bname, n_unit=uname)</span><br></pre></td></tr></table></figure><h4 id="Transformer模块"><a href="#Transformer模块" class="headerlink" title="Transformer模块"></a>Transformer模块</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, config, img_size, vis</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        <span class="comment">#创建一个嵌入层</span></span><br><span class="line">        self.embeddings = Embeddings(config, img_size=img_size)</span><br><span class="line">        <span class="comment">#创建一个编码层</span></span><br><span class="line">        self.encoder = Encoder(config, vis)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids</span>):</span><br><span class="line">        <span class="comment">#将输入张量传递给嵌入层，得到嵌入输出和特征图</span></span><br><span class="line">        embedding_output, features = self.embeddings(input_ids)</span><br><span class="line">        <span class="comment">#将嵌入层输出传递给编码器，得到编码结果和注意力权重</span></span><br><span class="line">        encoded, attn_weights = self.encoder(embedding_output)  <span class="comment"># (B, n_patch, hidden)</span></span><br><span class="line">        <span class="comment">#返回编码结果、注意力权重和特征图</span></span><br><span class="line">        <span class="keyword">return</span> encoded, attn_weights, features</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>语义分割的指标</title>
      <link href="/2023/07/15/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E6%8C%87%E6%A0%87/"/>
      <url>/2023/07/15/%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%E7%9A%84%E6%8C%87%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h3 id="语义分割：像素级别的分类"><a href="#语义分割：像素级别的分类" class="headerlink" title="语义分割：像素级别的分类"></a>语义分割：像素级别的分类</h3><h2 id="常用的评价指标："><a href="#常用的评价指标：" class="headerlink" title="常用的评价指标："></a>常用的评价指标：</h2><ul><li><p>像素准确率（PA）</p></li><li><p>类别像素准确率（CPA）</p></li><li><p>类别平均像素准确率（MPA）</p></li><li><p>交并比（IoU)</p></li><li><p>平均交并比（MIoU)</p></li></ul><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><p>定义：混淆矩阵就是统计分类模型的分类结果。统计归对类，归错类的样本的个数，然后把结果放在一个表里展示出来</p><ul><li>准确率（Accuracy），对应：语义分割的像素准确率 PA<br>公式：Accuracy &#x3D; (TP + TN) &#x2F; (TP + TN + FP + FN)<br>意义：对角线计算。预测结果中正确的占总预测值的比例（对角线元素值的和 &#x2F; 总元素值的和）</li><li>精准率（Precision），对应：语义分割的类别像素准确率 CPA<br>公式：Precision &#x3D; TP &#x2F; (TP + FP) 或 TN &#x2F; (TN + FN)<br>意义：竖着计算。预测结果中，某类别预测正确的概率</li><li>召回率（Recall），不对应语义分割常用指标<br>公式：Recall &#x3D; TP &#x2F; (TP + FN) 或 TN &#x2F; (TN + FP)<br>意义：横着计算。真实值中，某类别被预测正确的概率</li></ul><p><strong>像素准确率</strong> &#x3D; 准确率    </p><p><strong>类别像素准确率</strong> &#x3D; 精准率</p><p><strong>类别平均像素准确率</strong> &#x3D; 累计CPA求平均 MPA &#x3D; sum (Pi) &#x2F; 类别数 </p><p>Pi&#x3D;对角线值&#x2F;对应列的像素总数</p><p><strong>交并比</strong> ：模型对某一类别预测结果和真实值的交集与并集的比值</p><p>IoU &#x3D; TP &#x2F; (TP + FP +FN)</p><p><strong>平均交并比</strong> ：对IoU求和再求平均</p><p>MIoU &#x3D; (IoU正例+IoU反例) &#x2F; 2</p><p>dice系数</p><p><img src="https://img-blog.csdnimg.cn/20200318134203976.png"></p><p>IoU指标</p><p><img src="https://img-blog.csdnimg.cn/20200318134528407.png"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>医学图像预处理</title>
      <link href="/2023/07/13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/"/>
      <url>/2023/07/13/%E5%8C%BB%E5%AD%A6%E5%9B%BE%E5%83%8F%E9%A2%84%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h2 id="二值化处理"><a href="#二值化处理" class="headerlink" title="二值化处理"></a>二值化处理</h2><p>将图片上的点置为0或255，整个图像就会呈现出明显的黑白效果。选取合适的阈值，来决定像素为0或者255。</p><h4 id="全局阈值"><a href="#全局阈值" class="headerlink" title="全局阈值"></a>全局阈值</h4><p>将图像上每一个像素都选用相同的阈值</p><h4 id="局部阈值"><a href="#局部阈值" class="headerlink" title="局部阈值"></a>局部阈值</h4><p>假定图像在一定区域内受到的光照比较接近。它用一个滑窗扫描图像，并取<strong>滑窗中心点</strong>亮度与滑窗内其他区域（称为邻域, neighborhood area）<strong>的亮度进行比较。如果</strong>中心点<strong>亮度高于</strong>邻域<strong>亮度，则将</strong>中心点标记为白色，否则标记为黑色。</p><h2 id="腐蚀操作"><a href="#腐蚀操作" class="headerlink" title="腐蚀操作"></a>腐蚀操作</h2><h2 id="膨胀"><a href="#膨胀" class="headerlink" title="膨胀"></a>膨胀</h2><p>侵蚀（Erosion）：侵蚀的基本思想就像土壤侵蚀一样，它侵蚀了前景物体的边界。这样内核可以在图像中滑动。只有当内核下的所有像素都为1时，原始图像中的像素才会被认为是1，否则会被侵蚀（变成0）。这样的结果就是，取决于内核的大小，边界附近的所有像素都会被丢弃。因此，前景对象的厚度或大小会减少。</p><p>膨胀（Dilation）：膨胀操作正好与腐蚀相反。这里，如果内核下至少有一个像素是“1”，那么该像素元素就是“1”。因此，它增加了图像中的白色区域。</p><p>形态学梯度（Morphological Gradient）：其实就是一幅图像膨胀和腐蚀的差别。结果看上去就像前景物体的轮廓。</p><p>礼帽（Top Hat）：原始图像与开运算之后得到的图像的差。</p><p>黑帽（Black Hat）：进行闭运算之后得到的图像与原始图像的差。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unet_learning</title>
      <link href="/2023/07/09/Unet-learning/"/>
      <url>/2023/07/09/Unet-learning/</url>
      
        <content type="html"><![CDATA[<h1 id="Medical-Image-Segmentation-Review"><a href="#Medical-Image-Segmentation-Review" class="headerlink" title="Medical Image Segmentation Review:"></a>Medical Image Segmentation Review:</h1><p>The Success of U-Net</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/Unet_apply.png" alt="image-20240124174158820"></p><p>Unet 框架应用于医学图像挑战的数量变化</p><p>Unet变体的类别如下</p><ol><li>增强跳跃连接</li><li>增强骨干网络</li><li>增强瓶颈结构</li><li>使用transformer</li><li>增强表示信息</li><li>概率设计</li></ol><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401241804526.png" alt="image-20240124180410455"></p><h2 id="跳跃连接变形"><a href="#跳跃连接变形" class="headerlink" title="跳跃连接变形"></a>跳跃连接变形</h2><h3 id="增加跳跃连接数目——Unet-x2F-Unet3"><a href="#增加跳跃连接数目——Unet-x2F-Unet3" class="headerlink" title="增加跳跃连接数目——Unet++&#x2F;Unet3+"></a>增加跳跃连接数目——Unet++&#x2F;Unet3+</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401241913309.png" alt="image-20240124191341261"></p><p>跳跃连接方向可以有两种，一种是向前传播，将编码器的低级语义特征与解码器结合，另一种是向后跳跃连接，将解码器的高级语义特征传给同等级的编码器，编码器可以将它与输入信息的特征结合</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401241920052.png" alt="image-20240124192045009"></p><h3 id="在跳跃连接处处理特征图（RA-Unet-x2F-attention-Unet）"><a href="#在跳跃连接处处理特征图（RA-Unet-x2F-attention-Unet）" class="headerlink" title="在跳跃连接处处理特征图（RA-Unet&#x2F;attention Unet）"></a>在跳跃连接处处理特征图（RA-Unet&#x2F;attention Unet）</h3><h4 id="在跳跃连接处增加注意门，以隐式学习抑制输入图像中的不相关区域，同时突出显示当前分割任务感兴趣的区域。"><a href="#在跳跃连接处增加注意门，以隐式学习抑制输入图像中的不相关区域，同时突出显示当前分割任务感兴趣的区域。" class="headerlink" title="在跳跃连接处增加注意门，以隐式学习抑制输入图像中的不相关区域，同时突出显示当前分割任务感兴趣的区域。"></a>在跳跃连接处增加注意门，以隐式学习抑制输入图像中的不相关区域，同时突出显示当前分割任务感兴趣的区域。</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401241935048.png" alt="image-20240124193546987"></p><h4 id="将RNN-添加到Unet-的跳跃连接上"><a href="#将RNN-添加到Unet-的跳跃连接上" class="headerlink" title="将RNN 添加到Unet 的跳跃连接上"></a>将RNN 添加到Unet 的跳跃连接上</h4><p>原始的Unet 结构的最大池化操作会带来空间相对信息的丢失，RNN 通过学习多尺度和远程空间上下文来改善分割结果。通过跳过连接的最终特征映射是原始编码器特征映射和从这些映射中提取的RNN特征的组合。RNN模块的一个限制是，它们使训练更加困难，计算成本更高。为了弥补这一点，Li等人增加了深度监督</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401241945229.png" alt="image-20240124194541188"></p><h4 id="结合编码器和解码器特征映射的组合"><a href="#结合编码器和解码器特征映射的组合" class="headerlink" title="结合编码器和解码器特征映射的组合"></a>结合编码器和解码器特征映射的组合</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401241947269.png" alt="image-20240124194726219"></p><h2 id="主干网络设计"><a href="#主干网络设计" class="headerlink" title="主干网络设计"></a>主干网络设计</h2><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><h4 id="多分辨率模块（MultiResUnet-densenet）"><a href="#多分辨率模块（MultiResUnet-densenet）" class="headerlink" title="多分辨率模块（MultiResUnet\densenet）"></a>多分辨率模块（MultiResUnet\densenet）</h4><p>在相同的输入上并行使用不同核大小的卷积层，并且将不同尺度的感知传递到网络深处之前将其结合在一起</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242000341.png" alt="image-20240124200047292"></p><h4 id="重新考虑卷积"><a href="#重新考虑卷积" class="headerlink" title="重新考虑卷积"></a>重新考虑卷积</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242002766.png" alt="image-20240124200232718"></p><p>减少计算量</p><h4 id="RNN结构（RU-Net）"><a href="#RNN结构（RU-Net）" class="headerlink" title="RNN结构（RU-Net）"></a>RNN结构（RU-Net）</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242005215.png" alt="image-20240124200513178"></p><h2 id="瓶颈结构设计"><a href="#瓶颈结构设计" class="headerlink" title="瓶颈结构设计"></a>瓶颈结构设计</h2><p>Unet 结构可以分为三个主要的部分：编码器（收缩路径）、解码器（扩展路径）和位于编码器和解码器之间的瓶颈。 瓶颈用来被迫模型学习输入数据的压缩表示，该表示应该只含在解码器中恢复输入所需的重要和有用的信息。</p><p>最初的Unet 中,瓶颈由两个具有ReLu 激活的3×3 卷积层组成。</p><h4 id="在瓶颈处应用注意力模块（MA-Net-SA-UNet）"><a href="#在瓶颈处应用注意力模块（MA-Net-SA-UNet）" class="headerlink" title="在瓶颈处应用注意力模块（MA-Net\SA-UNet）"></a>在瓶颈处应用注意力模块（MA-Net\SA-UNet）</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242014088.png" alt="image-20240124201433044"></p><h4 id="多尺度表示（COPLE-Net-）"><a href="#多尺度表示（COPLE-Net-）" class="headerlink" title="多尺度表示（COPLE-Net ）"></a>多尺度表示（COPLE-Net ）</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242017687.png" alt="image-20240124201705646"></p><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="（TransUnet-x2F-GT-U-Net-x2F-MT-UNet-x2F-ScaleFormer-）-transformer-与CNN相结合"><a href="#（TransUnet-x2F-GT-U-Net-x2F-MT-UNet-x2F-ScaleFormer-）-transformer-与CNN相结合" class="headerlink" title="（TransUnet&#x2F;GT U-Net&#x2F;MT-UNet&#x2F;ScaleFormer ） transformer 与CNN相结合"></a>（TransUnet&#x2F;GT U-Net&#x2F;MT-UNet&#x2F;ScaleFormer ） transformer 与CNN相结合</h3><p>卷积神经网络的性能体现在多尺度表示和捕获局部语义和纹理信息的能力上，但是CNN的局部表示可能不够鲁棒，无法捕获医疗数据中存在的几何和结构信息,需要一种机制来捕获像素间的长关系</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242028355.png" alt="image-20240124202802305"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202401242038822.png" alt="image-20240124203815775"></p><h3 id="只用transformer"><a href="#只用transformer" class="headerlink" title="只用transformer"></a>只用transformer</h3>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>计算机视觉一些常用名词解释</title>
      <link href="/2023/07/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
      <url>/2023/07/01/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
      
        <content type="html"><![CDATA[<h6 id="CV-Computer-Vision-；计算机视觉，它的目的是来理解图像中所要表达的意思。可以对图像进行一些操作，例如分割、分类等。"><a href="#CV-Computer-Vision-；计算机视觉，它的目的是来理解图像中所要表达的意思。可以对图像进行一些操作，例如分割、分类等。" class="headerlink" title="CV(Computer Vision)；计算机视觉，它的目的是来理解图像中所要表达的意思。可以对图像进行一些操作，例如分割、分类等。"></a>CV(Computer Vision)；计算机视觉，它的目的是来理解图像中所要表达的意思。可以对图像进行一些操作，例如分割、分类等。</h6><h6 id="多尺度：即对于图像不同大小的采集，在多尺度下可以观察到不同的特征，比如更密集的采样有利于看到更多的细节，更稀疏的采样可以看到整体的联系。使用多尺度，既可以看到全局的信息，也知道局部的信息。"><a href="#多尺度：即对于图像不同大小的采集，在多尺度下可以观察到不同的特征，比如更密集的采样有利于看到更多的细节，更稀疏的采样可以看到整体的联系。使用多尺度，既可以看到全局的信息，也知道局部的信息。" class="headerlink" title="多尺度：即对于图像不同大小的采集，在多尺度下可以观察到不同的特征，比如更密集的采样有利于看到更多的细节，更稀疏的采样可以看到整体的联系。使用多尺度，既可以看到全局的信息，也知道局部的信息。"></a>多尺度：即对于图像不同大小的采集，在多尺度下可以观察到不同的特征，比如更密集的采样有利于看到更多的细节，更稀疏的采样可以看到整体的联系。使用多尺度，既可以看到全局的信息，也知道局部的信息。</h6><h6 id="梯度爆炸-x2F-梯度消失：由于网络层次过深，在反向传播中，会出现偏导累乘而趋于无穷大或者为0的情况。梯度爆炸会使得靠近输入层的参数权值更新变化大，在极端情况下，参数的值变得非常大，以至于溢出，导致-NaN-值。梯度消失会使靠近输入层的参数权值更新缓慢，会相当于只有后面几层网络在起作用。"><a href="#梯度爆炸-x2F-梯度消失：由于网络层次过深，在反向传播中，会出现偏导累乘而趋于无穷大或者为0的情况。梯度爆炸会使得靠近输入层的参数权值更新变化大，在极端情况下，参数的值变得非常大，以至于溢出，导致-NaN-值。梯度消失会使靠近输入层的参数权值更新缓慢，会相当于只有后面几层网络在起作用。" class="headerlink" title="梯度爆炸&#x2F;梯度消失：由于网络层次过深，在反向传播中，会出现偏导累乘而趋于无穷大或者为0的情况。梯度爆炸会使得靠近输入层的参数权值更新变化大，在极端情况下，参数的值变得非常大，以至于溢出，导致 NaN 值。梯度消失会使靠近输入层的参数权值更新缓慢，会相当于只有后面几层网络在起作用。"></a>梯度爆炸&#x2F;梯度消失：由于网络层次过深，在反向传播中，会出现偏导累乘而趋于无穷大或者为0的情况。梯度爆炸会使得靠近输入层的参数权值更新变化大，在极端情况下，参数的值变得非常大，以至于溢出，导致 NaN 值。梯度消失会使靠近输入层的参数权值更新缓慢，会相当于只有后面几层网络在起作用。</h6>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>swin-transformer学习</title>
      <link href="/2023/07/01/swin-transformer%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/07/01/swin-transformer%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="Swin-Transformer-的思想"><a href="#Swin-Transformer-的思想" class="headerlink" title="Swin Transformer 的思想"></a>Swin Transformer 的思想</h2><ol><li><p>Vision Transformer 的思想</p><p>将图像拆分为块（patch)，并将这些图像块的线性嵌入序列作为Transformer的输入，自注意力即要求每个像素去关注所有其它像素</p></li><li><p>分层思想</p><p>采取分层思想，随着特征层的加深，特征图的高和宽逐渐变小</p></li><li><p>移动窗口</p><p>解决窗口间信息无法交互的缺点</p></li></ol><h1 id="Swin-transformer"><a href="#Swin-transformer" class="headerlink" title="Swin transformer"></a>Swin transformer</h1><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307021216934.png"></p><p>参考博文及代码：<a href="http://t.csdn.cn/V3rMx">Swin-Transformer网络结构详解_swin transformer_太阳花的小绿豆的博客-CSDN博客</a></p><h2 id="人脑中的注意力机制"><a href="#人脑中的注意力机制" class="headerlink" title="人脑中的注意力机制"></a>人脑中的注意力机制</h2><p>注意力是一个用来分配有限的信息处理能力的选择机制。<br>以人眼为例，眼睛首先扫描整个场景元素，然后寻找感兴趣的影像区域，聚焦感兴趣的区域，仔细观察获得信息。人脑对于整个场景的关注不是均衡的，有一定的权重区分，感兴趣的区域会被人脑分配更多的权重。</p><h2 id="自注意力机制的实现"><a href="#自注意力机制的实现" class="headerlink" title="自注意力机制的实现"></a>自注意力机制的实现</h2><p>自注意机制就是通过权重矩阵自发地找到元素与元素之间的关系</p><h3 id="注意力公式以及其中所包含的意义"><a href="#注意力公式以及其中所包含的意义" class="headerlink" title="注意力公式以及其中所包含的意义"></a>注意力公式以及其中所包含的意义</h3><p><img src="https://img-blog.csdnimg.cn/20190802125242224.png"></p><p>Q:查询向量（query)，用来查询其它元素</p><p>K:关键字（key)，用来被其它元素查询<br>V:内容（value)</p><p>每一个元素都有自己的Q、K、V，元素使用自己的Q与其它元素的K相乘得到该元素与其它元素的相似度，这个相似度可以理解为关注度得分，关注度得分越高，这两个元素之间的联系越密切。</p><p>公式中的Q、K、V表示的是矩阵，是所有元素Q、K、V的集合。Q和K的转置点乘，得到的是元素之间的相互关系，经过softmax后得到的就是映射到（0,1）的各个元素的相互关联程度，与V相乘后得到的是内部元素有关联的集合。为什么还要再除上一个缩放因子呢，dK所表示的是k的维度，如果维度dk太大，那么点积的值也会变大，由softmax函数的特性会知道这会导致经过softmax函数后被推到一个梯度极小的区域，加入缩放因子可以抵消这种影响，保证可以顺利进行反向传播。</p><p>经过这个公式后，得到的结果就可以突出需要注意的地方（即关联程度更高的地方）</p><h3 id="Q、K、V的得来"><a href="#Q、K、V的得来" class="headerlink" title="Q、K、V的得来"></a>Q、K、V的得来</h3><p>Q、K、V是由上一层的输出乘上它们各自的权重矩阵，权重矩阵Wq、Wk、Wv先由上一层的输出经过线性变化得来，然后通过网络学习来不断更新</p><h3 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h3><p>可以类比CNN中同时使用多个卷积核的作用，设置多个Q、K、V权重矩阵，形成多个子空间，可以让模型去关注到不同方面的信息，最后再将各个方面信息综合起来，这样做有利于网络捕捉到更丰富的特征。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747604.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Optional</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint <span class="keyword">as</span> checkpoint</span><br></pre></td></tr></table></figure><h2 id="patch-amp-amp-Embedding"><a href="#patch-amp-amp-Embedding" class="headerlink" title="patch&amp;&amp;Embedding"></a>patch&amp;&amp;Embedding</h2><p>Patch Partition<br>将图片输入Patch Partition 模块中进行分块，每4×4作为一个patch，输入的是RGB三通道图片，则shape由[H,W,3]变成[H&#x2F;4,W&#x2F;4,48]</p><p>Linear Embedding<br>对每个像素的channel数据做线性变化，shape变为[H&#x2F;4,W&#x2F;4,C]</p><p>在源码中Patch Partition 和Linear Embeding 是直接通过一个卷积层实现</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747734.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patch_size=<span class="number">4</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">96</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        self.patch_size = patch_size</span><br><span class="line">        self.in_chans = in_c</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        self.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        <span class="comment"># 如果输入图片的H，W不是patch_size的整数倍，需要进行padding</span></span><br><span class="line">        pad_input = (H % self.patch_size[<span class="number">0</span>] != <span class="number">0</span>) <span class="keyword">or</span> (W % self.patch_size[<span class="number">1</span>] != <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> pad_input:</span><br><span class="line">            <span class="comment"># to pad the last 3 dimensions,</span></span><br><span class="line">            <span class="comment"># (W_left, W_right, H_top,H_bottom, C_front, C_back)</span></span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, self.patch_size[<span class="number">1</span>] - W % self.patch_size[<span class="number">1</span>],</span><br><span class="line">                          <span class="number">0</span>, self.patch_size[<span class="number">0</span>] - H % self.patch_size[<span class="number">0</span>],</span><br><span class="line">                          <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下采样patch_size倍</span></span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        _, _, H, W = x.shape</span><br><span class="line">        <span class="comment"># flatten: [B, C, H, W] -&gt; [B, C, HW]</span></span><br><span class="line">        <span class="comment"># transpose: [B, C, HW] -&gt; [B, HW, C]</span></span><br><span class="line">        x = x.flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x, H, W</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model=PatchEmbed()</span><br><span class="line">y,H,W=model(x)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([1, 3136, 96])</span><br></pre></td></tr></table></figure><h2 id="SwinTransformerBlock"><a href="#SwinTransformerBlock" class="headerlink" title="SwinTransformerBlock"></a>SwinTransformerBlock</h2><h3 id="Lay-normalization-模块的作用"><a href="#Lay-normalization-模块的作用" class="headerlink" title="Lay normalization 模块的作用"></a>Lay normalization 模块的作用</h3><h4 id="Internal-Covariate-Shift（协变量偏移）"><a href="#Internal-Covariate-Shift（协变量偏移）" class="headerlink" title="Internal Covariate Shift（协变量偏移）"></a>Internal Covariate Shift（协变量偏移）</h4><p>梯度下降使得每一层的参数都在不断发生变化，<br>进而使得每一层的线性与非线性计算结果分布产生变化。<br>后层网络就要不停地去适应这种分布变化，这个时候就会使得整个网络的学习速率过慢</p><p>因此可以通过固定每一层网络输入值的分布来对减缓这个问题</p><p>如果把x∈N×C×H×W类比为一摞书，这摞书总共有 N 本，每本有 C 页，每页有 H 行，每行 W 个数字。LN 求均值时，相当于把每本书里所有页，每页里的所有数字都相加起来，然后再除以该本书总的数字个数C<em>W</em>H，相当于得到某本书总体的数字平均值，每本书的均值相同，求标准差时也是同理。总结为“对应本”。<br>引用：<a href="http://t.csdn.cn/LQNTN">深度学习笔记-11.Normalization[规范化]方法总结_业余狙击手19的博客-CSDN博客</a></p><h3 id="VIT"><a href="#VIT" class="headerlink" title="VIT"></a>VIT</h3><p><img src="https://img-blog.csdnimg.cn/c7fec29094574dca88384767f1eae0fd.gif"></p><p>Swin Transformer 在VIT 上做出改进，引入W-MSA模块是为了减少计算量。</p><h4 id="MSA和W-MSA的计算量"><a href="#MSA和W-MSA的计算量" class="headerlink" title="MSA和W-MSA的计算量"></a>MSA和W-MSA的计算量</h4><p>计算量是hw×C×C,同理得K,V的计算量也为hw×C×C，一共为3hwC2，再计算Q×KT,计算量为hw×C×hw,忽略softmax,再×V，计算量hw×hw×C，所以总计算量4hwC2+2(hw)2C</p><p>划分窗口后，单个窗口的计算量为4M2C2+2M4C,窗口数量是wh&#x2F;M2,二者相乘总的计算量为4hwC2+2M2hwC</p><h3 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h3><p>W-MSA，只会在每个窗口内进行自注意力计算，所以窗口与窗口之间是无法进行信息传递的。为了解决这个问题，引入了SW-MSA</p><p>W-MSA和SW-MSA是成对使用的，SW-MSA的窗口从左上角分别向右侧和下方各偏移了[M&#x2F;2]</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747945.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer Block.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Window size.</span></span><br><span class="line"><span class="string">        shift_size (int): Shift size for SW-MSA.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float, optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, num_heads, window_size=<span class="number">7</span>, shift_size=<span class="number">0</span>,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>, drop_path=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 act_layer=nn.GELU, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.shift_size = shift_size</span><br><span class="line">        self.mlp_ratio = mlp_ratio</span><br><span class="line">        <span class="keyword">assert</span> <span class="number">0</span> &lt;= self.shift_size &lt; self.window_size, <span class="string">&quot;shift_size must in 0-window_size&quot;</span></span><br><span class="line"></span><br><span class="line">        self.norm1 = norm_layer(dim)</span><br><span class="line">        self.attn = WindowAttention(</span><br><span class="line">            dim, window_size=(self.window_size, self.window_size), num_heads=num_heads, qkv_bias=qkv_bias,</span><br><span class="line">            attn_drop=attn_drop, proj_drop=drop)</span><br><span class="line"></span><br><span class="line">        self.drop_path = DropPath(drop_path) <span class="keyword">if</span> drop_path &gt; <span class="number">0.</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line">        self.norm2 = norm_layer(dim)</span><br><span class="line">        mlp_hidden_dim = <span class="built_in">int</span>(dim * mlp_ratio)</span><br><span class="line">        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, attn_mask</span>):</span><br><span class="line">        H, W = self.H, self.W</span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        shortcut = x</span><br><span class="line">        x = self.norm1(x)</span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># pad feature maps to multiples of window size</span></span><br><span class="line">        <span class="comment"># 把feature map给pad到window size的整数倍</span></span><br><span class="line">        pad_l = pad_t = <span class="number">0</span></span><br><span class="line">        pad_r = (self.window_size - W % self.window_size) % self.window_size</span><br><span class="line">        pad_b = (self.window_size - H % self.window_size) % self.window_size</span><br><span class="line">        x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, pad_l, pad_r, pad_t, pad_b))</span><br><span class="line">        _, Hp, Wp, _ = x.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shifted_x = x</span><br><span class="line">            attn_mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># partition windows</span></span><br><span class="line">        x_windows = window_partition(shifted_x, self.window_size)  <span class="comment"># [nW*B, Mh, Mw, C]</span></span><br><span class="line">        x_windows = x_windows.view(-<span class="number">1</span>, self.window_size * self.window_size, C)  <span class="comment"># [nW*B, Mh*Mw, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># W-MSA/SW-MSA</span></span><br><span class="line">        attn_windows = self.attn(x_windows, mask=attn_mask)  <span class="comment"># [nW*B, Mh*Mw, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># merge windows</span></span><br><span class="line">        attn_windows = attn_windows.view(-<span class="number">1</span>, self.window_size, self.window_size, C)  <span class="comment"># [nW*B, Mh, Mw, C]</span></span><br><span class="line">        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  <span class="comment"># [B, H&#x27;, W&#x27;, C]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># reverse cyclic shift</span></span><br><span class="line">        <span class="keyword">if</span> self.shift_size &gt; <span class="number">0</span>:</span><br><span class="line">            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = shifted_x</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pad_r &gt; <span class="number">0</span> <span class="keyword">or</span> pad_b &gt; <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 把前面pad的数据移除掉</span></span><br><span class="line">            x = x[:, :H, :W, :].contiguous()</span><br><span class="line"></span><br><span class="line">        x = x.view(B, H * W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># FFN</span></span><br><span class="line">        x = shortcut + self.drop_path(x)</span><br><span class="line">        x = x + self.drop_path(self.mlp(self.norm2(x)))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="windowAttention"><a href="#windowAttention" class="headerlink" title="windowAttention"></a>windowAttention</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">WindowAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span></span><br><span class="line"><span class="string">    It supports both of shifted and non-shifted window.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        window_size (tuple[int]): The height and width of the window.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span></span><br><span class="line"><span class="string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, window_size, num_heads, qkv_bias=<span class="literal">True</span>, attn_drop=<span class="number">0.</span>, proj_drop=<span class="number">0.</span></span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.window_size = window_size  <span class="comment"># [Mh, Mw]</span></span><br><span class="line">        self.num_heads = num_heads</span><br><span class="line">        head_dim = dim // num_heads</span><br><span class="line">        self.scale = head_dim ** -<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># define a parameter table of relative position bias</span></span><br><span class="line">        self.relative_position_bias_table = nn.Parameter(</span><br><span class="line">            torch.zeros((<span class="number">2</span> * window_size[<span class="number">0</span>] - <span class="number">1</span>) * (<span class="number">2</span> * window_size[<span class="number">1</span>] - <span class="number">1</span>), num_heads))  <span class="comment"># [2*Mh-1 * 2*Mw-1, nH]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pair-wise relative position index for each token inside the window</span></span><br><span class="line">        coords_h = torch.arange(self.window_size[<span class="number">0</span>])</span><br><span class="line">        coords_w = torch.arange(self.window_size[<span class="number">1</span>])</span><br><span class="line">        coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing=<span class="string">&quot;ij&quot;</span>))  <span class="comment"># [2, Mh, Mw]</span></span><br><span class="line">        coords_flatten = torch.flatten(coords, <span class="number">1</span>)  <span class="comment"># [2, Mh*Mw]</span></span><br><span class="line">        <span class="comment"># [2, Mh*Mw, 1] - [2, 1, Mh*Mw]</span></span><br><span class="line">        relative_coords = coords_flatten[:, :, <span class="literal">None</span>] - coords_flatten[:, <span class="literal">None</span>, :]  <span class="comment"># [2, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        relative_coords = relative_coords.permute(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>).contiguous()  <span class="comment"># [Mh*Mw, Mh*Mw, 2]</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] += self.window_size[<span class="number">0</span>] - <span class="number">1</span>  <span class="comment"># shift to start from 0</span></span><br><span class="line">        relative_coords[:, :, <span class="number">1</span>] += self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_coords[:, :, <span class="number">0</span>] *= <span class="number">2</span> * self.window_size[<span class="number">1</span>] - <span class="number">1</span></span><br><span class="line">        relative_position_index = relative_coords.<span class="built_in">sum</span>(-<span class="number">1</span>)  <span class="comment"># [Mh*Mw, Mh*Mw]</span></span><br><span class="line">        self.register_buffer(<span class="string">&quot;relative_position_index&quot;</span>, relative_position_index)</span><br><span class="line"></span><br><span class="line">        self.qkv = nn.Linear(dim, dim * <span class="number">3</span>, bias=qkv_bias)</span><br><span class="line">        self.attn_drop = nn.Dropout(attn_drop)</span><br><span class="line">        self.proj = nn.Linear(dim, dim)</span><br><span class="line">        self.proj_drop = nn.Dropout(proj_drop)</span><br><span class="line"></span><br><span class="line">        nn.init.trunc_normal_(self.relative_position_bias_table, std=<span class="number">.02</span>)</span><br><span class="line">        self.softmax = nn.Softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: input features with shape of (num_windows*B, Mh*Mw, C)</span></span><br><span class="line"><span class="string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># [batch_size*num_windows, Mh*Mw, total_embed_dim]</span></span><br><span class="line">        B_, N, C = x.shape</span><br><span class="line">        <span class="comment"># qkv(): -&gt; [batch_size*num_windows, Mh*Mw, 3 * total_embed_dim]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size*num_windows, Mh*Mw, 3, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># permute: -&gt; [3, batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        qkv = self.qkv(x).reshape(B_, N, <span class="number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        q, k, v = qkv.unbind(<span class="number">0</span>)  <span class="comment"># make torchscript happy (cannot use tensor as tuple)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size*num_windows, num_heads, embed_dim_per_head, Mh*Mw]</span></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size*num_windows, num_heads, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        q = q * self.scale</span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># relative_position_bias_table.view: [Mh*Mw*Mh*Mw,nH] -&gt; [Mh*Mw,Mh*Mw,nH]</span></span><br><span class="line">        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="number">1</span>)].view(</span><br><span class="line">            self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], self.window_size[<span class="number">0</span>] * self.window_size[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        relative_position_bias = relative_position_bias.permute(<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>).contiguous()  <span class="comment"># [nH, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        attn = attn + relative_position_bias.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># mask: [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            nW = mask.shape[<span class="number">0</span>]  <span class="comment"># num_windows</span></span><br><span class="line">            <span class="comment"># attn.view: [batch_size, num_windows, num_heads, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            <span class="comment"># mask.unsqueeze: [1, nW, 1, Mh*Mw, Mh*Mw]</span></span><br><span class="line">            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="number">1</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">            attn = attn.view(-<span class="number">1</span>, self.num_heads, N, N)</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            attn = self.softmax(attn)</span><br><span class="line"></span><br><span class="line">        attn = self.attn_drop(attn)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># @: multiply -&gt; [batch_size*num_windows, num_heads, Mh*Mw, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># transpose: -&gt; [batch_size*num_windows, Mh*Mw, num_heads, embed_dim_per_head]</span></span><br><span class="line">        <span class="comment"># reshape: -&gt; [batch_size*num_windows, Mh*Mw, total_embed_dim]</span></span><br><span class="line">        x = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B_, N, C)</span><br><span class="line">        x = self.proj(x)</span><br><span class="line">        x = self.proj_drop(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h4 id="掩码操作"><a href="#掩码操作" class="headerlink" title="掩码操作"></a>掩码操作</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747945.png"></p><h4 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h4><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747879.png"></p><h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747670.png"></p><p>作用：增强网络的表达能力</p><h4 id="Dropout层的作用"><a href="#Dropout层的作用" class="headerlink" title="Dropout层的作用"></a>Dropout层的作用</h4><p>一般来说，当相对较大的模型用在较小的数据集时，通过Dropout的方法可以防止过拟合，并提高泛化性</p><p>本质是通过随机删除部分神经元（特征）及其对应连接，实现对网络特征提取的随机修正，这种过程被称作随机正则化</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Mlp</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; MLP as used in Vision Transformer, MLP-Mixer and related networks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_features, hidden_features=<span class="literal">None</span>, out_features=<span class="literal">None</span>, act_layer=nn.GELU, drop=<span class="number">0.</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        out_features = out_features <span class="keyword">or</span> in_features</span><br><span class="line">        hidden_features = hidden_features <span class="keyword">or</span> in_features</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features, hidden_features)</span><br><span class="line">        self.act = act_layer()</span><br><span class="line">        self.drop1 = nn.Dropout(drop)</span><br><span class="line">        self.fc2 = nn.Linear(hidden_features, out_features)</span><br><span class="line">        self.drop2 = nn.Dropout(drop)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.fc1(x)</span><br><span class="line">        x = self.act(x)</span><br><span class="line">        x = self.drop1(x)</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        x = self.drop2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="window-partition"><a href="#window-partition" class="headerlink" title="window_partition"></a>window_partition</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_partition</span>(<span class="params">x, window_size: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将feature map按照window_size划分成一个个没有重叠的window</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">        window_size (int): window size(M)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B, H, W, C = x.shape</span><br><span class="line">    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)</span><br><span class="line">    <span class="comment"># permute: [B, H//Mh, Mh, W//Mw, Mw, C] -&gt; [B, H//Mh, W//Mh, Mw, Mw, C]</span></span><br><span class="line">    <span class="comment"># view: [B, H//Mh, W//Mw, Mh, Mw, C] -&gt; [B*num_windows, Mh, Mw, C]</span></span><br><span class="line">    windows = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(-<span class="number">1</span>, window_size, window_size, C)</span><br><span class="line">    <span class="keyword">return</span> windows</span><br></pre></td></tr></table></figure><h3 id="window-reverse"><a href="#window-reverse" class="headerlink" title="window_reverse"></a>window_reverse</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">window_reverse</span>(<span class="params">windows, window_size: <span class="built_in">int</span>, H: <span class="built_in">int</span>, W: <span class="built_in">int</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将一个个window还原成一个feature map</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        windows: (num_windows*B, window_size, window_size, C)</span></span><br><span class="line"><span class="string">        window_size (int): Window size(M)</span></span><br><span class="line"><span class="string">        H (int): Height of image</span></span><br><span class="line"><span class="string">        W (int): Width of image</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        x: (B, H, W, C)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    B = <span class="built_in">int</span>(windows.shape[<span class="number">0</span>] / (H * W / window_size / window_size))</span><br><span class="line">    <span class="comment"># view: [B*num_windows, Mh, Mw, C] -&gt; [B, H//Mh, W//Mw, Mh, Mw, C]</span></span><br><span class="line">    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># permute: [B, H//Mh, W//Mw, Mh, Mw, C] -&gt; [B, H//Mh, Mh, W//Mw, Mw, C]</span></span><br><span class="line">    <span class="comment"># view: [B, H//Mh, Mh, W//Mw, Mw, C] -&gt; [B, H, W, C]</span></span><br><span class="line">    x = x.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>).contiguous().view(B, H, W, -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="DropPath"><a href="#DropPath" class="headerlink" title="DropPath"></a>DropPath</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DropPath</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, drop_prob=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DropPath, self).__init__()</span><br><span class="line">        self.drop_prob = drop_prob</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> drop_path_f(x, self.drop_prob, self.training)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">drop_path_f</span>(<span class="params">x, drop_prob: <span class="built_in">float</span> = <span class="number">0.</span>, training: <span class="built_in">bool</span> = <span class="literal">False</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).</span></span><br><span class="line"><span class="string">    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,</span></span><br><span class="line"><span class="string">    the original name is misleading as &#x27;Drop Connect&#x27; is a different form of dropout in a separate paper...</span></span><br><span class="line"><span class="string">    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I&#x27;ve opted for</span></span><br><span class="line"><span class="string">    changing the layer and argument names to &#x27;drop path&#x27; rather than mix DropConnect as a layer name and use</span></span><br><span class="line"><span class="string">    &#x27;survival rate&#x27; as the argument.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> drop_prob == <span class="number">0.</span> <span class="keyword">or</span> <span class="keyword">not</span> training:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    keep_prob = <span class="number">1</span> - drop_prob</span><br><span class="line">    shape = (x.shape[<span class="number">0</span>],) + (<span class="number">1</span>,) * (x.ndim - <span class="number">1</span>)  <span class="comment"># work with diff dim tensors, not just 2D ConvNets</span></span><br><span class="line">    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)</span><br><span class="line">    random_tensor.floor_()  <span class="comment"># binarize</span></span><br><span class="line">    output = x.div(keep_prob) * random_tensor</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h2 id="patch-merging"><a href="#patch-merging" class="headerlink" title="patch merging"></a>patch merging</h2><p>作用： 进行下采样</p><p>假设输入Patch Merging的是一个4x4大小的单通道特征图（feature map），Patch Merging会将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map。接着将这四个feature map在深度方向进行concat拼接，然后再通过一个LayerNorm层。最后通过一个全连接层在feature map的深度方向做线性变化，将feature map的深度由C变成C&#x2F;2。通过这个例子可以看出，通过Patch Merging层后，feature map的高和宽会减半，深度会翻倍。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202307051747807.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchMerging</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Patch Merging Layer.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, norm_layer=nn.LayerNorm</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.reduction = nn.Linear(<span class="number">4</span> * dim, <span class="number">2</span> * dim, bias=<span class="literal">False</span>)</span><br><span class="line">        self.norm = norm_layer(<span class="number">4</span> * dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: B, H*W, C</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        B, L, C = x.shape</span><br><span class="line">        <span class="keyword">assert</span> L == H * W, <span class="string">&quot;input feature has wrong size&quot;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(B, H, W, C)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># padding</span></span><br><span class="line">        <span class="comment"># 如果输入feature map的H，W不是2的整数倍，需要进行padding</span></span><br><span class="line">        pad_input = (H % <span class="number">2</span> == <span class="number">1</span>) <span class="keyword">or</span> (W % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> pad_input:</span><br><span class="line">            <span class="comment"># to pad the last 3 dimensions, starting from the last dimension and moving forward.</span></span><br><span class="line">            <span class="comment"># (C_front, C_back, W_left, W_right, H_top, H_bottom)</span></span><br><span class="line">            <span class="comment"># 注意这里的Tensor通道是[B, H, W, C]，所以会和官方文档有些不同</span></span><br><span class="line">            x = F.pad(x, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, W % <span class="number">2</span>, <span class="number">0</span>, H % <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">        x0 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x1 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">0</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x2 = x[:, <span class="number">0</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x3 = x[:, <span class="number">1</span>::<span class="number">2</span>, <span class="number">1</span>::<span class="number">2</span>, :]  <span class="comment"># [B, H/2, W/2, C]</span></span><br><span class="line">        x = torch.cat([x0, x1, x2, x3], -<span class="number">1</span>)  <span class="comment"># [B, H/2, W/2, 4*C]</span></span><br><span class="line">        x = x.view(B, -<span class="number">1</span>, <span class="number">4</span> * C)  <span class="comment"># [B, H/2*W/2, 4*C]</span></span><br><span class="line"></span><br><span class="line">        x = self.norm(x)</span><br><span class="line">        x = self.reduction(x)  <span class="comment"># [B, H/2*W/2, 2*C]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x=torch.rand([<span class="number">1</span>,<span class="number">56</span>*<span class="number">56</span>,<span class="number">96</span>])</span><br><span class="line">model1=PatchMerging(<span class="number">96</span>)</span><br><span class="line">x=model1(x,H,W)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">torch.Size([1, 784, 192])</span><br></pre></td></tr></table></figure><h2 id="BasicLayer"><a href="#BasicLayer" class="headerlink" title="BasicLayer"></a>BasicLayer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A basic Swin Transformer layer for one stage.</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        dim (int): Number of input channels.</span></span><br><span class="line"><span class="string">        depth (int): Number of blocks.</span></span><br><span class="line"><span class="string">        num_heads (int): Number of attention heads.</span></span><br><span class="line"><span class="string">        window_size (int): Local window size.</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span></span><br><span class="line"><span class="string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop (float, optional): Dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span></span><br><span class="line"><span class="string">        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm</span></span><br><span class="line"><span class="string">        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim, depth, num_heads, window_size,</span></span><br><span class="line"><span class="params">                 mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>, drop=<span class="number">0.</span>, attn_drop=<span class="number">0.</span>,</span></span><br><span class="line"><span class="params">                 drop_path=<span class="number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="literal">None</span>, use_checkpoint=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.dim = dim</span><br><span class="line">        self.depth = depth</span><br><span class="line">        self.window_size = window_size</span><br><span class="line">        self.use_checkpoint = use_checkpoint</span><br><span class="line">        self.shift_size = window_size // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build blocks</span></span><br><span class="line">        self.blocks = nn.ModuleList([</span><br><span class="line">            SwinTransformerBlock(</span><br><span class="line">                dim=dim,</span><br><span class="line">                num_heads=num_heads,</span><br><span class="line">                window_size=window_size,</span><br><span class="line">                shift_size=<span class="number">0</span> <span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>) <span class="keyword">else</span> self.shift_size,</span><br><span class="line">                mlp_ratio=mlp_ratio,</span><br><span class="line">                qkv_bias=qkv_bias,</span><br><span class="line">                drop=drop,</span><br><span class="line">                attn_drop=attn_drop,</span><br><span class="line">                drop_path=drop_path[i] <span class="keyword">if</span> <span class="built_in">isinstance</span>(drop_path, <span class="built_in">list</span>) <span class="keyword">else</span> drop_path,</span><br><span class="line">                norm_layer=norm_layer)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(depth)])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># patch merging layer</span></span><br><span class="line">        <span class="keyword">if</span> downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.downsample = downsample(dim=dim, norm_layer=norm_layer)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.downsample = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_mask</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        <span class="comment"># calculate attention mask for SW-MSA</span></span><br><span class="line">        <span class="comment"># 保证Hp和Wp是window_size的整数倍</span></span><br><span class="line">        Hp = <span class="built_in">int</span>(np.ceil(H / self.window_size)) * self.window_size</span><br><span class="line">        Wp = <span class="built_in">int</span>(np.ceil(W / self.window_size)) * self.window_size</span><br><span class="line">        <span class="comment"># 拥有和feature map一样的通道排列顺序，方便后续window_partition</span></span><br><span class="line">        img_mask = torch.zeros((<span class="number">1</span>, Hp, Wp, <span class="number">1</span>), device=x.device)  <span class="comment"># [1, Hp, Wp, 1]</span></span><br><span class="line">        h_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">        w_slices = (<span class="built_in">slice</span>(<span class="number">0</span>, -self.window_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.window_size, -self.shift_size),</span><br><span class="line">                    <span class="built_in">slice</span>(-self.shift_size, <span class="literal">None</span>))</span><br><span class="line">        cnt = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> h_slices:</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> w_slices:</span><br><span class="line">                img_mask[:, h, w, :] = cnt</span><br><span class="line">                cnt += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        mask_windows = window_partition(img_mask, self.window_size)  <span class="comment"># [nW, Mh, Mw, 1]</span></span><br><span class="line">        mask_windows = mask_windows.view(-<span class="number">1</span>, self.window_size * self.window_size)  <span class="comment"># [nW, Mh*Mw]</span></span><br><span class="line">        attn_mask = mask_windows.unsqueeze(<span class="number">1</span>) - mask_windows.unsqueeze(<span class="number">2</span>)  <span class="comment"># [nW, 1, Mh*Mw] - [nW, Mh*Mw, 1]</span></span><br><span class="line">        <span class="comment"># [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        attn_mask = attn_mask.masked_fill(attn_mask != <span class="number">0</span>, <span class="built_in">float</span>(-<span class="number">100.0</span>)).masked_fill(attn_mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="number">0.0</span>))</span><br><span class="line">        <span class="keyword">return</span> attn_mask</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, H, W</span>):</span><br><span class="line">        attn_mask = self.create_mask(x, H, W)  <span class="comment"># [nW, Mh*Mw, Mh*Mw]</span></span><br><span class="line">        <span class="keyword">for</span> blk <span class="keyword">in</span> self.blocks:</span><br><span class="line">            blk.H, blk.W = H, W</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> torch.jit.is_scripting() <span class="keyword">and</span> self.use_checkpoint:</span><br><span class="line">                x = checkpoint.checkpoint(blk, x, attn_mask)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                x = blk(x, attn_mask)</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            x = self.downsample(x, H, W)</span><br><span class="line">            H, W = (H + <span class="number">1</span>) // <span class="number">2</span>, (W + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, H, W</span><br></pre></td></tr></table></figure><h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><h2 id="SwinTransformer"><a href="#SwinTransformer" class="headerlink" title="SwinTransformer"></a>SwinTransformer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SwinTransformer</span>(nn.Module):</span><br><span class="line">    <span class="string">r&quot;&quot;&quot; Swin Transformer</span></span><br><span class="line"><span class="string">        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -</span></span><br><span class="line"><span class="string">          https://arxiv.org/pdf/2103.14030</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        patch_size (int | tuple(int)): Patch size. Default: 4</span></span><br><span class="line"><span class="string">        in_chans (int): Number of input image channels. Default: 3</span></span><br><span class="line"><span class="string">        num_classes (int): Number of classes for classification head. Default: 1000</span></span><br><span class="line"><span class="string">        embed_dim (int): Patch embedding dimension. Default: 96</span></span><br><span class="line"><span class="string">        depths (tuple(int)): Depth of each Swin Transformer layer.</span></span><br><span class="line"><span class="string">        num_heads (tuple(int)): Number of attention heads in different layers.</span></span><br><span class="line"><span class="string">        window_size (int): Window size. Default: 7</span></span><br><span class="line"><span class="string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4</span></span><br><span class="line"><span class="string">        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True</span></span><br><span class="line"><span class="string">        drop_rate (float): Dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        attn_drop_rate (float): Attention dropout rate. Default: 0</span></span><br><span class="line"><span class="string">        drop_path_rate (float): Stochastic depth rate. Default: 0.1</span></span><br><span class="line"><span class="string">        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.</span></span><br><span class="line"><span class="string">        patch_norm (bool): If True, add normalization after patch embedding. Default: True</span></span><br><span class="line"><span class="string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, patch_size=<span class="number">4</span>, in_chans=<span class="number">3</span>, num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 embed_dim=<span class="number">96</span>, depths=(<span class="params"><span class="number">2</span>, <span class="number">2</span>, <span class="number">6</span>, <span class="number">2</span></span>), num_heads=(<span class="params"><span class="number">3</span>, <span class="number">6</span>, <span class="number">12</span>, <span class="number">24</span></span>),</span></span><br><span class="line"><span class="params">                 window_size=<span class="number">7</span>, mlp_ratio=<span class="number">4.</span>, qkv_bias=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 drop_rate=<span class="number">0.</span>, attn_drop_rate=<span class="number">0.</span>, drop_path_rate=<span class="number">0.1</span>,</span></span><br><span class="line"><span class="params">                 norm_layer=nn.LayerNorm, patch_norm=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 use_checkpoint=<span class="literal">False</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.num_layers = <span class="built_in">len</span>(depths)</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.patch_norm = patch_norm</span><br><span class="line">        <span class="comment"># stage4输出特征矩阵的channels</span></span><br><span class="line">        self.num_features = <span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** (self.num_layers - <span class="number">1</span>))</span><br><span class="line">        self.mlp_ratio = mlp_ratio</span><br><span class="line"></span><br><span class="line">        <span class="comment"># split image into non-overlapping patches</span></span><br><span class="line">        self.patch_embed = PatchEmbed(</span><br><span class="line">            patch_size=patch_size, in_c=in_chans, embed_dim=embed_dim,</span><br><span class="line">            norm_layer=norm_layer <span class="keyword">if</span> self.patch_norm <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        self.pos_drop = nn.Dropout(p=drop_rate)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># stochastic depth</span></span><br><span class="line">        dpr = [x.item() <span class="keyword">for</span> x <span class="keyword">in</span> torch.linspace(<span class="number">0</span>, drop_path_rate, <span class="built_in">sum</span>(depths))]  <span class="comment"># stochastic depth decay rule</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># build layers</span></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> i_layer <span class="keyword">in</span> <span class="built_in">range</span>(self.num_layers):</span><br><span class="line">            <span class="comment"># 注意这里构建的stage和论文图中有些差异</span></span><br><span class="line">            <span class="comment"># 这里的stage不包含该stage的patch_merging层，包含的是下个stage的</span></span><br><span class="line">            layers = BasicLayer(dim=<span class="built_in">int</span>(embed_dim * <span class="number">2</span> ** i_layer),</span><br><span class="line">                                depth=depths[i_layer],</span><br><span class="line">                                num_heads=num_heads[i_layer],</span><br><span class="line">                                window_size=window_size,</span><br><span class="line">                                mlp_ratio=self.mlp_ratio,</span><br><span class="line">                                qkv_bias=qkv_bias,</span><br><span class="line">                                drop=drop_rate,</span><br><span class="line">                                attn_drop=attn_drop_rate,</span><br><span class="line">                                drop_path=dpr[<span class="built_in">sum</span>(depths[:i_layer]):<span class="built_in">sum</span>(depths[:i_layer + <span class="number">1</span>])],</span><br><span class="line">                                norm_layer=norm_layer,</span><br><span class="line">                                downsample=PatchMerging <span class="keyword">if</span> (i_layer &lt; self.num_layers - <span class="number">1</span>) <span class="keyword">else</span> <span class="literal">None</span>,</span><br><span class="line">                                use_checkpoint=use_checkpoint)</span><br><span class="line">            self.layers.append(layers)</span><br><span class="line"></span><br><span class="line">        self.norm = norm_layer(self.num_features)</span><br><span class="line">        self.avgpool = nn.AdaptiveAvgPool1d(<span class="number">1</span>)</span><br><span class="line">        self.head = nn.Linear(self.num_features, num_classes) <span class="keyword">if</span> num_classes &gt; <span class="number">0</span> <span class="keyword">else</span> nn.Identity()</span><br><span class="line"></span><br><span class="line">        self.apply(self._init_weights)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init_weights</span>(<span class="params">self, m</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear):</span><br><span class="line">            nn.init.trunc_normal_(m.weight, std=<span class="number">.02</span>)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Linear) <span class="keyword">and</span> m.bias <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(m, nn.LayerNorm):</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">            nn.init.constant_(m.weight, <span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x: [B, L, C]</span></span><br><span class="line">        x, H, W = self.patch_embed(x)</span><br><span class="line">        x = self.pos_drop(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x, H, W = layer(x, H, W)</span><br><span class="line"></span><br><span class="line">        x = self.norm(x)  <span class="comment"># [B, L, C]</span></span><br><span class="line">        x = self.avgpool(x.transpose(<span class="number">1</span>, <span class="number">2</span>))  <span class="comment"># [B, C, 1]</span></span><br><span class="line">        x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">        x = self.head(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch学习</title>
      <link href="/2023/06/29/pytorch%E4%B8%ADnn%E6%A8%A1%E5%9E%8B%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/06/29/pytorch%E4%B8%ADnn%E6%A8%A1%E5%9E%8B%E6%A8%A1%E5%9D%97%E7%9A%84%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="F-pad"><a href="#F-pad" class="headerlink" title="F.pad()"></a>F.pad()</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line">F.pad(<span class="built_in">input</span>,pad,mode=<span class="string">&#x27;constant&#x27;</span>,value=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">input</span>: 需要扩充的tensor，可以是图像数据，也可以是特征矩阵数据</span><br><span class="line">pad: 扩充维度，用于预先定义出某维度上的扩充参数(每两个参数用来扩充一维）)</span><br><span class="line">mode: 扩充方法，有‘constant’,‘reflect’,‘replicate’,三种模式，</span><br><span class="line">表示常量，反射，复制</span><br><span class="line">value: 扩充时指定补充值，value只能在constant模式才能赋值</span><br><span class="line"></span><br><span class="line">一维填充</span><br><span class="line">a=t》orch.empty(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>) </span><br><span class="line">pld=(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">t1=F.pad(a,pld,<span class="string">&#x27;constant&#x27;</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t1.shape)</span><br><span class="line">&gt;&gt;</span><br><span class="line">tensor([[[[<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]]])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">同理得二维填充</span><br><span class="line">a=torch.empty(<span class="number">1</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line">pld=(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">t1=F.pad(a,pld,<span class="string">&#x27;constant&#x27;</span>,<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(t1)</span><br><span class="line"><span class="built_in">print</span>(t1.shape)</span><br><span class="line">&gt;&gt;</span><br><span class="line">tensor([[[[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">          [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]]]])</span><br><span class="line">torch.Size([<span class="number">1</span>, <span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>])</span><br><span class="line">其它维数同理，不想扩充该维可用<span class="number">0</span>,<span class="number">0</span>代替</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>english articles</title>
      <link href="/2023/06/25/english/"/>
      <url>/2023/06/25/english/</url>
      
        <content type="html"><![CDATA[<h1 id="6-7"><a href="#6-7" class="headerlink" title="6.7"></a>6.7</h1><p>风景旅游景点</p><h2 id="West-Lake"><a href="#West-Lake" class="headerlink" title="West Lake"></a>West Lake</h2><p>        The beautiful West Lake,located in Hangzhou,Zhejiang Province,China, is one of the most famous and picturesque （风景如画的，美丽的）<u>tourist attractions</u>(旅游景点) in the country.  It covers an area of approximately 6.5 square kilometers and is surrounded by mountains on three sides.The lake is divided into five sections by three causeways（提岸）,and there are numerous temples,pagodas（佛塔）,gardens,and bridges <u>dotted around</u>（点缀，散步） its <u>shore</u>s（海岸，湖畔）.</p><p>            West Lake has a long and rich history,dating back to over 2,000 years ago.Throughout the <u> centuries</u>（世纪）,it has been a source of inspiration for poets,painters,and <u>scholars</u> (学者) ,who have praised its natural beauty and cultural significance in countless literary（文学） works.</p><p>        Today,     West Lake remains a popular destination for both domestic and international tourists. Vistors can take a boat ride on the lake, <u>stroll</u>（漫步） along its picturesque paths, and enjoy the <u>stunning</u>（壮丽的，令人震惊的，打昏迷的） scenery（风景） of the surrounding hills and gardens. West Lake is truly a treasure of China and a must-visit for anyone traveling to the country.</p><h1 id="6-8"><a href="#6-8" class="headerlink" title="6.8"></a>6.8</h1><h2 id="Blue"><a href="#Blue" class="headerlink" title="Blue"></a>Blue</h2><p>        Blue is a popular color that is often associated with <u>calmness</u>（平静）,serenity（宁静）,and <u>stability</u>（稳定）.It is also the color of the sky and the ocean, which makes it a natural choice for evoking feelings of peace and t<u>ranquility</u>（宁静）.</p><p>        In art,blue is often used to created a sense of depth and <u>perspective</u>（透视感）.It can also be used to created a cool and calming atmosphere, as well as a sense of melancholy（忧郁） or sadness.</p><p>        In fashion,blue is a versatile（多样化的） color that can be <u>worn</u>（wear 的过去分词） in many different shades and styles.From <u>pale</u> （苍白的，无力的）baby blues to deep navy <u>hues</u>（色彩）,blue can be used to create a range of looks that are both stylish（现代的、流行的） and timeless.</p><p>        Interestingly, blue is also the color of many important symbols and flags around the world. For example, the United Nations flag is blue to <u>represent</u>（代表） the importance of peace and unity（团结） among <u>nations</u>（国家）.</p><p>        Overall,the color blue has a wide range of meanings and uses, making it a versatile and popular choice in many different <u>contexts</u>（背景，上下文，情境）.</p><h1 id="6-9"><a href="#6-9" class="headerlink" title="6.9"></a>6.9</h1><p>景点</p><h2 id="Rome-The-Eternal（永恒的）-City"><a href="#Rome-The-Eternal（永恒的）-City" class="headerlink" title="Rome: The Eternal（永恒的） City"></a>Rome: The Eternal（永恒的） City</h2><p>        Rome, the capital city of Italy, is a fascinating（迷人的） destination that attracts millions of tourists every year. Known as the “Eternal City”,Rome is steeped（充满） in history and culture, with ancient ruins, <u>stunning</u> （壮丽的，令人震惊的，打昏迷的）<u>architecture</u>（建筑）, and delicious Italian <u> cuisine</u>（烹饪，厨房，美食）.</p><p>        One of Rome’s most famous attractions is the Colosseum（罗马圆形大剧场）, an enormous amphitheater （圆形剧场）that dates back to 80 <u>AD</u>（公元）. Visitors can explore the ruins and learn about the <u>gladiators</u>（脚斗士） who fought there. Another popular site is the <u> Catican</u>（焚帝冈） City, home to <u>St. Peter’s Basilica</u>（圣彼得大教堂） and <u>the Sistine Chapel</u>（西斯廷教堂）. which features <u>breathtaking</u>（令人惊叹的） frescoes（壁画） by Michelangelo（米开朗琪罗）.</p><p>        Rome is also famous for its food, especially <u>pasta</u> （意大利面）and pizza. Visitors can try <u>traditional dishes</u>（传统菜肴） like <u>carbonara, amatriciana, and cacio epepe</u>（培根蛋面、阿马特里齐亚面和黑胡椒奶酪面）, and enjoy a glass of Itlian <u>wine</u>（葡萄酒）.</p><p>        If you’re visiting Rome, be sure to take a stroll（漫步） through the city’s charming neighborhoods, like <u>Trastevere</u>（特拉斯特韦雷区）, and toss a coin into the Trevi <u>Fountain</u>（喷泉） for good luck. With so much to see and do, it’s no wonder that Rome is one of the world’s most popular tourist destinations.</p><h1 id="6-10"><a href="#6-10" class="headerlink" title="6.10"></a>6.10</h1><p>水果，种植</p><h2 id="Xianju-Waxberry-a-Delicious-Fruit"><a href="#Xianju-Waxberry-a-Delicious-Fruit" class="headerlink" title="Xianju Waxberry, a Delicious Fruit"></a>Xianju Waxberry, a Delicious Fruit</h2><p>        Xianju Waxberry（杨梅）is a type of fruit that is grown in the Xianju County（县） of Zhejiang Province（省） in China. Waxberry is a small, round fruit that is about the size of a <u>cherry</u>（樱桃）. It has a bright red color and a sweet, juicy taste. The fruit is rich in v<u>itamins</u>（维生素） and <u>minerals</u>（矿物质）, making it a healthy snack option.</p><p>        Waxberry trees grow in the mountains of Xianju County, where the climate is <u>mild</u>（温和的） and the soil is fertile（肥沃的）. The fruit is usually <u>harvested</u> （收割，收获）in June and July, when it is at its <u> ripest</u>（rioe最高级，最成熟的）. During this time, the county holds a Waxberry Festival to celebrate the harvest.</p><p>        In addition to being eaten <u>fresh</u>（新鲜的）, waxberry can also be used to make <u>jams</u>（果酱）, <u>jellies</u>（果冻）, and other sweet treats. It is a popular food in China and is exported（出口） to other countries as well.</p><p>        If you ever have the chance to visit Xianju County, be sure to try the local waxberry. It’s a delicious fruit that is unique to the <u>region</u>（当地，地区） and is sure to be a memorable（难忘的） part of your trip.</p><h1 id="6-11"><a href="#6-11" class="headerlink" title="6.11"></a>6.11</h1><p>景点 山</p><h2 id="Le-Mont-Saint-Michel"><a href="#Le-Mont-Saint-Michel" class="headerlink" title="Le Mont-Saint-Michel"></a>Le Mont-Saint-Michel</h2><p>        Le Mont-Saint-Michel is a small island located in Normandy, France. It is famous for its beautiful <u>abbey</u>（修道院） and stunning architecture. The island has a rich history, dating back to the 8th century, when a <u>bishop</u>（主教） named Aubert was visited by the Archangel（大天使长） Michael in a dream. According to legend, the Archangel Michael <u>instructed</u>（指导） Aubert to build a <u>chapel</u>（教堂） on the island <u>in his honor</u>（为了纪念他）.</p><p>         Over the centuries, the chapel grew into an abbey, and the island became a popular pilgrimage（朝圣） site. Today, Le Mont-Saint-Michel is a <u>UNESCO</u>（联合国教科文组织） World Heritage （遗产）Site and attracts millions of visitors each year.</p><p>        In addition to its religious significance, the island is also known for its unique tidal <u>phenomenon</u>（现象） , During high tide, the island is completely surrounded by water, giving it the appearance of a floating castle. During low tide, visitors can walk along the exposed sand and explore the surrounding <u>bay</u>（海湾）.</p><p>        If you ever have the chance to visit France, be sure to add Le-Mont-Saint-Michel to your itinerary（行程）. It is a truly magical place that will leave you in awe（惊叹） of its beauty and history.</p><h1 id="6-12"><a href="#6-12" class="headerlink" title="6.12"></a>6.12</h1><p>旅游景点，故宫</p><h2 id="The-Forbidden-City"><a href="#The-Forbidden-City" class="headerlink" title="The Forbidden City"></a>The Forbidden City</h2><p>        <u>The Forbidden City</u>（紫禁城）, also known as the <u>Palace</u>（宫殿） Museum, is a vast imperial（皇家） complex located in the heart of Beijing, China. It served as the home of emperors and their households for almost 500 years, from the Ming dynasty（朝代） to the end of the Qing dynasty. The palace was constructed between 1406 and 1420, and it covers an area of over 720,000 square meters.</p><p>        The Forbidden City is a <u>remarkable</u>（不平常的，值得注意的） showcase of traditional Chinese architecture, with its red walls, yellow glazed roof <u>tiles</u>（瓷砖）, and intricate（精美的） <u>carvings</u>（雕刻品） and <u>decorations</u>（装饰品）. It is composed of over 90 palaces,countyards, and gardens, with more than 8,000 rooms containing a vast collection of precious artworks, ancient artifacts（文物）, and historical documents.</p><p>        Today , the Forbidden City is one of China’s most popular tourist attractions（景点）, attracting millions of visitors each year. It has been recognized as a UNESCO World Heritage Site and is a symbol of China’s rich cultural heritage. A visit to the Forbidden City is a must for anyone interested in Chinese history and culture.</p><h1 id="6-13"><a href="#6-13" class="headerlink" title="6.13"></a>6.13</h1><p>人物</p><h2 id="Diao-Chan"><a href="#Diao-Chan" class="headerlink" title="Diao Chan"></a>Diao Chan</h2><p>        Diao Chan, one of the Four Beauties of ancient China, is a character <u> shrouded in </u> （笼罩）intrigue（阴谋诡计） and <u>mystery</u>（神秘）.     Her story, originating from the historical text “Romance of the Three Kingdoms”（三国演义）, captivates （迷住，吸引）readers with a <u>blend </u> （混合，融合）of beauty, brains, and bravery.</p><p>        Born in the Late Han Dynasty, Diao Chan is celebrated not just for her unrivaled（无与伦比的） beauty. but also for her central role in the “chain plan”. This was a strategy concocted （策划，调制）by Wang Yun, her adopted father, to <u>exploit</u>（利用） the powerful <u>warlord</u>（军阀） Dong Zhuo’s <u>lust</u>（淫欲） and his adopted son Lu Bu’s jealousy, <u>consequently</u> （所以）leading to Dong Zhuo’s downfall.</p><p>        Diao Chan’s <u>allure </u> （诱惑，魅力） was said to eclipse（日食，黯然失色） the moon and shame flowers. Yet, it was her intelligence and courage that truly sets her apart. <u>Deftly</u>（灵巧的） <u>navigating</u>（航行，游走） the dangerous court politics, she sacrificed her happiness for the greater good of the kingdom. Despite her fate being largely undocumented, Diao Chan’s legacy lives on, a <u>testament</u>（遗嘱，证明） to the power of beauty used as a political weapon in turbulent （动荡）times.</p><h1 id="单词记录"><a href="#单词记录" class="headerlink" title="单词记录"></a>单词记录</h1><p><strong>republic 共和国        permit 允许        indifferent 漠不关心的         advance 前进</strong></p><p><strong>accumulate 积累      attribute 特征    superb 最佳的                       unlikely 未必的</strong></p><p><strong>discard 丢弃              adapt 改编         transplant 移植                     exhibit 展览</strong></p><p><strong>interface 接口           abolish 废除       interview 面谈                       transport 运输</strong></p><p><strong>superme 最高的                                   unemployment 失业</strong></p><h1 id="6-14"><a href="#6-14" class="headerlink" title="6.14"></a>6.14</h1><p>节日</p><h2 id="Children’s-Day-in-China"><a href="#Children’s-Day-in-China" class="headerlink" title="Children’s Day in China"></a>Children’s Day in China</h2><p>        Children’s Day is celebrated on June 1st every year in China. It’s a day to honor children and promote （促进）their well-being. On this day. schools often organize fun activities, such as games, performances, and art exhibitions. Parents also give their children gifts or take them out for a special meal.</p><p>        The origins（起源） of Children’s Day can be traced（追溯） back to the World <u>Conference</u>（会议） for the Well-being of Children in <u>Geneva</u>（日内瓦）, <u>Switzerland</u>（瑞士）, in 1925. After the conference, many countries began celebrating Children’s Day on different dates. In China, Children’s Day was first celebrated on June 1st in 1950, and it became a notional holiday in 1956.</p><p>        Children’s Day is an important day for children to feel loved and apprectated. It’s also a reminder for adults to cherish ( 珍惜 ) and protect the rights of children, So let’s celebrate Children’s Day by spending time with the children in our lives and showing them how much we care.</p><h1 id="6-15"><a href="#6-15" class="headerlink" title="6.15"></a>6.15</h1><p>景点</p><h2 id="Hongya-Cave"><a href="#Hongya-Cave" class="headerlink" title="Hongya Cave"></a>Hongya Cave</h2><p>        <u>&amp;nbsp;Hongya Cave</u>（洪崖洞）, nestled（位于） in the heart of Chongqing, China is a <u>marvel</u>（奇迹） that elegantly <u>marries</u> （结合）tradition with modernity. This <u>striking</u>（令人惊叹的） structure, built into the side of a <u>cliff</u>（悬崖）, is <u>reminiscent </u> （回想起）of ancient <u>stilt houses</u>（吊脚楼）, transporting you back to the age of the Ba and Yu kingdoms.</p><p>        By day. the sunlight <u>gleam</u>s（光束） off the red <u>pavilions</u>（亭台）, casting a warm glow that invites visitors into a labyrinth （迷宫，深渊）of shops, restaurants, and cultural exhibitions. Local <u>crafts</u>（手工艺品）, delicacies（美食）, and teas await <u>exploration</u>（探索）, offering a <u>delightful</u>（令人愉快的） glimpse into China’s rich cultural heritage.</p><p>        By night, Hongya Cave <u>transforms</u>（转变） into a dazzling display of lights, <u>mirroring</u>（反射，倒映） its <u>reflection</u> （映像）on the Jialing River below. The <u>captivating</u>（迷人） view, combined with the soft melodies（曲调） of traditional Chinese music <u>wafting</u>（吹送） through the air, makes it a truly enchanting experience.</p><p>            Hongya Cave is more than just a tourist spot; it is a vivd <u>tableau</u>（场景） of China’s history and culture, reflecting its past and embracing its future in its <u>distinctive</u>（独特的） <u>silhouette</u>（轮廓）.</p><h1 id="6-16"><a href="#6-16" class="headerlink" title="6.16"></a>6.16</h1><p>景点</p><h2 id="The-Terracotta-Army"><a href="#The-Terracotta-Army" class="headerlink" title="The Terracotta Army"></a>The Terracotta Army</h2><p>        The <u>Terracotta</u> （赤土，陶瓷）Army is a world-renowned archaeological （考古）site found in Xi’an, China. This stunning underground army, discovered in 1974, was part of  Emperor Qin Shi Huang’s grand mausoleum（陵墓）. Crafted around 210 BCE, the army boasts a vast array of life-sized soldiers, each with distinctive features and expressions, revealing the intricate artistry（工艺，艺术性） of ancient China.</p><p>        The soldiers（士兵）, <u>accompanied</u>（陪伴） by horses and <u>chariots</u>（战车）, were believed to protect the emperor in his afterlife. The scale of the project reveals the emperor’s formidable（强大的） power and determination to control even the <u>realms</u> （领域）beyond life. This site is not just a testament to ancient Chinese military organization but also a <u>monumental</u>（纪念的） symbol of the emperor’s belief in life after death.</p><p>        The Terracotta Army is a fascinating <u>gateway</u>（通路，门） to understand ancient China’s grandeur（庄严，辉煌）, offering valuable insights into the life, beliefs, and artistic skill of a civilization that thrived over two thousand years ago.        </p><h1 id="6-17"><a href="#6-17" class="headerlink" title="6.17"></a>6.17</h1><p>景点</p><h2 id="The-Mysteries-of-Egyptian-Pyramids"><a href="#The-Mysteries-of-Egyptian-Pyramids" class="headerlink" title="The Mysteries of Egyptian Pyramids"></a>The Mysteries of Egyptian Pyramids</h2><p>        The <u>Egyptian pyramids</u>（埃及金字塔） are some of the most <u>iconic</u> （图标，标志性的）and mysterious structures in the world . Built over 4,500 years ago, these colossal （巨大的）<u>tombs</u>（陵墓） were constructed for the <u>pharaohs</u>（暴君，统治者）, who were thr rulers of Egypt.</p><p>        The most famous pyramid is the Great Pyramid of Khufu, which is also the largest and oldest of the three. It was built for the Pharaoh（法老） Khufu and is considered one of the Seven Wonders of the Ancient World. Standing at a staggering （惊人的）146 meters tall, it was the tallest man-made structure for over 3,800 years.</p><p>        The pyramids were constructed using massive <u> limestone</u>（石灰石） blocks, some weighing over 10 tons. The precise methods used to trasport and <u>align</u>（排列） these stones remain a subject of debate among historians and archaeologists. Many theories have been proposed, ranging from <u> ramps</u>（斜坡） and <u>levers</u>（杠杆） to more imaginative ideas like aliens.</p><p>        The Egyptian pyramids continue to captivate the world with their <u>grandeur</u>（庄严） and enigma（迷）. They serve as a testment to the ingenuity （智慧，心灵手巧）and ambition of an ancient civilization that still fascinates us today.</p><h1 id="6-18"><a href="#6-18" class="headerlink" title="6.18"></a>6.18</h1><p>国家，景点</p><h2 id="The-Vatican"><a href="#The-Vatican" class="headerlink" title="The Vatican"></a>The Vatican</h2><p>        <u>&amp;nbsp;The Vatican</u>（梵蒂冈） is a city-state located in the heart of Rome, Italy. It is the smallest independent（独立） state in the world, with an area of just 44 hectares （公顷）and a population of around 800 people. The Vatican is the spirtual（精神的） center of the Roman <u> Catholic Church</u>（天主教会） and the <u>residence</u>（住所） of the <u>Pope</u>（罗马教皇）.</p><p>        The Vatican is home to some of the most important art and <u>architecture</u>（建筑体系） in the world. The <u>Sistine Chapel</u>（西斯廷教堂）, with its famous <u>ceiling</u>（天花板） painted by Michelangelo, is one of the most visited sites in the Vatican. The Basilica（大教堂） of St. Peter, the largest church in the world, is also located in the Vatican.</p><p>        In addition to its religious significance, the Vatican is also an important political <u>entity</u>（实体）. It is a member of the United Nations and has diplomatic（外交） relations with many countries around the world.</p><p>        Visitors of the Vatican can explore its <u>museums</u>（博物馆）, gardens, and libraries, as well as attend <u>Masses</u>（弥撒）. The Vatican is a must-visit destination for anyone interested in art, history, and religion.</p><h1 id="单词记录-1"><a href="#单词记录-1" class="headerlink" title="单词记录"></a>单词记录</h1><h6 id="retrospest-回顾-spectator-观众-exceed-超过-excess-超过，过量"><a href="#retrospest-回顾-spectator-观众-exceed-超过-excess-超过，过量" class="headerlink" title="retrospest 回顾            spectator 观众            exceed 超过        excess 超过，过量"></a>retrospest 回顾            spectator 观众            exceed 超过        excess 超过，过量</h6><h6 id="proceed-继续前进-process-处理，加工-procession-队伍-procedure-程序步骤"><a href="#proceed-继续前进-process-处理，加工-procession-队伍-procedure-程序步骤" class="headerlink" title="proceed 继续前进        process 处理，加工      procession 队伍 procedure 程序步骤"></a>proceed 继续前进        process 处理，加工      procession 队伍 procedure 程序步骤</h6><h6 id="precede-领先-predecessor-前任-recede-后退-access-方法；途径"><a href="#precede-领先-predecessor-前任-recede-后退-access-方法；途径" class="headerlink" title="precede 领先                predecessor 前任        recede 后退        access 方法；途径"></a>precede 领先                predecessor 前任        recede 后退        access 方法；途径</h6><h1 id="6-19"><a href="#6-19" class="headerlink" title="6.19"></a>6.19</h1><p>动物</p><h2 id="The-Space-Octopus"><a href="#The-Space-Octopus" class="headerlink" title="The Space Octopus"></a>The Space Octopus</h2><p>        In the vastness（无垠） of <u>outer space</u>（外太空）, a lone <u>octopus</u> （章鱼）floated in the silence. It had been carried into the <u> cosmos</u>（宇宙） by a research <u>vessel</u>（船）, but now found itself <u>adrift</u>（漂浮） in the <u>void</u>（虚白的，空间）.</p><p>        At first, the octopus was frightened and disoriented（无方向感的，迷失）. But as it floated, it began to adapt to its new environment. Its tentacles（触手）, once used to <u>navigate</u>（航行） the ocean depths, now moved <u>gracefully </u> （优雅的）through the <u>weightlessness</u>（失重） of space.</p><p>        The octopus explored its surroundings, <u>marveling</u>（惊人的） at the stars and planets that surrounded it. It found ways to <u>feed</u> （喂养）itself, catching drifting（漂浮的） <u>particles </u> （粒子）of space dust with its tentacles.</p><p>        Days turned into weeks, and weeks turned into months. The octopus grew stronger and more <u>self-sufficient</u>（自给自足）. It had become a true space <u>pioneer</u>（先驱）, the first of its kind.</p><p>        But as the years passed, the octopus began to feel a sense of loneliness. It longed for the company of other creatures like itself, and <u>yearned</u>（渴望） for the oceans it had left behind.</p><p>        Still, the octopus continued to float through space, a testament to the resilience（恢复力） and adaptability of life in the universe.</p><h1 id="6-20"><a href="#6-20" class="headerlink" title="6.20"></a>6.20</h1><p>景点</p><h2 id="Lapland-A-Northern-Wonder"><a href="#Lapland-A-Northern-Wonder" class="headerlink" title="Lapland: A Northern Wonder"></a>Lapland: A Northern Wonder</h2><p>        Lapland is an enchanting region that stretches across four nations: Norways,Sweden, Finland, and Russia. This area is the embodiment（化身，象征） of pristine（原始的）natural beauty, known for its <u>mesmerizing</u>（迷人的） Northern Lights, <u>unspoiled</u>（未被破坏的） wilderness, and the indigenous（土著） Sami people with their rich culture.</p><p>        In winter, Lapland transforms into a snowy wonderland, inviting visitors to enjoy <u>thrilling</u>（毛骨悚然的，刺激的） activities like dog sledding and <u>snowmobiling</u>（摩托雪橇）. The region is also home to Rovaniemi, the so-called ‘official’ hometown of Santa Claus, charming children and adults alike.</p><p>        In summer, Lapland’s midnight sun creates 24 hours of daylight, a phenomenon that presents a unique sightseeing experience. From hiking in national parks or exploring Sami traditions, there’s always something captivating（引人入胜的，吸引人的） in Lapland.</p><p>        Whether you’re a nature enthusiast（爱好者） or a culture explorer,Lapland offers a memorable <u>blend</u>（混合） of natural wonders and cultural richness. It turly is a magical <u>corner</u>（角落） of the world that invites you to discover its secrets.</p><h1 id="6-21"><a href="#6-21" class="headerlink" title="6.21"></a>6.21</h1><p>国家</p><h2 id="Vienna-A-Symphony-of-Culture"><a href="#Vienna-A-Symphony-of-Culture" class="headerlink" title="Vienna: A Symphony of Culture"></a>Vienna: A Symphony of Culture</h2><p>        Vienna, Austria’s capital, is a city <u>steeped</u>（充满的） in culture, history, and music. This charming city, with its elegant architecture and refined（优雅的） culture, sits like a jewel in the heart of Europe.</p><p>        A city of music, Vienna is known as the brithplace（生源地） of classical music, where legends like Mozart, Beethoven, and <u>Schubert</u>（舒伯特） once lived and worked. Its <u>STATE Opera</u>（国家歌剧院） and Musikverein are internationally recoginized temples of music.</p><p>        In addition to its musical legacy, Vienna is renowned for its historical museums and art galleries. From the imperial grandeur（宏伟的） of <u>Schonbrunn</u> （美泉宫）<u>Palace</u>（宫殿） to the artistic treasures housed in the Museum of Fine Arts, there is an <u>intriguing</u>（吸引人的） historical <u>narrative</u>（叙述故事） woven into every corner of the city.</p><p>        Vienna’s culinary （美食，烹饪的）scene is also a <u>delightful</u>（令人愉快的） <u>fusion</u>（融合） of local and international flavors, with its famous coffee houses and vineyard-speckled hills adding to its <u>allure</u>（魅力）.</p><p>        Vienna, a city that beautifully blends tradition and modernity, invites you to <u>immerse </u> （沉沦）yourself in its rich tapestry（挂毯） of experiences.</p><h1 id="6-22"><a href="#6-22" class="headerlink" title="6.22"></a>6.22</h1><p>景点</p><h2 id="Mahe-Island-Seychelles’-Crown-Jewel"><a href="#Mahe-Island-Seychelles’-Crown-Jewel" class="headerlink" title="Mahe Island: Seychelles’ Crown Jewel"></a>Mahe Island: Seychelles’ Crown Jewel</h2><p>        <mark>Tucked</mark>（隐藏） in the<mark> azure</mark>（碧蓝） waters of the Indain Ocean, Mahe Island, the largest in Seychelles, stands as a <u>paradise</u>（天堂） for beach lovers. Its <u>verdant</u>（翠绿的） landspcape, painted with <u>lush</u>（郁郁葱葱） hills and stunning coastlines（海岸线）, is truly a sight to <u>behold</u>（注视）.</p><p>        This <u>idyllic</u> （田园）island is known for its <mark>exquisite</mark>（精致的） beaches like Beau Vallon and Anse Royale. Nature <u>enthusiasts</u>（爱好者） will <u>revel in</u> （陶醉于）exploring the Morne Seychellois National Park, teeming （充满）with <u>exotic</u> （异国）fauna and lush vegetation.</p><p>        Mahe also <u>nurtures</u>（培育） Seychelles’ <u>vibrant</u>（活跃的） culture, reflected in the Creole <u>architecture</u>（建筑） and <u>cuisine</u>（菜肴）. Victoria, its capital, boasts a bustling market and a clock tower that echoes London’s Big Ben.</p><p>        Whether <u>basking</u>（晒） in the sun, diving into <u>crystal-clear</u>（透明的、清澈的） waters, or indulging in the island’s history and culture, Mahe offers an unforgettable experience that leave visitors <mark>yearning</mark>（向往的） for more.</p><h1 id="6-23"><a href="#6-23" class="headerlink" title="6.23"></a>6.23</h1><p>经济</p><h2 id="Bitcoin"><a href="#Bitcoin" class="headerlink" title="Bitcoin"></a>Bitcoin</h2><p>        Bitcoin is a <u>digital</u>（数字的）<u>currency</u>（货币）that was created in 2009 by an unknown person using the name Satoshi Nakmamoto. Transactions with Bitcoin are made without<mark> intermediaries</mark>（中间人）, meaning no banks or governments are involved. Instead, users can send and receive bitcoins using a peer-to-peer network.</p><p>        One of the key features of Bitcoin is that it operates on a <u>decentralized</u>（分散的，去中心化的） system, which means that it is not controlled by any government or <u>institution</u>（机构）. This has made it popular among people who are concerned about government <u>interference</u>（干涉，交涉） in financial transactions.</p><p>        Another advantage of Bitcoin is that transactions are <mark>anonymous</mark>（匿名）, which means that users can keep their identities private. However , this has also made it <u>attractive</u>（青睐，吸引） to criminals who use it for illegal purposes such as money laundering.</p><p>        Despite its popularity, Bitcoin remains a <u>volatile</u>（波动性的） investment. Its value has <mark>fluctuated</mark>（波动） greatly over the years and there is no guarantee（保证） that it will continue to <u>appreciate</u>（涨值）. Nevertheless , many people believe that Bitcoin has the potential to <u>revolutionize</u> （革新）the way we think about money and finance.</p><h1 id="6-24"><a href="#6-24" class="headerlink" title="6.24"></a>6.24</h1><p>人物</p><h2 id="Barack-Obama"><a href="#Barack-Obama" class="headerlink" title="Barack Obama"></a>Barack Obama</h2><p>        Barack Obama was the 44th President of the United States, serving from 2009 to 2017. He was born in Hawaii in 1961 and raised by his mother and grandparents. Obama attended Columbia University in New York City and later went on to attend Harvard Law School.</p><p>        After law school, Obama worked as a community organizer in Chicago and taught constitiutional（宪法） law at the University of Chicago Law School. He was elected to the illinois State <u>Senate</u>（参议院） in 1996 and served there until 2004, when he was elected to the US Senate.</p><p>        In 2008, Obama ran for President and won, becoming the first African American to hold the office. During his presidency, he signed into law the <mark>Affordable</mark>（普及的，平价的） Care Act, which aimed to provide health insurance for all Americans. He also <mark>oversaw</mark>（监督） the killing of Osama bin Laden, the <mark>mastermind</mark>（头目） behind the September 11th terrorist attacks.</p><p>        Today, Obama is a bestselling author and public speaker. He continues to work on issues such as climate change and criminal justice reform through his <mark>foundation</mark>（基金会）, the Obama Foundation.</p><h1 id="6-25"><a href="#6-25" class="headerlink" title="6.25"></a>6.25</h1><p>人物</p><h2 id="My-Childhood-Hero"><a href="#My-Childhood-Hero" class="headerlink" title="My Childhood Hero"></a>My Childhood Hero</h2><p>        When I was a child, I <u>looked up</u> to my grandfather as my hero. He was a hardworking farmer who always  had a smile on his face and a kind word to say. He taught me the value of hard work and <mark>perseverance</mark>（毅力）, and showed me that anything is possible if you set your mind to it.</p><p>        As I grew older, my grandfather’s  lessons stayed with me. I worked hard in school, and eventually went on to college. Even when times were <mark>tough</mark>（困难）, I remembered my grandfather’s example and refused to give up.</p><p>        Now, as an adult, I realize just how much my grandfather influenced my life. He taught me to be kind, to work hard, and to never give up on my dream. He may no longer be with us, but his <mark>legacy</mark>（遗产） lives on through me and the lessons he taught me.</p><p>        Looking back on my childhood, I am <mark>grateful</mark> （感激）for the time I spent with my hero, and I know that I will carry his lessons with me for the rest of my  life.</p><h1 id="6-26"><a href="#6-26" class="headerlink" title="6.26"></a>6.26</h1><p>城市</p><h2 id="Copenhagen-A-Blend-of-Old-and-New"><a href="#Copenhagen-A-Blend-of-Old-and-New" class="headerlink" title="Copenhagen: A Blend of Old and New"></a>Copenhagen: A Blend of Old and New</h2><p>        Nestled on the eastern coast of Zealand, Copenhagen, Denmark’s capital, presents a fascinating <u>fusion</u>（融合） of traditional charm and mordern <u>flair</u>（鉴别能力）.</p><p>        The city’s <mark>cobbled</mark>（鹅卵石） streets, <mark>laced with</mark>（点缀着） colourful townhouses, lead to iconic<u> landmarks</u>（目标，象征） like the Little Mermaid statue and Tivoli Cardens. Yet,Copenhagen also nurtures a <mark>thriving</mark> （繁华的）<u>contemporary</u> （同时代的）scene, with spaces such as the Danish Design Museum and <u>innovative</u>（创新的） restaurants showcasing New Nordic cuisine. </p><p>        From cycling along the <u>vibrant</u>（活力的） Nyhavn <u>harbor</u>（港口） to exploring the treasures of Rosenborg <u>Castle</u>（城堡）, Copenhagen <u>caters</u>（满足，迎合） to a variety of tastes. It’s a city that <mark>enthralls</mark>（陶醉，迷住） visitors with its rich history and <u>invigorates</u>（生机勃勃的） with its dynamic present , leaving a lasting impression.</p><h1 id="6-26-1"><a href="#6-26-1" class="headerlink" title="6.26"></a>6.26</h1><p>城市</p><h2 id="Northern-Germany-Wismar"><a href="#Northern-Germany-Wismar" class="headerlink" title="Northern Germany: Wismar"></a>Northern Germany: Wismar</h2><p>        Nestled in northern Germany, the Hanseatic city of Wismar is a <u>delightful </u>（完美的）blend of <mark>antiquity</mark>（古老的） and modernity. This UNESCO World Heitage Site, abundant in historical <mark>allure</mark>（魅力）, never fails to captivate visitor.</p><p>         Founded in the 13th <u>century</u>（世纪）, Wismar’s <u>maritime</u>（海上的） <u>legacy</u>（传统） is <u>manifested</u>（显示） in its impressive harbor, once a significant <u>hub</u>（中心，枢纽） in the Hanseatic League. Gabled houses, red-brick Gothic  architecture, and cobblestone streets <u>dominate</u>（支配，构成） the city’s enchanting old town. St.George’s Church, a  symbol of Wismar’s rich past, stands tall <u>amidst</u>（在…当中） the urban landscape, its <u>grandeur</u>（庄严） a <mark>testament</mark>（见证） to the city’s <u>resilience</u>（弹性，韧性）.</p><p>        This city also embraces the future with open arms. Alongside the historical structures, you will find <u>contemporary</u>（当代的） art galleries, bustling cafes, and innovative local businesses, perfectly illustrating the city’s dynamic <mark>juxtaposition</mark>（交融,并列） of old and new.</p><p>        In short, Wismar offers an <u>immersive</u>（拟真的） experience; it is a living museum that beautifully <mark>intertwines</mark> （交织）its rich history with an ever-evolving present.</p><h1 id="6-27"><a href="#6-27" class="headerlink" title="6.27"></a>6.27</h1><p>地震</p><h2 id="Emergency-Planning-for-Earthquakes"><a href="#Emergency-Planning-for-Earthquakes" class="headerlink" title="Emergency  Planning for Earthquakes"></a>Emergency  Planning for Earthquakes</h2><p>        Earthquakes are a natural <mark>phenomenon</mark>（现象） that can occur anywhere in the world. They are caused by the movement of <mark>tectonic</mark>（构造） plates<u> beneath </u>（在…下面）the Earth’s surface, which can result in sudden and violent shaking of the ground. Earthquakes can vary in intensity, with some being barely noticeable while others can cause widespread destruction.</p><p>        In order to prepare for earthquakes, it is important to be aware of the potential riks and to take steps to <mark>minimize</mark> （最小化）their impact. This can include securing furniture and other objects that may fall during an earthquake, creating an emergency plan with your family, and <mark>stocking up</mark>（囤积） on essential supplies such as food, water, and first aid kits.</p><p>        In the event of an earthquake, it is important to stay calm and seek shelter in a  safe location. This may include under a <mark>sturdy</mark>（坚固的） table or desk. If you are outdoors, move away from buildings and other structures that may be at risk of <mark>collapsing.</mark>（崩塌）</p><p>        By being prepared and knowing what to do in the event of an earthquake, you can help to keep yourself and those around you safe.</p><h1 id="6-28"><a href="#6-28" class="headerlink" title="6.28"></a>6.28</h1><h2 id="Overcoming-Procrastination"><a href="#Overcoming-Procrastination" class="headerlink" title="Overcoming Procrastination"></a>Overcoming Procrastination</h2><p>        <mark>Procrastination</mark>（拖延症） is a common problem that affects many people. It is the act of <mark>postponing</mark> （推迟）tasks need to be done.  While it may seem harmless at first, it can lead to stress , anxiety ,and poor performance. The good news is that procrastination is a habit that can be overcome.</p><p>        To overcome procrastination, it is important to understand why it happends. Often, people procrastinate because they feel <mark>overwhelmed</mark>（不知所措） ot unsure of how to start a task. To avoid this, break tasks into smaller, more manageable pieces. This can help you feel more in control and less <u>intimidated</u>（恐惧感）.</p><p>        Another way to overcome procrastination is to set goals and deadlines. This can help you stay focused and <u>motiviated</u>（动力的）. It is also important to <mark>eliminate</mark> <u>distractions</u>（干扰，分心的）, such as social media or the TV, that can take away from your <u>productivity</u>（生产力，生产效率）.</p><p>        Remember, overvcoming  procrastination takes time and effort. Start small and celebrate your progress along the way. With <mark>persistence</mark>（坚持） and determination, you can break the habit of procrastination and achieve your goals.</p><h1 id="6-29"><a href="#6-29" class="headerlink" title="6.29"></a>6.29</h1><p>辣椒</p><h2 id="The-Secret-of-Eating-Spicy-Food"><a href="#The-Secret-of-Eating-Spicy-Food" class="headerlink" title="The Secret of Eating Spicy Food"></a>The Secret of Eating Spicy Food</h2><p>        If you enjoy eating spicy food, you might wonder why some people can handle it better than others. The secret lies in a chemical called <mark>capsaicin</mark>（辣椒素）, which is found in<u> chilipeppers</u>（辣椒）.</p><p>        Capsaicin causes a burning <mark>sensation</mark>（感觉） by stimulating <u>nerve</u> （神经）endings in the mouth and throat. However, over time, people can bulit up a <mark>tolerance</mark>（耐受度） to capsaicin by eating spicy food regularly. This is because the nerve ending become less sensitive less sensitive to the chemical.</p><p>        Other factors may also influence your tolerance for spicy food. For example, genetics play a role in determining how <mark>sensitive</mark> （敏感的）you are to capsaicin. Additionally, eating <u>fatty</u>（脂肪） foods before consuming spicy food can help to reduce the burning sensation.</p><p>        While some people may find eating spicy food uncomfortable, others enjoy the   rush of <u>endorphins</u>（多巴胺） that come with it. So if you’re a fan of spicy food, embrace the heat and enjoy the unique flavors and sensations that come with it.</p><h1 id="7-1"><a href="#7-1" class="headerlink" title="7.1"></a>7.1</h1><h2 id="The-Great-Wall-of-China"><a href="#The-Great-Wall-of-China" class="headerlink" title="The Great Wall of China"></a>The Great Wall of China</h2><p>        The Great of China is an iconic symbol of China and one of the world’s most famous <u>landmarks</u>.（标志） It is a series of walls and <mark>fortifications</mark>（防御工事） that were built along the northern boders of China during different dynasties to protect the country from invasions.</p><p>        The Great Wall is over 13,000 miles long and is made up of many different sections, some of which have been restored and are open to tourists. The most popular section is the one near Beijing, which is known as the Badaling Great Wall. It is a <mark>magnificent</mark>（宏伟的） sight to <u>behold</u>（被看到） and attracts millions of tourists every year.</p><p>        The Great Wall was built over 2,000 years ago and is a teastamenrt to the <mark>ingenuity</mark>（聪明才智） and determination of the Chinese people. It is a reminder of China’s rich history and cultural heritiage and has become a symbol of national pride.</p><p>        Visiting the Great Wall is an unforgettable experience. As you walk along the wall, you can feel the history and significance of this <mark>incredible</mark>（不可思议的） structure. It is truly one of the world’s greatest wonders and a must-see for anyone visiting China.</p><h1 id="7-2"><a href="#7-2" class="headerlink" title="7.2"></a>7.2</h1><h2 id="The-Evolution-of-Insurance-Industry"><a href="#The-Evolution-of-Insurance-Industry" class="headerlink" title="The Evolution of Insurance Industry"></a>The Evolution of Insurance Industry</h2><p>        The insurance industry has a long and <mark>fascinating</mark>（迷人的） history that dates backs to ancient times. One of the earliest recorded examples of insurance comes from ancient China, where <u>merchants</u> （商人）would <mark>distribute</mark>（分布） their goods across multiple ships to minimize the risk of loss due to <u>piracy</u> （海盗行为）or accidents at sea.</p><p>        In Europe, the insurance industry began to take shape in the 14th century when merchants and <u>traders</u> （交易者）would  gather in coffee houses to discuss business and financial <mark>transactions</mark>（处理，交易）. These gatherings eventually led to the formation of the first insurance companies, which offered protection against losses due to fire, theft,and other risk.</p><p>        The modern insurance industry, as we know it today, began to emerge  in the late 19th and early 20th centuries. Major advancements in transportation  and communication technology made it easier for insurance companies to <mark>assess </mark>（评估）risk and offer policies to a wider range of customers.</p><p>        Today, the insurance industry is a vital part of the global economy, providing protection and financial  security to individuals and businesses around the world. A s new risks <u>emerge</u>（浮现）, such as <u>cyber</u>（计算机的） threats and climate change, the industry continues to evolve and <mark>adapt</mark>（发展） to meet the changing needs of its customers.</p><h1 id="7-3"><a href="#7-3" class="headerlink" title="7.3"></a>7.3</h1><h2 id="The-History-and-Culture-of-Tea"><a href="#The-History-and-Culture-of-Tea" class="headerlink" title="The History and Culture of Tea"></a>The History and Culture of Tea</h2><p>        Tea has a rich history and culture that <mark>spans</mark> （跨越）thousands of years. It is believed to have originated in China, where it was first used for medicinal purposes before it become a popular beverage.</p><p>        The Chinese have a long-standing tradition of tea culture, with <mark>elaborate</mark>（精致的） tea <u>ceremonies</u>（礼节） and a deep <u>appreciation </u>（鉴赏）for the art of <mark>brewing</mark>（泡） and serving tea. Tea was also an important part of the Japanese tea ceremony, which was developed in the 16th century. </p><p>        Tea was introduced to Europe in the 17th century, where it quickly  became popular among the wealthy. It was also heavily traded and taxed, leading to the <mark>infamous</mark>（声名狼藉的） Boston Tea Party in 1773, a key event in the American Revolution.</p><p>        Today, tea is enjoyed all over the world, with different countries and cultures developing their own unique tea traditions. In India, for example, chai is a popular spiced tea that is often served with milk and sugar. In the UK, afternoon tea is a beloved tradition that includes tea, sandwiches, and <u>pastries</u>（甜点）.</p><p>        Despite the many <mark>variations</mark>（变化） in tea culture, one thing remains constant: the sense of calm and relaxation that comes from <u>sipping</u> （小口抿）on a warm cup of tea.</p><h1 id="7-4"><a href="#7-4" class="headerlink" title="7.4"></a>7.4</h1><h2 id="Volunteering-Giving-Back-to-Your-Community"><a href="#Volunteering-Giving-Back-to-Your-Community" class="headerlink" title="Volunteering: Giving Back to Your Community"></a>Volunteering: Giving Back to Your Community</h2><p>       <mark> Volunteering</mark>（志愿服务） is a great way to give back to your <u>community</u>（社区） and help those in need. By volunteering, you can make a positive impact on the lives of others while also gaining valuable experience and skills.</p><p>        There are many different types of volunteer <mark>opportunities </mark>（机会）<u>available</u>（可获得的）, from working with children or the elderly to helping animals or the environment. You can choose to volunteer for a one-time event or <u>commit</u>（委托，做） to a regular schedule.</p><p>        Volunteering is not only beneficial to the community , but also to the volunteer. It can help you develop new skills, build your <mark>resume</mark>（简历）,and meet new people. Additionnally, volunteering can be a great way to give back to a <mark>cause</mark>（目标,事业） that is important to you.        </p><p>        Remember, volunteering is not just about giving back, but also about personal growth and development. So, give it a try and see how it can benefit both you and your community.</p><h1 id="7-8"><a href="#7-8" class="headerlink" title="7.8"></a>7.8</h1><h2 id="Climate-Change-A-Global-Challenge"><a href="#Climate-Change-A-Global-Challenge" class="headerlink" title="Climate Change: A Global Challenge"></a>Climate Change: A Global Challenge</h2><p>        Climate change is a <u>pressing</u>（紧迫的） issue facing our world today. It refers to the long-term changes in <u>temperature</u>,（温度） <mark>precipitation</mark>（降水）, and other weather patterns caused by humman activities, such as burning fossil fuels and <u>deforestation</u>（砍伐树木）.</p><p>        The impacts of climate change are far-reaching and affect every aspect of our lives, from the food we eat to the air we breathe.Rising temperatures are causing glaciers to melt, sea levels to rise, and extreme weather events  to become more frequent and severe. This has devastating consequences fro ecosystems around the world.</p><p>        Climate change also contributes to the loss of <mark>bilodiversity</mark>（生物多样性）, as species struggle to adapt to changing conditions. It affects agriculture and food security, as changing weather patterns and extreme weather events make it harder to grow crops and raise <u>livestock</u>（家畜）.</p><p>            We all have a role to play in addressing climate change. Governments, businesses, and individuals can take actions to reduce greenhouse gas emissions, such as transitioning to renewable energy sources and reducing waste. By working together, we can create a more sustainable future for ourselves and for generations to come.</p>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 英语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据库常考知识点复习</title>
      <link href="/2023/06/24/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B8%B8%E8%80%83%E7%9F%A5%E8%AF%86%E7%82%B9%E5%A4%8D%E4%B9%A0/"/>
      <url>/2023/06/24/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%B8%B8%E8%80%83%E7%9F%A5%E8%AF%86%E7%82%B9%E5%A4%8D%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="SQL语句"><a href="#SQL语句" class="headerlink" title="SQL语句"></a>SQL语句</h1><h6 id="创表"><a href="#创表" class="headerlink" title="创表"></a>创表</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> 表名(</span><br><span class="line">    id number(<span class="number">10</span>) <span class="keyword">primary</span> key <span class="keyword">not</span> <span class="keyword">null</span>, <span class="operator">/</span><span class="operator">/</span>列名 类型 主键 不为空</span><br><span class="line">    name <span class="type">varchar</span>(<span class="number">20</span>) <span class="keyword">not</span> <span class="keyword">null</span>, <span class="operator">/</span><span class="operator">/</span><span class="type">varchar</span> (可变长度，指定最大长度<span class="number">20</span>字节) 不为空</span><br><span class="line">    mobile <span class="type">varchar</span>(<span class="number">11</span>) <span class="keyword">check</span>(length(mobile)<span class="operator">=</span><span class="number">11</span>) <span class="keyword">unique</span> <span class="operator">/</span><span class="operator">/</span>约束长度等于<span class="number">11</span> 取唯一值</span><br><span class="line">    <span class="keyword">constraint</span> 自命名 <span class="keyword">foreign</span> key(address) <span class="keyword">references</span> Massage(address) <span class="operator">/</span><span class="operator">/</span>address是外码，被参照表是Massage</span><br><span class="line">    <span class="keyword">constraint</span> 自命名 <span class="keyword">primary</span> key(mobile)</span><br><span class="line">)</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>常用数据类型</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="type">varchar</span>(size) : 存储可变长度字符串， size 规定字符串最大长度</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> number(m,n)  : m 表示总长度，n表示小数位的精度，只有m表示可以存入最大为m位的整数</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> <span class="type">date</span> : 表示日期和时间，<span class="number">7</span>个字节固定宽度，有<span class="number">7</span>个属性，分别为世纪<span class="operator">-</span>年<span class="operator">-</span>月<span class="operator">-</span>日<span class="operator">-</span>小时<span class="operator">-</span>分<span class="operator">-</span>秒</span><br></pre></td></tr></table></figure><h6 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> 视图名 <span class="keyword">as</span> <span class="keyword">select</span> ....;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> 视图名;</span><br><span class="line"><span class="keyword">view</span> 和 <span class="keyword">with</span> <span class="keyword">as</span> 的区别：<span class="keyword">view</span> 创建后不删除就一直都还在，<span class="keyword">with</span> <span class="keyword">as</span> 执行后就不存在了</span><br></pre></td></tr></table></figure><p>例题：建立一个视图V1，显示老师与学生的授课关系，包括年份，学期，课程名称，老师ID，老师姓名，学生ID，学生姓名</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> v1 <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">year</span>,semester,title,a.id i_id,d.name</span><br><span class="line">i_name,b.id s_id,c.name s_name</span><br><span class="line"><span class="keyword">from</span> takes a <span class="keyword">join</span> teaches b </span><br><span class="line"><span class="keyword">using</span>(course_id,sec_id,<span class="keyword">year</span>,semester)</span><br><span class="line"><span class="keyword">join</span> student c <span class="keyword">on</span>(a.id <span class="operator">=</span> c.id)</span><br><span class="line"><span class="keyword">join</span> instructor d <span class="keyword">on</span> (b.id <span class="operator">=</span> d.id)</span><br><span class="line"><span class="keyword">join</span> course <span class="keyword">using</span>(course_id)</span><br></pre></td></tr></table></figure><h6 id="向表中添加或删除约束"><a href="#向表中添加或删除约束" class="headerlink" title="向表中添加或删除约束"></a>向表中添加或删除约束</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="operator">/</span><span class="operator">/</span> 添加主键约束</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> 表名 <span class="keyword">add</span> <span class="keyword">constraint</span> 自定义主键名 <span class="keyword">primary</span> key(字段)</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>添加外键约束</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> 表名 <span class="keyword">add</span> <span class="keyword">constraint</span> 自定义外键名 <span class="keyword">foreign</span> key(外键字段) </span><br><span class="line"><span class="keyword">references</span> 表名(字段)</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>删除主键约束</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> 表名 <span class="keyword">drop</span> <span class="keyword">constraint</span> 主键名</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>删除外键约束</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> 表名 <span class="keyword">drop</span> <span class="keyword">constraint</span> 外键名</span><br></pre></td></tr></table></figure><h6 id="添加信息"><a href="#添加信息" class="headerlink" title="添加信息"></a>添加信息</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> 表名 <span class="keyword">values</span>(值<span class="number">1</span>,值<span class="number">2</span>,....);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> 表名(字段<span class="number">1</span>，字段<span class="number">3</span>) <span class="keyword">values</span>(值<span class="number">1</span>，值<span class="number">3</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> 表名 <span class="keyword">select</span> ...;</span><br></pre></td></tr></table></figure><p>例题：给“Aufr”同学选上2010年秋季学期的所有课程</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> takes</span><br><span class="line"><span class="keyword">select</span> id,course_id,sec_id,semester,<span class="keyword">year</span>,<span class="keyword">null</span></span><br><span class="line"><span class="keyword">from</span> student a,section b</span><br><span class="line"><span class="keyword">where</span> a.name<span class="operator">=</span><span class="string">&#x27;Aufr&#x27;</span> <span class="keyword">and</span> b.year <span class="operator">=</span> <span class="number">2010</span> <span class="keyword">and</span> b.semester</span><br><span class="line"><span class="operator">=</span> <span class="string">&#x27;Fall&#x27;</span>;</span><br></pre></td></tr></table></figure><h6 id="删除信息"><a href="#删除信息" class="headerlink" title="删除信息"></a>删除信息</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> 表名 <span class="keyword">where</span> 条件;</span><br></pre></td></tr></table></figure><p>例题：删除“Comp. Sci.”学院“Ploski”同学，所有成绩为’C-’的选课记录</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> takes a</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">exists</span>(<span class="keyword">select</span> <span class="number">1</span> <span class="keyword">from</span> student b <span class="keyword">where</span> a.id<span class="operator">=</span>b.id</span><br><span class="line"><span class="keyword">and</span> b.dept_name <span class="operator">=</span> <span class="string">&#x27;Comp. Sci.&#x27;</span> <span class="keyword">and</span> b.name <span class="operator">=</span><span class="string">&#x27;Ploski&#x27;</span>)</span><br><span class="line"><span class="keyword">and</span> a.grade <span class="operator">=</span> <span class="string">&#x27;C-&#x27;</span>;</span><br></pre></td></tr></table></figure><h6 id="更新信息"><a href="#更新信息" class="headerlink" title="更新信息"></a>更新信息</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> 表名 <span class="keyword">set</span> 字段<span class="operator">=</span><span class="keyword">new</span>字段 <span class="keyword">where</span> 条件;</span><br></pre></td></tr></table></figure><p>例题： 将“Comp. Sci.” 学院所有低于学校平均工资老师的涨薪10%，但是最高不能超过学校平均工资</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">update</span> instructor</span><br><span class="line"><span class="keyword">set</span> salary <span class="operator">=</span></span><br><span class="line"><span class="keyword">case</span> <span class="keyword">when</span> salary <span class="operator">*</span><span class="number">1.1</span> <span class="operator">&gt;</span> (<span class="keyword">select</span> <span class="built_in">avg</span>(salary) <span class="keyword">from</span></span><br><span class="line">instructor) <span class="keyword">then</span></span><br><span class="line">(<span class="keyword">select</span> <span class="built_in">avg</span>(salary) <span class="keyword">from</span></span><br><span class="line">instructor)</span><br><span class="line"><span class="keyword">else</span> salary <span class="operator">*</span> <span class="number">1.1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">where</span> dept_name <span class="operator">=</span> <span class="string">&#x27;Comp. Sci.&#x27;</span></span><br><span class="line"><span class="keyword">and</span> salary <span class="operator">&lt;</span> (<span class="keyword">select</span> <span class="built_in">avg</span>(salary) <span class="keyword">from</span> instructor);</span><br></pre></td></tr></table></figure><h5 id="查询常用函数"><a href="#查询常用函数" class="headerlink" title="查询常用函数"></a>查询常用函数</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="built_in">avg</span>() :求平均值</span><br><span class="line"><span class="keyword">distinct</span> : 去重</span><br><span class="line"><span class="built_in">max</span>() : 求最大值</span><br><span class="line"><span class="built_in">min</span>() : 求最小值</span><br><span class="line"><span class="built_in">sum</span>() : 求和</span><br><span class="line"><span class="built_in">count</span>() : 求记录的行数</span><br><span class="line"><span class="built_in">count</span>(<span class="operator">*</span>) : 包括<span class="keyword">null</span></span><br><span class="line"><span class="built_in">count</span>(字段) : 该字段中不为<span class="keyword">null</span> 的行数</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> 字段 : 按字段分组</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> 字段,字段 : 按字段排序，<span class="keyword">desc</span> 降序，默认为升序</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span> 合并不去重</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>窗口函数</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>排序<span class="built_in">rank</span>(),<span class="built_in">dense_rank</span>(),<span class="built_in">row_number</span>()</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span> row_number 不存在并列，不会有相同的数字</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>dense_rank 存在并列，不会跳数字</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span><span class="built_in">rank</span>() 存在并列，会出现数字的中断</span><br><span class="line"><span class="keyword">select</span> id,score,<span class="built_in">row_number</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">as</span> row_number,</span><br><span class="line"><span class="built_in">dense_rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">as</span> dense_rank1,</span><br><span class="line"><span class="built_in">rank</span>() <span class="keyword">over</span> (<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) <span class="keyword">as</span> rank1</span><br><span class="line"><span class="keyword">from</span> scores</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span></span><br><span class="line"><span class="keyword">over</span> : 在什么条件之上</span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span>  字段 : 按字段划分</span><br></pre></td></tr></table></figure><table><thead><tr><th>id</th><th>score</th><th>row_number1</th><th>dense_rank1</th><th>rank1</th></tr></thead><tbody><tr><td>01</td><td>99</td><td>1</td><td>1</td><td>1</td></tr><tr><td>03</td><td>99</td><td>2</td><td>1</td><td>1</td></tr><tr><td>02</td><td>88</td><td>3</td><td>2</td><td>3</td></tr></tbody></table><h6 id="例题：使用标量子查询，查询各院开设课程修课人数最多的前三门课程"><a href="#例题：使用标量子查询，查询各院开设课程修课人数最多的前三门课程" class="headerlink" title="例题：使用标量子查询，查询各院开设课程修课人数最多的前三门课程"></a>例题：使用标量子查询，查询各院开设课程修课人数最多的前三门课程</h6><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> ta <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> dept_name,course_id,title,<span class="built_in">count</span> (<span class="keyword">distinct</span> id) cnt</span><br><span class="line"> <span class="keyword">from</span> course <span class="keyword">natural</span> <span class="keyword">join</span> takes</span><br><span class="line"> <span class="keyword">group</span> <span class="keyword">by</span> dept_name,course_id,title),</span><br><span class="line">tb <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> dept_name,course_id,title,cnt,<span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> </span><br><span class="line"><span class="keyword">by</span> dept_name <span class="keyword">order</span> <span class="keyword">by</span> cnt <span class="keyword">desc</span>) rk</span><br><span class="line"><span class="keyword">from</span> ta)</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb</span><br><span class="line"><span class="keyword">where</span> rk<span class="operator">&lt;=</span><span class="number">3</span>;</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>不使用窗口函数</span><br><span class="line"><span class="keyword">with</span> ta <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> dept_name,course_id,title,<span class="built_in">count</span>(<span class="keyword">distinct</span> id) cnt</span><br><span class="line"><span class="keyword">from</span> course <span class="keyword">natural</span> <span class="keyword">join</span> takes</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> dept_name,course_id,title),</span><br><span class="line">tb <span class="keyword">as</span></span><br><span class="line">(<span class="keyword">select</span> dept_name,course_id,title,cnt,</span><br><span class="line">(<span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>)<span class="operator">+</span><span class="number">1</span> <span class="keyword">from</span> ta b</span><br><span class="line"><span class="keyword">where</span> a.dept_name <span class="operator">=</span> b.dept_name <span class="keyword">and</span> b.cnt <span class="operator">&gt;</span> a.cnt) rk</span><br><span class="line"><span class="keyword">from</span> ta a)</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">where</span> rk <span class="operator">&lt;=</span> <span class="number">3</span> <span class="keyword">order</span> <span class="keyword">by</span> dept_name,rk</span><br></pre></td></tr></table></figure><h6 id="exists-和-not-exists-的使用"><a href="#exists-和-not-exists-的使用" class="headerlink" title="exists 和 not exists 的使用"></a>exists 和 not exists 的使用</h6><p>eixsts()会返回具体的查询到的数据，只是会返回true或者false，如果外层sql的字段在子查询中存在则返回true，不存在则返回false</p><p>双not exists 的使用</p><p>例题：查询修了ID&#x3D;‘82402‘同学所有选修课程的同学的ID，姓名</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> id, name</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>同学</span><br><span class="line"><span class="keyword">from</span> student a</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>不存在</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">not</span> <span class="keyword">exists</span>(</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>这样一门课程</span><br><span class="line">    <span class="keyword">select</span> <span class="number">1</span> <span class="keyword">from</span> takes b</span><br><span class="line"><span class="operator">/</span><span class="operator">/</span>这门课程<span class="number">82402</span>选了，但她没选</span><br><span class="line"><span class="keyword">where</span> id<span class="operator">=</span><span class="string">&#x27;82402&#x27;</span> <span class="keyword">and</span> <span class="keyword">not</span> <span class="keyword">exists</span>(</span><br><span class="line">    <span class="keyword">select</span> <span class="number">1</span> <span class="keyword">from</span> takes c</span><br><span class="line"><span class="keyword">where</span> a.id<span class="operator">=</span>c.id <span class="keyword">and</span> b.course_id<span class="operator">=</span>c.course_id)</span><br><span class="line">);</span><br></pre></td></tr></table></figure><h6 id="case-的使用"><a href="#case-的使用" class="headerlink" title="case 的使用"></a>case 的使用</h6><p>CASE语句遍历条件并在满足第一个条件时返回一个值（如IF-THEN-ELSE语句）。因此，一旦条件为真，它将停止读取并返回结果。如果没有条件为 true，则返回 ELSE 子句中的值。</p><p>如果没有其他部分，并且没有条件为 true，则返回 NULL。</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CASE</span></span><br><span class="line"> <span class="keyword">WHEN</span> condition1 <span class="keyword">THEN</span> result1</span><br><span class="line"> <span class="keyword">WHEN</span> condition2 <span class="keyword">THEN</span> result2</span><br><span class="line"> <span class="keyword">WHEN</span> conditionN <span class="keyword">THEN</span> resultN</span><br><span class="line"> <span class="keyword">ELSE</span> <span class="keyword">result</span></span><br><span class="line"><span class="keyword">END</span>;</span><br></pre></td></tr></table></figure><p>例题：查询各课程各级成绩人数，显示课程名称，A级人数，B级人数，C级人数，总人数</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> title,</span><br><span class="line"><span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> grade <span class="keyword">like</span> <span class="string">&#x27;A%&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">null</span> <span class="keyword">end</span>) A,</span><br><span class="line"><span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> grade <span class="keyword">like</span> <span class="string">&#x27;B%&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">null</span> <span class="keyword">end</span>) B,</span><br><span class="line"><span class="built_in">count</span>(<span class="keyword">case</span> <span class="keyword">when</span> grade <span class="keyword">like</span> <span class="string">&#x27;C%&#x27;</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="keyword">null</span> <span class="keyword">end</span>) C,</span><br><span class="line"><span class="built_in">count</span>(grade) total</span><br><span class="line"><span class="keyword">from</span> course <span class="keyword">natural</span> <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> takes</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> title</span><br></pre></td></tr></table></figure><h1 id="范式"><a href="#范式" class="headerlink" title="范式"></a>范式</h1><h6 id="超码：超码能唯一确定一个元组"><a href="#超码：超码能唯一确定一个元组" class="headerlink" title="超码：超码能唯一确定一个元组"></a>超码：超码能唯一确定一个元组</h6><h6 id="候选码：最小的超码、"><a href="#候选码：最小的超码、" class="headerlink" title="候选码：最小的超码、"></a>候选码：最小的超码、</h6><h6 id="1NF：非主属性部分依赖于R的候选码"><a href="#1NF：非主属性部分依赖于R的候选码" class="headerlink" title="1NF：非主属性部分依赖于R的候选码"></a>1NF：非主属性部分依赖于R的候选码</h6><h6 id="2NF：非主属性完全函数依赖于R的候选码，存在传递依赖（即非主属性由另一个非主属性决定）"><a href="#2NF：非主属性完全函数依赖于R的候选码，存在传递依赖（即非主属性由另一个非主属性决定）" class="headerlink" title="2NF：非主属性完全函数依赖于R的候选码，存在传递依赖（即非主属性由另一个非主属性决定）"></a>2NF：非主属性完全函数依赖于R的候选码，存在传递依赖（即非主属性由另一个非主属性决定）</h6><h6 id="3NF：左边是超码或者右边是主属性"><a href="#3NF：左边是超码或者右边是主属性" class="headerlink" title="3NF：左边是超码或者右边是主属性"></a>3NF：左边是超码或者右边是主属性</h6><h6 id="BCNF-左边都是超码"><a href="#BCNF-左边都是超码" class="headerlink" title="BCNF: 左边都是超码"></a>BCNF: 左边都是超码</h6><h6 id="无损分解"><a href="#无损分解" class="headerlink" title="无损分解"></a>无损分解</h6><p>定义：无损连接是指分解后的关系通过自然连接可以恢复成原来的关系，即通过自然连接得到的关系与原来的关系相比，既不多出信息、又不丢失信息。</p><p>判断方法： 图示法</p><h6 id="保持函数依赖"><a href="#保持函数依赖" class="headerlink" title="保持函数依赖"></a>保持函数依赖</h6><p>如果F上的每一个函数依赖都在其分解后的某一个关系上成立，则这个分解是保持依赖的（充分条件）。</p><p>如果上述判断失败，并不能断言分解不是保持依赖的，因为上面只是充分条件，还要使用下面的算法来做进一步判断。</p><p>对F上的每一个α→β使用下面的过程:</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">result:=α;</span><br><span class="line"><span class="keyword">while</span>(result发生变化)<span class="keyword">do</span></span><br><span class="line">    <span class="keyword">for</span> each 分解后的Ri</span><br><span class="line">        t=(result ∩ Ri)+ ∩ Ri</span><br><span class="line">        result=result ∪ t</span><br><span class="line">如果result中包含了β的所有属性，则函数依赖α→β成立，</span><br><span class="line">这时分解是保持依赖的</span><br></pre></td></tr></table></figure><h6 id="分解为符合3NF标准"><a href="#分解为符合3NF标准" class="headerlink" title="分解为符合3NF标准"></a>分解为符合3NF标准</h6><p>判断是否为3NF</p><p>求正则覆盖，求候选码，进行分解</p><p>去重</p><p>判断有无候选码，无则加上</p><p>例题：<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242004714.png"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242005127.png"></p><h6 id="分解为符合BCNF标准"><a href="#分解为符合BCNF标准" class="headerlink" title="分解为符合BCNF标准"></a>分解为符合BCNF标准</h6><p>函数依赖中非平凡函数依赖的左边都是超码</p><p>先判断左边是否为超码，不是则分解为符合超码的集合</p><p>例题：<img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242013994.png"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242014309.png"></p><p>例题：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242023013.jpg"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242025118.png"></p><p>例题：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242027712.jpg"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242028715.png"></p><h6 id="正则覆盖和最小函数依赖的区别：最小覆盖的右端必然只有一个属性"><a href="#正则覆盖和最小函数依赖的区别：最小覆盖的右端必然只有一个属性" class="headerlink" title="正则覆盖和最小函数依赖的区别：最小覆盖的右端必然只有一个属性"></a>正则覆盖和最小函数依赖的区别：最小覆盖的右端必然只有一个属性</h6><p>推荐课程: <a href="https://www.bilibili.com/video/BV1Bf4y127rX/?share_source=copy_web&vd_source=a8bcacd0bccd6339b854522544b2fb0b">录课｜数据库系统概念-范式3NF BCNF分解习题</a> </p><h1 id="数据库设计"><a href="#数据库设计" class="headerlink" title="数据库设计"></a>数据库设计</h1><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242059707.jpg"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242100755.png"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242109759.jpg"></p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202306242110452.png"></p>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-transformer学习</title>
      <link href="/2023/06/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-transformer%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/06/05/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-transformer%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h4 id="学习LSTM（Long-Short-Term"><a href="#学习LSTM（Long-Short-Term" class="headerlink" title="学习LSTM（Long Short Term)"></a>学习LSTM（Long Short Term)</h4><p>学习参考博客 <a href="https://blog.csdn.net/v_JULY_v/article/details/89894058?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522168594487216800192245602%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=168594487216800192245602&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-89894058-null-null.142%5Ev88%5Einsert_down28v1,239%5Ev2%5Einsert_chatgpt&utm_term=LSTM&spm=1018.2226.3001.4187">如何从RNN起步，一步一步通俗理解LSTM_lstm网络_v_JULY_v的博客-CSDN博客</a> </p><p>RNN的理解：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/rnn.png"></p><p>可以连接先前的信息到当前的任务上</p><p>RNN存在的问题——长期依赖问题</p><p>当间隔不断增大时，RNN没有学习不到连接如此远的信息的能力。当一条序列足够长的时候，RNN很难将信息从较早的时间传送到后面的时间。（存在梯度消失问题，在反向传播中，梯度不断减少，会使前面层不会更新参数，导致<strong>RNN会忘记它在较长序列中以前看到的内容，因此RNN只具有短时记忆</strong>）</p><p>LSTM（记住重要的，忘记无关紧要的）</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/LSTM.png"></p><p>tanh层：帮助调经网路的值，使得数值被限制在-1到1之间</p><p>Sigmoid层：数值被限制在0到1之间。目的：为了更新或者忘记信息，0乘任何数都是0，这部分信息将会遗忘，1乘任何数都是其本身，这部分信息将会保留。</p><p>LSTM的核心思想</p><p>信息在水平线上传递，通过‘门’结构来对信息进行增加或者删除</p><p>忘记门</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/LSTM1.png"></p><p>输入门</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/LSTM2.png"></p><p>输出门</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/LSTM3.png"></p><p>transformer 直观认识</p><p>transformer 和LSTM的最大区别在于LSTM是迭代的，串行的，必须要等当前字处理完，才能处理下一个字，而transformer训练时是并行的，所有的字是同时训练的。</p><p>transformer 两大模块——Encoder 和Decoder</p><p>Encoder 负责把输入隐射为隐藏层</p><p>Decoder 负责把隐藏层映射为自然语言序列</p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/transformer.png" title="" alt="" width="506"><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/transformer1.png" title="" alt="" width="508"><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/transformer2.png" title="" alt="" width="505"><p>编码器在结构上都是相同的，但不共享参数，每个编码器可以分解为两个子层</p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/transformer3.jpeg" title="" alt="" width="472"><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/transformer4.jpeg" title="" alt="" width="474"><p>transformer 核心特性</p><img title="" src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/transformer5.jpeg" alt="" width="468"><p>各个单词是同时流入编码器中的，通过自己单独的路径</p><p>自注意力机制（self-attention)</p><p>通过向量方式计算自注意力</p><ol><li><p>从每个编码器的输入向量生成三个向量：查询向量query-vec,键向量key-vec，值向量value-vec。输入向量乘上三个不同的权重矩阵Wq,Wk,Wv,从而得到Q,K,V（权重矩阵先初始化，然后不断学习得到）。</p></li><li><p>查询向量Query 是当前单词的表示形式，键向量Key可以看做是序列中所有单词的标签，q和k相乘得到对应的相关度打分，值向量value是单词的实际表示</p></li></ol><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/self-attention.png" title="" alt="" width="530"><hr><h4 id="注意力"><a href="#注意力" class="headerlink" title="注意力"></a>注意力</h4><h5 id="人脑中的注意力机制"><a href="#人脑中的注意力机制" class="headerlink" title="人脑中的注意力机制"></a>人脑中的注意力机制</h5><p>注意力是一个用来分配有限的信息处理能力的选择机制。</p><p>以人眼为例，眼睛首先扫描整个场景元素，然后寻找感兴趣的影像区域，聚焦感兴趣的区域，仔细观察获得信息。人脑对于整个场景的关注不是均衡的，有一定的权重区分，感兴趣的区域会被人脑分配更多的权重。</p><h5 id="自注意力机制的实现"><a href="#自注意力机制的实现" class="headerlink" title="自注意力机制的实现"></a>自注意力机制的实现</h5><p>自注意机制就是通过权重矩阵自发地找到元素与元素之间的关系</p><h6 id="注意力公式以及其中所包含的意义："><a href="#注意力公式以及其中所包含的意义：" class="headerlink" title="注意力公式以及其中所包含的意义："></a>注意力公式以及其中所包含的意义：</h6><p><img src="https://img-blog.csdnimg.cn/20190802125242224.png"></p><p>Q:查询向量（query)，用来查询其它元素</p><p>K:关键字（key)，用来被其它元素查询<br>V:内容（value)</p><p>每一个元素都有自己的Q、K、V，元素使用自己的Q与其它元素的K相乘得到该元素与其它元素的相似度，这个相似度可以理解为关注度得分，关注度得分越高，这两个元素之间的联系越密切。</p><p>公式中的Q、K、V表示的是矩阵，是所有元素Q、K、V的集合。Q和K的转置点乘，得到的是元素之间的相互关系，经过softmax后得到的就是映射到（0,1）的各个元素的相互关联程度，与V相乘后得到的是内部元素有关联的集合。为什么还要再除上一个缩放因子呢，dK所表示的是k的维度，如果维度dk太大，那么点积的值也会变大，由softmax函数的特性会知道这会导致经过softmax函数后被推到一个梯度极小的区域，加入缩放因子可以抵消这种影响，保证可以顺利进行反向传播。</p><p>经过这个公式后，得到的结果就可以突出V中需要注意的地方（即关联程度更高的地方）</p><h6 id="Q、K、V怎么得来的"><a href="#Q、K、V怎么得来的" class="headerlink" title="Q、K、V怎么得来的"></a>Q、K、V怎么得来的</h6><p>Q、K、V是由上一层的输出乘上它们各自的权重矩阵，权重矩阵都是先随机初始化，然后通过网络学习来不断更新。</p><h6 id="自注意力机制和注意力机制的区别"><a href="#自注意力机制和注意力机制的区别" class="headerlink" title="自注意力机制和注意力机制的区别"></a>自注意力机制和注意力机制的区别</h6><p>自注意力机制中Q、K、V来源于同一集合</p><p>注意力机制则会有Q和K的来源不同。在翻译中，K来自被翻译语言，Q来自目的语言，Q与K的转置点乘得到的是每个源单词翻译到目标单词对应的权重，再与目标单词的V矩阵加权求和，经过一个分类器就能得到每一个源单词对应的目标单词。</p><h6 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h6><p>可以类比CNN中同时使用多个卷积核的作用，设置多个Q、K、V权重矩阵，形成多个子空间，可以让模型去关注到不同方面的信息，最后再将各个方面信息综合起来，这样做有利于网络捕捉到更丰富的特征。</p>]]></content>
      
      
      <categories>
          
          <category> 图像分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>操作系统</title>
      <link href="/2023/05/25/%E8%80%83%E7%A0%94-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
      <url>/2023/05/25/%E8%80%83%E7%A0%94-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<h1 id="操作系统"><a href="#操作系统" class="headerlink" title="操作系统"></a>操作系统</h1><h2 id="一、-操作系统概述"><a href="#一、-操作系统概述" class="headerlink" title="一、 操作系统概述"></a>一、 操作系统概述</h2><h4 id="操作系统定义：是计算机系统最基本、最重要的系统软件，是其它软件的支撑软件。它控制和管理计算机系统的硬件和软件资源，合理组织计算机工作流程，并为用户使用计算机提供公共的和基本的服务。它有以下两个主要目标："><a href="#操作系统定义：是计算机系统最基本、最重要的系统软件，是其它软件的支撑软件。它控制和管理计算机系统的硬件和软件资源，合理组织计算机工作流程，并为用户使用计算机提供公共的和基本的服务。它有以下两个主要目标：" class="headerlink" title="操作系统定义：是计算机系统最基本、最重要的系统软件，是其它软件的支撑软件。它控制和管理计算机系统的硬件和软件资源，合理组织计算机工作流程，并为用户使用计算机提供公共的和基本的服务。它有以下两个主要目标："></a>操作系统定义：是计算机系统最基本、最重要的系统软件，是其它软件的支撑软件。它控制和管理计算机系统的硬件和软件资源，合理组织计算机工作流程，并为用户使用计算机提供公共的和基本的服务。它有以下两个主要目标：</h4><ol><li><p>高效性：操作系统允许以更加高效的方式使用计算机系统资源。</p></li><li><p>方便性：操作系统使得用户使用计算机更加方便</p></li></ol><h5 id="计算机系统的组成："><a href="#计算机系统的组成：" class="headerlink" title="计算机系统的组成："></a>计算机系统的组成：</h5><p>运算器、存储器、控制器、输入设备，输出设备</p><h5 id="操作系统与计算机系统："><a href="#操作系统与计算机系统：" class="headerlink" title="操作系统与计算机系统："></a>操作系统与计算机系统：</h5><p>单用户操作系统—&gt;具有并行能力的计算机系统</p><h5 id="操作系统的发展过程："><a href="#操作系统的发展过程：" class="headerlink" title="操作系统的发展过程："></a>操作系统的发展过程：</h5><p> 简单计算机系统（无操作系统）、单道批处理系统、多道批处理系统、分时系统、实时系统</p><p>单道批处理系统</p><img title="" src="file:///C:/Blog/source/_posts/image/921e5e9822813e0a510c30c1766575231f451502.png" alt="图片1.png" width="202"><p>单道程序工作示意图</p><p><img src="C:\Blog\source_posts\image\8cc2188403b3da9932a5dcccc4abe1b8d8052c04.png" alt="图片2.png"></p><p>多道程序工作示意图</p><p><img src="C:\Blog\source_posts\image\e85e1e7fcc8f000dcb2098a1d8282b9c2a585645.png" alt="图片3.png"></p><h5 id="操作系统的主要功能："><a href="#操作系统的主要功能：" class="headerlink" title="操作系统的主要功能："></a>操作系统的主要功能：</h5><ul><li><p>处理机管理</p></li><li><p>存储管理</p></li><li><p>设备管理</p></li><li><p>文件管理</p></li><li><p>用户接口</p></li></ul><h6 id="处理机管理："><a href="#处理机管理：" class="headerlink" title="处理机管理："></a>处理机管理：</h6><ul><li><p>进程控制：基本功能是创建和撤销进程、控制进程状态之间的转换</p></li><li><p>进程同步：进程同步是指系统对并发执行的进程进行协调，使它们能有条不紊的运行</p></li><li><p>进程通信：进程通信是指相关进程之间的信息交换</p></li><li><p>进程调度：指按照一定的调度算法在等待执行的进程中选出其中一个，并为其分配CPU、设置运行环境，使其投入运行</p></li></ul><h6 id="存储管理："><a href="#存储管理：" class="headerlink" title="存储管理："></a>存储管理：</h6><ul><li><p>内存分配：为每道程序分配必要的内存空间，提高存储器的利用率，减少空间浪费，在实现内存分配时，可采取静态和动态两种方式</p></li><li><p>内存保护：内存保护的主要任务是确保每道程序都只在自己的内存空间里运行，防止因一道程序的错误而干扰其它程序，也绝不允许用户程序随意访问操作系统的程序和数据</p></li><li><p>地址映射：把目标程序中的逻辑地址转换成为内存空间中的物理地址</p></li><li><p>内存扩充：内存扩充是借助虚拟存储技术，在不增加物理内存空间的前提下，从逻辑上对内存进行扩充，使系统能够运行内存需求量比实际内存更大的作业，或是让更多的作业能够并发执行</p></li></ul><h6 id="设备管理："><a href="#设备管理：" class="headerlink" title="设备管理："></a>设备管理：</h6><ul><li><p>缓冲管理：缓冲是指在内存中划出来用作暂时存放信息的一部分区域。在CPU和I&#x2F;O设备之间设置缓冲区，则可以有效缓解速度不匹配的矛盾，提高CPU的利用率，从而提高系统吞吐量</p></li><li><p>设备分配：根据用户所请求的设备类型、数量，按照一定分配算法对设备进行分配</p></li><li><p>设备处理：设备处理程序又称为设备驱动程序，其基本任务是由CPU向设备控制器发出I&#x2F;O命令，启动指定的I&#x2F;O设备、完成用户规定的I&#x2F;O操作，并对设备发来的中断请求进行及时响应和处理。</p></li><li><p>虚拟设备管理：虚拟设备也称逻辑设备，是指操作系统通过设备虚拟技术，把每次仅供一个进程使用的独享设备改造成能被多个用户使用的设备</p></li></ul><h6 id="文件管理-："><a href="#文件管理-：" class="headerlink" title="文件管理 ："></a>文件管理 ：</h6><ul><li><p>文件存储空间管理：一些当前需要使用的系统文件和用户文件，都必须放在可随机存取的磁盘上。为此，必须由操作系统统一对文件的存储空间进行管理，提高存储空间的利用率，同时也提高文件系统的存取速度</p></li><li><p>目录管理：目录又称文件目录，是用来描述系统中所有文件基本情况的一个表。为了使用户能够方便的在外存上找到自己所需的文件，系统会为每个文件建立一个目录项。在不同的系统中，目录有着不同的组织方式</p></li><li><p>文件读写管理：对文件进行读写操作，是文件管理必须具备的最基本的操作。该功能可以根据用户的请求，从外存指定区域把指定数量的信息读入到内存指定的用户区或系统区，或将指定数量的信息从内存写入外存指定区域。</p></li><li><p>文件保护：为了防止系统中的文件被非法窃取和破坏，必须提供有效的存取控制机制</p></li><li><p>文件系统的安全性：是指文件系统避免因软件或硬件故障而造成信息破坏的能力</p></li></ul><h6 id="用户接口："><a href="#用户接口：" class="headerlink" title="用户接口："></a>用户接口：</h6><ul><li><p>命令接口：为了便于用户直接或间接控制自己的作业，操作系统向用户提供了命令接口。用户可以通过该接口向作业发出命令，以控制作业的运行。 </p></li><li><p>程序接口：该接口是为用户程序在执行过程中访问系统资源而设定的，是用户程序取得操作系统服务的唯一途径。程序接口是由一组系统调用组成，每当应用程序要求操作系统提供某种类型的服务时，便调用具有相应功能的系统调用。 </p></li><li><p>图形接口：采用图形化的操作界面，用非常容易识别的图标将系统的各种命令直观、逼真的表示出来，用户通过简单的点击鼠标，借助菜单、对话框，就可以完成对应用程序和文件的操作，极大的方便了用户的使用。</p></li></ul><h5 id="操作系统结构："><a href="#操作系统结构：" class="headerlink" title="操作系统结构："></a>操作系统结构：</h5><p> 单体、模块化、可扩展内核、层次结构</p><img src="file:///C:/Blog/source/_posts/image/a3f06f8a7a194691060f877f7915d1595327bf2c.png" title="" alt="图片4.png" width="254"><img title="" src="file:///C:/Blog/source/_posts/image/097907d2de1ead3a9fa0461e76a48795371d091a.png" alt="图片5.png" width="253"><img src="file:///C:/Blog/source/_posts/image/76f88575b562e5d8e2410eadd6f5b3ea0ec50072.png" title="" alt="图片6.png" width="254"><img src="file:///C:/Blog/source/_posts/image/15c7167f15575e2169557a4731e926fba26b7e97.png" title="" alt="图片7.png" width="257"><h4 id="操作系统的特性-：并发、共享、虚拟和异步"><a href="#操作系统的特性-：并发、共享、虚拟和异步" class="headerlink" title="操作系统的特性 ：并发、共享、虚拟和异步"></a>操作系统的特性 ：并发、共享、虚拟和异步</h4><ul><li><p>并发性：<strong>是指在一段时间内有多道程序同时在计算机内运行</strong></p></li><li><p>共享性：互斥共享，是指该类资源的分配必须以作业（或进程）为单位，在一个作业（或进程）没有运行完之前，另一个作业（或进程）不得使用该类资源；“同时”共享，是指多个作业（或进程）可“同时”使用该类资源，这里的“同时”和并发性中的“同时”有着相同的含义</p></li><li><p>虚拟性：操作系统的虚拟性是指操作系统使用某种技术，将物理上的一个资源或设备变成逻辑上的多个资源或设备。虚拟出来的东西不过是用户的“错觉”，并不是客观存在的东西</p></li><li><p>异步性：操作系统的异步性又称之为不确定性，不是说操作系统本身的功能不确定，也不是说在操作系统控制下运行的用户程序的结果是不确定的。<strong>异步性指在操作系统控制下的多个作业的执行顺序和每个作业的执行时间是不确定的，即进程是以人们不可预知的速度向前推进</strong></p></li></ul><p>操作系统的新特征：微内核体系结构、多线程、对称多处理、分布式操作系统、面向对象设计</p><h2 id="二、中断"><a href="#二、中断" class="headerlink" title="二、中断"></a>二、中断</h2><h5 id="中断的基本概念"><a href="#中断的基本概念" class="headerlink" title="中断的基本概念"></a>中断的基本概念</h5><p>所谓中断，就是指CPU在执行一个程序时，对系统发生的某个事件（程序自身或外界的原因引起的）会做出的一种反应，<strong>即CPU暂停正在执行的程序，保留当前程序的运行现场后自动转去处理相应的事件，处理完该事件后，又返回到之前的程序断点，继续执行被中断的程序。</strong></p><img src="file:///C:/Blog/source/_posts/image/f68261117259b228a237710e34f1530b1af0682a.png" title="" alt="图片8.png" width="263"><p>中断的特点：随机性、可恢复性、自动性</p><p>中断优先级：硬件故障中断&gt;自愿性中断&gt;程序性中断&gt;外部中断&gt;输入&#x2F;输出中断</p><h5 id="中断的响应过程"><a href="#中断的响应过程" class="headerlink" title="中断的响应过程"></a>中断的响应过程</h5><ul><li>发现中断源</li></ul><img title="" src="file:///C:/Blog/source/_posts/image/6ecdde332ade5b5db9c6bfbb9818d9c99a4825fb.png" alt="图片9.png" data-align="center" width="348"><ul><li><p>保护和恢复现场：现场是指在中断的那一时刻能确保程序继续运行的有关信息。 为了确保被中断的程序能从恢复点继续运行，必须在该程序重新运行之前，把保留的该程序的现场信息从主存中送至相应的各个寄存器当中，把完成这些工作称为恢复现场。 </p></li><li><p>中断响应：中断响应是当CPU发现已有中断请求时，终止现行程序的执行，并自动引出中断处理程序的过程。 当发生中断事件时，中断系统必须立即将程序断点的现场信息存放到主存约定单元进行保存，用于中断返回时恢复现场使用。<strong>中断响应的实质就是交换用户程序和相应中断处理程序的指令执行地址和处理器状态，以达到保存断点和自动执行中断处理程序的目的。</strong></p></li></ul><h4 id="中断的处理过程"><a href="#中断的处理过程" class="headerlink" title="中断的处理过程"></a>中断的处理过程</h4><h6 id="保护现场和传递参数"><a href="#保护现场和传递参数" class="headerlink" title="保护现场和传递参数"></a>保护现场和传递参数</h6><p>  对现场进行保护，包括对断点的保护和对通用寄存器以及状态寄存器的保护。  </p><h6 id="执行相应的中断服务程序"><a href="#执行相应的中断服务程序" class="headerlink" title="执行相应的中断服务程序"></a>执行相应的中断服务程序</h6><p> 针对响应的中断事件，执行处理该事件的中断服务程序。</p><h6 id="恢复现场并退出中断"><a href="#恢复现场并退出中断" class="headerlink" title="恢复现场并退出中断"></a>恢复现场并退出中断</h6><p>  执行完中断处理程序，系统要返回到之前的断点处继续执行，所以要将先前保存的断点信息重新加载进系统的各个寄存器当中，并将中断屏蔽字还原，这一过程称为恢复现场。</p><img title="" src="file:///C:/Blog/source/_posts/image/36fbb2f2437942005cdbd79ca0e2c8162e195356.png" alt="图片10.png" width="292" data-align="center"><h2 id="三、-进程和线程"><a href="#三、-进程和线程" class="headerlink" title="三、 进程和线程"></a>三、 进程和线程</h2><h6 id="程序的顺序执行及其特征："><a href="#程序的顺序执行及其特征：" class="headerlink" title="程序的顺序执行及其特征："></a>程序的顺序执行及其特征：</h6><p>通常可以把一个应用程序分成若干个程序段，各程序段之间必须按照某种先后次序顺序执行，仅当前一程序段（操作）执行完后，才能执行后继程序段（操作）。</p><p>特征：顺序性、封闭性、可再现性</p><h6 id="程序的并发执行及其特征："><a href="#程序的并发执行及其特征：" class="headerlink" title="程序的并发执行及其特征："></a>程序的并发执行及其特征：</h6><p>为了提高计算机的利用率、处理速度和系统的吞吐量，并行处理技术和并发程序设计技术在计算机中已经得到了广泛应用，成为了现代操作系统的基本特征之一。</p><p>特征：异步性、失去封闭性、不可再现性</p><h4 id="进程的概念及其特征：进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。"><a href="#进程的概念及其特征：进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。" class="headerlink" title="进程的概念及其特征：进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。"></a>进程的概念及其特征：进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。</h4><p>程序和进程之前的区别和联系：</p><p>程序是完成特定任务的一组指令的结合，可以永久保存，具有静态性；<br>进程是程序在某一数据结构上的一次执行过程，是系统进行资源分配和调度的基本单位，具有动态性；<br>一个进程可以包含多个程序，一个程序也可以被多个进程执行。</p><h4 id="进程状态："><a href="#进程状态：" class="headerlink" title="进程状态："></a>进程状态：</h4><h5 id="两状态模型"><a href="#两状态模型" class="headerlink" title="两状态模型"></a>两状态模型</h5><p>包含运行态（Running）和非运行态（Not running）两种进程状态<br>创建了一个新进程之后，它会以非运行态加入到系统中，等到操作系统为其分派处理器<br>当前处于运行态的进程会不时地中断，由系统中的分派器选择处于非运行状中的某一个进程运行</p><img src="file:///C:/Blog/source/_posts/image/8f7f7f67590b9d22d3b6b5ce741de687515c5703.png" title="" alt="图片11.png" width="404"><h5 id="五状态模型"><a href="#五状态模型" class="headerlink" title="五状态模型"></a>五状态模型</h5><p>包括<strong>就绪态</strong>（Ready）、<strong>运行态</strong>（Running）、<strong>阻塞态</strong>（Blocked）、<strong>新建态</strong>（New）和<strong>终止态</strong>（Terminate）<br>进程状态描述：<br>    （1）新建态：刚刚创建的新进程，通常是指进程控制块已经创建，但还没有加载到系统内存中的进程。<br>    （2）就绪态：进程等待系统为其分派处理器，而此时处理器被其它进程占据，所以该状态进程不能执行，但已经具备了除处理器之外的进程执行所需要的所有条件。</p><p>    （3）运行态：进程已获得所需资源并占据处理器，处理器正在执行该进程。<br>   （4）阻塞态：也称为等待态、挂起态或睡眠态，进程在等待某个事情的发生而暂时不能运行，例如等待某个I&#x2F;O操作的完成。<br>   （5）终止态：进程或者因为执行结束或者因为被撤销而从可执行进程组中退出。</p><p><img src="C:\Blog\source_posts\image\19ca50a1cec642b400724b7ae3d1cbc3ca966f38.png" alt="图片12.png"></p><h5 id="引入挂起状态"><a href="#引入挂起状态" class="headerlink" title="引入挂起状态"></a>引入挂起状态</h5><p>      对于内存中的多个进程，处理器依次选中运行，当一个进程正在等待I&#x2F;O事件发生时，处理器转移到另一个进程。但是，处理器的速度比I&#x2F;O要快很多，有可能内存中所有进程都在等待I&#x2F;O事件的完成，导致处理器处于空闲状态。<br>     引入挂起（Suspend）的概念：内存中没有就绪的进程时，系统将内存中处于阻塞的进程换出到外存中的挂起队列，而将外存中的就绪进程激活，换入到内存</p><p><img src="C:\Blog\source_posts\image\204ce929ebdf54c0baf2f91927f7c4bed6de3eaf.jpg" alt="图片13.jpg"></p><h6 id="进程控制块"><a href="#进程控制块" class="headerlink" title="进程控制块"></a>进程控制块</h6><p>进程控制块（Process control block, PCB）是操作系统用来记录进程状态和相关信息，控制进程运行的数据结构，是进程的唯一标识符</p><h6 id="进程控制"><a href="#进程控制" class="headerlink" title="进程控制"></a>进程控制</h6><p>进程控制是进程管理中最基本的功能<br>在操作系统中，不同功能都是通过执行各种原语（Primitive）操作实现<br>原语是由若干条指令构成、可完成特定功能的程序段</p><h6 id="线程简介"><a href="#线程简介" class="headerlink" title="线程简介"></a>线程简介</h6><p>在操作系统中引入进程的目的，是为了使多个程序能并发执行，以提高资源利用率和系统吞吐量。<br>在操作系统中再引入线程，则是为了减少程序在并发执行时所付出的时空开销，使操作系统具有更好的并发性。<br>通常把调度和分派的基本单位称做线程或轻量级进程（Light weight process, LWP）,而把资源分配的基本单位称做进程或者任务。</p><h4 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h4><p>进程在任一时刻只有一个执行控制流，通常将这种结构的进程称为单线程进程（Single threaded process）。<br>多线程进程（Multiple threaded process）——同一进程中设计出多条控制流，并且满足：<br>   （1）多控制流之间可以并行执行；<br>   （2）多控制流切换不需通过进程调度；<br>   （3）多控制流之间可以通过内存直接通信联系，从而降低通信开销。</p><img src="file:///C:/Blog/source/_posts/image/a116731aebd5813540005d33074ea7b3acb466bf.png" title="" alt="图片13.png" width="481"><h6 id="线程实现与线程模型"><a href="#线程实现与线程模型" class="headerlink" title="线程实现与线程模型"></a>线程实现与线程模型</h6><p>用户级线程（User level thread, ULT）<br>            线程管理的所有工作都由应用程序完成，内核意识不到线程的存在。</p><p>内核级线程（Kernel level thread, KLT）<br>            线程管理的所有工作都是由内核完成，应用程序并没有参与其中。即无论是用户进程中的线程，还是系统进程中的线程，他们的创建、终止和切换等也是依靠内核，在内核空间实现的。</p><p>组合方式<br>            同一个进程内的多个线程可以同时在多处理器上并行执行，而且一个线程被阻塞时，同一进程内的其它线程可以调度运行，并不需要阻塞整个进程。所以，组合方式多线程机制能够结合KLT和ULT两者的优点，并克服了其各自的不足。</p><img src="file:///C:/Blog/source/_posts/image/dbd834c4e5bc4f014a46c799787cd50f1bdc0482.png" title="" alt="图片14.png" width="448"><h6 id="线程模型"><a href="#线程模型" class="headerlink" title="线程模型"></a>线程模型</h6><p>多对一模型</p><img src="file:///C:/Blog/source/_posts/image/fc511176f52c39d2b6445148979c3023742c2c97.png" title="" alt="图片15.png" width="394"><p>一对一模型</p><img src="file:///C:/Blog/source/_posts/image/b89bc2be5d9bf184ed46dc23f1db2a38f9567311.png" title="" alt="图片16.png" width="467"><p>多对多模型</p><p><img src="C:\Blog\source_posts\image\29d6d8f501541eab2d4006f9573b04d750d50edb.png" alt="图片17.png"></p><h4 id="并发原理：临界资源、临界区等多种术语"><a href="#并发原理：临界资源、临界区等多种术语" class="headerlink" title="并发原理：临界资源、临界区等多种术语"></a>并发原理：临界资源、临界区等多种术语</h4><h6 id="临界区：是一段程序代码，进程将在此代码中访问共享的资源，当另一个进程已经在该代码中运行时，则该进程不能在这段代码中执行"><a href="#临界区：是一段程序代码，进程将在此代码中访问共享的资源，当另一个进程已经在该代码中运行时，则该进程不能在这段代码中执行" class="headerlink" title="临界区：是一段程序代码，进程将在此代码中访问共享的资源，当另一个进程已经在该代码中运行时，则该进程不能在这段代码中执行"></a>临界区：是一段程序代码，进程将在此代码中访问共享的资源，当另一个进程已经在该代码中运行时，则该进程不能在这段代码中执行</h6><h6 id="竞争-：多个进程在访问一个共享数据时，结果依赖于它们执行的相对时间，这种关系称为竞争"><a href="#竞争-：多个进程在访问一个共享数据时，结果依赖于它们执行的相对时间，这种关系称为竞争" class="headerlink" title="竞争 ：多个进程在访问一个共享数据时，结果依赖于它们执行的相对时间，这种关系称为竞争"></a>竞争 ：多个进程在访问一个共享数据时，结果依赖于它们执行的相对时间，这种关系称为竞争</h6><h6 id="同步：系统中有一些相互合作、协同工作的进程，它们之间的相互联系称为进程的同步"><a href="#同步：系统中有一些相互合作、协同工作的进程，它们之间的相互联系称为进程的同步" class="headerlink" title="同步：系统中有一些相互合作、协同工作的进程，它们之间的相互联系称为进程的同步"></a>同步：系统中有一些相互合作、协同工作的进程，它们之间的相互联系称为进程的同步</h6><h6 id="互斥：多个进程因争用临界区内的共享资源而互斥的执行，即当一个进程在临界区访问共享资源时，其它进程不能进入该临界区访问任何共享资源"><a href="#互斥：多个进程因争用临界区内的共享资源而互斥的执行，即当一个进程在临界区访问共享资源时，其它进程不能进入该临界区访问任何共享资源" class="headerlink" title="互斥：多个进程因争用临界区内的共享资源而互斥的执行，即当一个进程在临界区访问共享资源时，其它进程不能进入该临界区访问任何共享资源"></a>互斥：多个进程因争用临界区内的共享资源而互斥的执行，即当一个进程在临界区访问共享资源时，其它进程不能进入该临界区访问任何共享资源</h6><h6 id="死锁：两个或两个以上的进程因其中的每个进程都在等待其它进程执行完毕而不能继续执行，这样的情形称为死锁"><a href="#死锁：两个或两个以上的进程因其中的每个进程都在等待其它进程执行完毕而不能继续执行，这样的情形称为死锁" class="headerlink" title="死锁：两个或两个以上的进程因其中的每个进程都在等待其它进程执行完毕而不能继续执行，这样的情形称为死锁"></a>死锁：两个或两个以上的进程因其中的每个进程都在等待其它进程执行完毕而不能继续执行，这样的情形称为死锁</h6><h6 id="饥饿：是指一个可运行的进程虽然能继续执行，但被调度程序无限期的忽视而不能执行的情况"><a href="#饥饿：是指一个可运行的进程虽然能继续执行，但被调度程序无限期的忽视而不能执行的情况" class="headerlink" title="饥饿：是指一个可运行的进程虽然能继续执行，但被调度程序无限期的忽视而不能执行的情况"></a>饥饿：是指一个可运行的进程虽然能继续执行，但被调度程序无限期的忽视而不能执行的情况</h6><h4 id="硬件同步"><a href="#硬件同步" class="headerlink" title="硬件同步"></a>硬件同步</h4><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"> TestAndSet指令实现互斥的示例如下：</span><br><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(TestAndSet(&amp;lock))</span><br><span class="line">           ;<span class="comment">// do nothing</span></span><br><span class="line">  <span class="comment">// critical section</span></span><br><span class="line">  lock = FALSE;</span><br><span class="line">  <span class="comment">// remainder section</span></span><br><span class="line">&#125;<span class="keyword">while</span>(TRUE);</span><br><span class="line"></span><br><span class="line"> .Swap指令</span><br><span class="line">Swap指令为对换指令，定义如下：</span><br><span class="line"><span class="type">void</span> <span class="title function_">Swap</span><span class="params">(boolean *a, boolean *b)</span>&#123;</span><br><span class="line">  Boolean temp = *a;</span><br><span class="line">  *a = *b;</span><br><span class="line">  *b = temp;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> 使用Swap指令实现互斥的示例如下：</span><br><span class="line"><span class="keyword">do</span>&#123;</span><br><span class="line">  data = TRUE;</span><br><span class="line">  <span class="keyword">while</span>(data == TRUE)</span><br><span class="line">    Swap(&amp;lock, &amp;data);</span><br><span class="line">  <span class="comment">// critical section</span></span><br><span class="line">  lock = FALSE;</span><br><span class="line">  <span class="comment">// reminder section</span></span><br><span class="line">&#125;<span class="keyword">while</span>(TRUE);</span><br></pre></td></tr></table></figure><h4 id="信号量机制"><a href="#信号量机制" class="headerlink" title="信号量机制"></a>信号量机制</h4><p><strong>整型信号量</strong><br>           Dijkstra把整型信号量s定义成一个用于表示资源数目的整型变量。进程通过信号量传送信号，利用两个特殊的操作发送和接收信号。<br>   signal(s)：通过信号量s传送信号<br>   wait(s)：   通过信号量s接收信号<br>   如果相应的信号仍然没有发送，则进程被挂起，直到发送完为止。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">wait()操作定义如下</span><br><span class="line"><span class="type">void</span> <span class="title function_">wait</span><span class="params">(s)</span>&#123;</span><br><span class="line">   <span class="keyword">while</span>(s&lt;=<span class="number">0</span>)</span><br><span class="line">     ; <span class="comment">//do nothing</span></span><br><span class="line">   s--;</span><br><span class="line">&#125;</span><br><span class="line">signal()操作定义如下</span><br><span class="line"><span class="type">void</span> <span class="title function_">signal</span><span class="params">(s)</span>&#123;</span><br><span class="line">  s++;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p><strong>记录型信号量</strong><br>整型信号量机制没有满足让权等待的原则，可能使进程处于饥饿的忙等状态。<br>假设s为一个记录型数据结构，其中一个分量为整形量value，另一个为信号量队列queue。value通常是一个具有非负初值的整型变量，queue是一个初始状态为空的进程队列，当一个进程必须等待信号量时，就加入到进程队列中去。</p><p>wait和signal操作定义如下：<br>       wait(s)：信号量s减l，若结果小于0，则调用wait(s)的进程被设置成等待信号量s的状态。<br>      signal(s)：将信号量s加1，若结果不大于0，则释放一个等待信号量s的进程。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"> 记录型信号量数据结构定义如下：</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line"><span class="type">int</span> value;</span><br><span class="line">QueueType <span class="built_in">queue</span>;</span><br><span class="line">&#125;semaphore;</span><br><span class="line"> wait( )操作定义如下：</span><br><span class="line"><span class="type">void</span> <span class="title function_">wait</span><span class="params">(semaphore *s)</span>&#123;</span><br><span class="line">  s.value--;</span><br><span class="line">  <span class="keyword">if</span>(s.value &lt; <span class="number">0</span>)&#123;</span><br><span class="line">    block(s.<span class="built_in">queue</span>); <span class="comment">// add this process into s.queue</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">signal( )操作定义如下：</span><br><span class="line"><span class="type">void</span> <span class="title function_">signal</span><span class="params">(semaphore *s)</span>&#123;</span><br><span class="line">  s.value++;</span><br><span class="line">  <span class="keyword">if</span>(s.value &lt;= <span class="number">0</span>)&#123;</span><br><span class="line">   wakeup(s.<span class="built_in">queue</span>); <span class="comment">// remove a process from s.queue</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">（<span class="number">1</span>）若信号量s.value值为正，</span><br><span class="line"> 则该值表示在对进程进行阻塞之前对信号量s可以实施的wait()操作个数，</span><br><span class="line"> 即系统中某类资源实际可用数目；</span><br><span class="line">（<span class="number">2</span>）若信号量s.value值为负，</span><br><span class="line"> 则其绝对值表示阻塞队列s.<span class="built_in">queue</span>中等待的进程个数；</span><br><span class="line">（<span class="number">3</span>）每次wait()操作，意味着进程请求一个单位的该类资源，</span><br><span class="line"> 使系统中可供分配的该类资源数减少一个，每次signal()操作，</span><br><span class="line"> 表示执行进程释放一个单位资源，使系统中可供分配的该类资源数增加一个。</span><br></pre></td></tr></table></figure><p><strong>二元信号量</strong><br>假设s为一个记录型数据结构，其中一个分量为value，它仅能取值0和1，另一个分量为信号量队列queue。<br>一个二元信号量的值只能是0或者1</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span>&#123;</span></span><br><span class="line"><span class="class"><span class="keyword">enum</span> &#123;</span>zero, one&#125; value; </span><br><span class="line">QueueType <span class="built_in">queue</span>;</span><br><span class="line">&#125;binary_semaphore;</span><br><span class="line"><span class="type">void</span> <span class="title function_">waitB</span><span class="params">(binary_semaphore *s)</span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(s.value == one)</span><br><span class="line">     s.value = zero;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">     block(s.<span class="built_in">queue</span>); <span class="comment">// add this process into s.queue</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">signalB</span><span class="params">(binary_semaphore *s)</span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(s.<span class="built_in">queue</span> is empty())&#123;</span><br><span class="line">     s.value = one;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    wakeup(s.<span class="built_in">queue</span>); <span class="comment">// remove a process from s.queue</span></span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>利用信号量实现进程互斥</strong></p><p>多个进程互斥地访问某临界资源，只需为该资源设置互斥信号量mutex，并设其初始值为1，然后将各进程访问该资源的临界区置于wait(mutex)和signal(mutex)操作之间即可。 </p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">Semaphore mutex = <span class="number">1</span>;</span><br><span class="line">     PA() &#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">             wait(mutex);</span><br><span class="line">             临界区</span><br><span class="line">             signal(mutex);</span><br><span class="line">             剩余区</span><br><span class="line">        &#125; </span><br><span class="line">     &#125;</span><br><span class="line">    PB () &#123;</span><br><span class="line">        <span class="keyword">while</span>(<span class="number">1</span>)&#123;</span><br><span class="line">             wait(mutex);</span><br><span class="line">             临界区 </span><br><span class="line">             signal(mutex);</span><br><span class="line">             剩余区</span><br><span class="line">        &#125; </span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><p><strong>利用信号量实现前趋关系</strong></p><p>还可利用信号量来描述程序或语句之间的前趋关系。设有两个并发执行的进程P1和P2。P1中有语句S1；P2中有语句S2。我们希望在S1执行后再执行S2。为实现这种前趋关系，只需使进程P1和P2共享一个公用信号量S，并赋予其初值为0，将signal(S)操作放在语句S1后面，而在S2语句前面插入wait(S)操作，即<br>　　在进程P1中，用S1；signal(S)；　　</p><p>          在进程P2中，用wait(S)；S2；</p><p>由于S被初始化为0，这样，若P2先执行必定阻塞，只有在进程P1执行完S1； signal(S)；操作后使S增为1时，P2进程方能成功执行语句S2</p><h6 id="管程机制"><a href="#管程机制" class="headerlink" title="管程机制"></a>管程机制</h6><p>        管程是由一个或多个过程、一个初始化序列和数据组成的软件模块，是一种程序设计语言结构成分，具有和信号量同等的表达能力。进程可以通过调用管程实现对资源的请求和释放。</p><p>cwait和csignal操作意义：<br>   （1）cwait(c)操作：正在调用管程过程的进程因条件c没有满足而被阻塞或者挂起，则调用cwait操作将自己插入到条件变量c的等待进程队列中。与此同时，被阻塞进程释放管程，直到条件c发生改变，其它进程可以调用管程。<br>   （2）csignal(c)操作：正在调用管程的进程检测到条件c发生了改变，则调用csignal操作重新唤醒一个因条件c而被阻塞或者挂起的进程。如果等待进程队列中有多个进程，则选择其中一个唤醒，否则继续执行原进程。</p><h4 id="三个经典的进程同步问题"><a href="#三个经典的进程同步问题" class="headerlink" title="三个经典的进程同步问题"></a>三个经典的进程同步问题</h4><h6 id="生产者-消费者问题"><a href="#生产者-消费者问题" class="headerlink" title="生产者-消费者问题"></a>生产者-消费者问题</h6><p>问题描述<br>        假设有n个生产者和m个消费者，连接在一个有k个公用缓冲区的有界缓冲上，pi表示生产者进程，cj表示消费者进程。 满足：<br>只要缓冲区未满，生产者pi即可将生产的产品放 入空闲缓冲区中<br>只要缓冲区不为空，消费者进程cj就可从缓冲区从取走并消耗产品</p><p>用信号量解决生产者-消费者问题<br>利用互斥信号量mutex实现多个进程对公用缓冲区的互斥使用，初始化为1<br>利用信号量empty和full分别记录公用缓冲区中空缓冲区和满缓冲区的个数，分别初始化为k和0。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> nextin = <span class="number">0</span>, nextout = <span class="number">0</span>;</span><br><span class="line">item buffer[k];</span><br><span class="line">semaphore mutex = <span class="number">1</span>, empty = k, full = <span class="number">0</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">producer</span><span class="params">()</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(TRUE)&#123;</span><br><span class="line">      <span class="comment">//produce an item in nextp;</span></span><br><span class="line">      wait(empty);</span><br><span class="line">      wait(mutex);</span><br><span class="line">      buffer[nextin] = nextp;</span><br><span class="line">      nextin = (nextin + <span class="number">1</span>) % k;</span><br><span class="line">      signal(mutex);</span><br><span class="line">      signal(full);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">consumer</span><span class="params">()</span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(TRUE)&#123;</span><br><span class="line">         wait(full);</span><br><span class="line">         wait(mutex);</span><br><span class="line">         nextc = buffer[nextout];</span><br><span class="line">         nextout = (nextout + <span class="number">1</span>) % k;</span><br><span class="line">         signal(mutex);</span><br><span class="line">         signal(empty);</span><br><span class="line">         <span class="comment">//consume the item in nextc;</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">  parbegin (producer, consumer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="读者-写者问题"><a href="#读者-写者问题" class="headerlink" title="读者-写者问题"></a>读者-写者问题</h6><p>问题描述<br>        有一个多个进程共享的数据区，我们把只要读该数据区的进程记为Reader进程（读者），把只要往数据区中写数据的进程记为Writer进程（写者）。满足：<br>允许多个读者同时执行读操作<br>一次只能有一个写者可以执行写操作<br>如果一个写者在执行写操作，则其它任何读者都不能执行读操作</p><p>用信号量解决读者-写者问题</p><p>利用互斥信号量wmutex实施读者与写者在读写时的互斥，<br>设置整型变量readercount用于记录正在读的进程个数<br>计数变量readercount本身是可以被多个读者访问的临界资源，设置互斥信号量mutex控制多个读者对readercount的修改</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">semaphore mutex = <span class="number">1</span>, wmutex = <span class="number">1</span>;</span><br><span class="line"><span class="type">int</span> readercount = <span class="number">0</span>;</span><br><span class="line"><span class="type">void</span> <span class="title function_">reader</span><span class="params">()</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(TRUE)</span><br><span class="line">&#123;</span><br><span class="line">  wait(mutex);</span><br><span class="line">  <span class="keyword">if</span> (readcount == <span class="number">0</span>)</span><br><span class="line">    wait(wmutex);</span><br><span class="line">  readercount++;</span><br><span class="line">  signal(mutex);</span><br><span class="line">  <span class="comment">// read operation</span></span><br><span class="line">  wait(mutex);</span><br><span class="line">  readercount--;</span><br><span class="line"><span class="keyword">if</span> (readcount == <span class="number">0</span>)</span><br><span class="line">    signal(wmutex);</span><br><span class="line">        signal(mutex);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">writer</span><span class="params">()</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(TRUE)</span><br><span class="line">  &#123;</span><br><span class="line">    wait(wmutex);</span><br><span class="line">    <span class="comment">// write operation</span></span><br><span class="line">    signal(wmutex);</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">  parbegin (reader, writer);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h6 id="哲学家就餐问题"><a href="#哲学家就餐问题" class="headerlink" title="哲学家就餐问题"></a>哲学家就餐问题</h6><p>问题描述<br>        有五位哲学家，用一生来思考和吃饭。他们围坐在一张圆桌旁边，桌子中央有一大碗米饭，桌上还有五个碗和五只筷子，他们的生活方式是交替地进行思考和进餐。平时，当某位哲学家进行思考时，他不与其它哲学家交互。当他感觉到饥饿时，便试图拿起与其左右最靠近他的筷子。满足：<br>一个哲学家每次只能拿起一只筷子，且他不能从其他哲学家手里拿筷子<br>只有在他拿到两只筷子时才能进餐</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">semaphore chopstick[<span class="number">5</span>] = &#123;<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>&#125;;</span><br><span class="line">semaphore room = &#123;<span class="number">4</span>&#125;;</span><br><span class="line"><span class="type">int</span> i;</span><br><span class="line"><span class="type">void</span> <span class="title function_">philosopher</span> <span class="params">(<span class="type">int</span> i)</span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(TRUE)&#123;</span><br><span class="line">    <span class="comment">// think</span></span><br><span class="line">    wait(room);</span><br><span class="line">    wait(chopstick[i]);</span><br><span class="line">    wait(chopstick[(i+<span class="number">1</span>) % <span class="number">5</span>]);</span><br><span class="line">    <span class="comment">// eat</span></span><br><span class="line">    signal(chopstick[(i+<span class="number">1</span>) % <span class="number">5</span>]);</span><br><span class="line">    signal(chopstick[i]);</span><br><span class="line">    signal(room);</span><br><span class="line">  &#125;</span><br><span class="line">&#125; </span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">  parbegain (philosopher(<span class="number">0</span>), philosopher(<span class="number">1</span>), philosopher(<span class="number">2</span>), philosopher(<span class="number">3</span>), philosopher(<span class="number">4</span>));&#125;</span><br></pre></td></tr></table></figure><h6 id="消息传递"><a href="#消息传递" class="headerlink" title="消息传递"></a>消息传递</h6><p>消息传递（Message passing）作为当前应用最为广泛的一种进程间通信机制，为进程间信息传递和交换的实现提供了很好的保障<br>消息是一组信息，由消息头和消息体组成。<br>send(destination, message)<br>receive(source, message)</p><h2 id="四、处理机调度"><a href="#四、处理机调度" class="headerlink" title="四、处理机调度"></a>四、处理机调度</h2><h6 id="调度简介"><a href="#调度简介" class="headerlink" title="调度简介"></a>调度简介</h6><p>计算机系统中，处理器和内存资源会出现供不应求的情况，特别是多个I&#x2F;O设备与主机交互，作业不断进入系统，或者是多个批处理作业在磁盘的后备队列中等待进入内存的情况。操作系统在管理有限的资源的同时，需要考虑如何选取进入内存的作业，如何分配有限的处理器资源给多个进程等重要问题。处理器的调度正是处理器和内存资源调度和分配相关的工作。</p><p>高级调度：对象是作业，又称作业调度、长调度</p><p>低级调度：对象是进程，又称进程调度、短调度</p><p>中级调度：中级调度介于高级调度和低级调度之间。该调度根据进程状态决定辅存和主存之间的进程对换。主存资源紧缺时，将暂时不能运行的进程换出，进程转为挂起状态；主存资源空闲，并且进程满足运行条件时，再将进程调回主存。对主存的利用和系统吞吐率都有很大的提升。</p><p>在上述三种调度中，进程调度是操作系统最核心的部分，运行频率最高，因此把它称为短程调度。为避免进程调度占用太多的CPU时间，进程调度算法不宜太复杂。作业调度往往是发生在一个(批)作业运行完毕，退出系统，而需要重新调入一个(批)作业进入内存时，故作业调度的周期较长，大约几分钟一次，因此把它称为长程调度。在纯粹的分时或实时操作系统中通常不需要作业调度。中级调度的运行频率基本上介于上述两种调度之间。一些功能完善的操作系统为了提高主存利用率和作业吞吐率，会引入中级调度。</p><h4 id="调度算法"><a href="#调度算法" class="headerlink" title="调度算法"></a>调度算法</h4><p><strong>先来先服务调度算法</strong></p><p>先来先服务算法（First Come First Served， FCFS）按照作业&#x2F;进程进入队列的先后顺序进行挑选，先进入的将优先进行后续步骤。该算法既可用于高级调度，也可用于低级调度。当在高级调度中采用该算法时，每次调度都是从后备作业队列中选择一个或多个最先进入该队列的作业，将它们调入内存，为它们分配资源、创建进程，然后放入就绪队列。在低级调度中采用该算法时，则每次调度是从就绪队列中选择一个最先进入该队列的进程，为之分配处理机，使之投入运行。该进程一直运行到完成或发生某事件而阻塞后才放弃处理机。该算法比较有利于长作业(进程)，而不利于短作业(进程)。</p><p><strong>短作业优先调度算法</strong></p><p>短作业优先调度算法（shot job first, SJF）按照进入系统的作业所要求的CPU运行时间的长短为挑选依据，优先选取预计计算时间最短的作业。可以分别用于高级调度和低级调度。</p><p>短作业优先(SJF)的调度算法是从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。而短进程优先(SPF)调度算法则是从就绪队列中选出一个估计运行时间最短的进程，将处理机分配给它，使它立即执行并一直执行到完成，或发生某事件而被阻塞放弃处理机时再重新调度。SJF调度算法能有效地降低作业的平均等待时间，提高系统吞吐量。</p><p><strong>优先级算法</strong></p><p>优先级算法根据事先设定好的进程的优先级来选取就绪队列中优先级最高的进程投入运行。在运行过程中，如果就绪队列中出现优先级更高的进程，则根据系统的策略：抢占式或非抢占式进行调度 </p><p>非抢占式：当前进程继续运行直到完成；或出现需要等待的事件放弃处理机，再调度优先级高的进程运行。<br>抢占式：当前进程占用的处理机被立即剥夺，将处理机分配给优先级高的进程，使之运行。只要高优先级的进程出现，无论当前进程完成与否，都必须立即停止，重新将处理机分配给新到的优先权最高的进程。抢占式的优先级调度算法能更好地满足紧迫作业的要求。</p><p><strong>时间片轮转算法</strong></p><p>时间片轮转算法将CPU分配给就绪队列中的第一个进程，每次分配一个时间片。但时间片耗尽时，如果进程未完成，则让出处理机，转到就绪队列的队尾，等待下一轮的时间片的分配。<br>系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把处理机分配给队首进程，并令其执行一个时间片。当执行的时间片用完时发出中断请求，调度程序便据此信号来停止该进程的执行，并将它送往就绪队列的末尾；然后，再把处理机分配给就绪队列中新的队首进程，同时也让它执行一个时间片。这样就可以保证系统能在给定的时间内响应所有用户的请求。</p><p>在时间片轮转算法中，时间片的大小对系统性能有很大的影响<br>选择很小的时间片将有利于短作业，因为它能较快地完成，但会频繁地发生中断、进程上下文的切换，从而增加系统的开销<br>选择太长的时间片，使得每个进程都能在一个时间片内完成，时间片轮转算法便退化为FCFS算法，无法满足交互式用户的需求<br>一个较为可取的大小是时间片略大于一次典型的交互时间，这样可使大多数进程在一个时间片内完成</p><p><strong>高响应比优先算法</strong></p><p>最高响应比优先算法同时兼顾作业的等待时间和处理时间，做有效的协调和折中，既能照顾短作业的调度，同时也不会让长作业等待的时间超出合理的范围。</p><p><strong>几种算法比较</strong></p><p>先来先服务：只考虑作业等待时间，忽视作业计算时间。<br>短作业优先：考虑作业预计的计算时间，忽视作业的等待时间。<br>最高响应比：综合前两种算法的优点：既照顾了短作业，又考虑了作业到达的先后次序，不会使长作业长期得不到服务。缺点：计算每个作业的响应比需要耗费一定的时间，性能比短作业优先算法略差。<br>如果我们能为每个作业引入前面所述的动态优先权，并使作业的优先级随着等待时间的增加而以一定的速率提高，则长作业在等待一定的时间后，必然有机会分配到处理机。</p><p>高响应比调度算法：<br>如果作业的等待时间相同，则要求服务的时间愈短，其优先权愈高，因而该算法有利于短作业<br>当要求服务的时间相同时，作业的优先权决定于其等待时间，等待时间愈长，其优先权愈高，因而实现的是先来先服务　<br>对于长作业，作业的优先级可以随等待时间的增加而提高，当其等待时间足够长时，其优先级便可升到很高，从而也可获得处理机</p><h6 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h6><p>计算机系统中存在一些只能被一个进程所使用的资源，如磁带机、打印机等独占设备。<br>两个进程同时进入临界区会导致数据错误或者系统错误。所以操作系统需要控制进程独立对这些临界资源进行访问。<br>对资源的访问需要以下步骤：申请，访问，归还。如果某一个进程申请资源的时候，资源正在被其他进程使用，则该申请的进程需要等待。</p><p>资源可以分成如下两类：<br>可剥夺性资源，是指某进程在获得这类资源后，该资源可以再被其他进程或系统剥夺。如：处理机和内存。优先权高的进程可以剥夺优先权低的进程的处理机。内存区可由存储器管理程序把一个进程从一个存储区移到另一个存储区，此即剥夺了该进程原来占有的存储区。甚至可将一个进程从内存调出到外存上。<br>不可剥夺性资源，当系统把这类资源分配给某进程后，再不能强行收回，只能在进程用完后自行释放，如磁带机、打印机等。 </p><h4 id="死锁产生的原因和必要条件"><a href="#死锁产生的原因和必要条件" class="headerlink" title="死锁产生的原因和必要条件"></a>死锁产生的原因和必要条件</h4><h6 id="死锁的定义：所谓死锁-Deadlock-，是指多个进程因竞争资源而造成的一种僵局-Deadly-Embrace-，若无外力作用，这些进程将永远不能再向前推进。"><a href="#死锁的定义：所谓死锁-Deadlock-，是指多个进程因竞争资源而造成的一种僵局-Deadly-Embrace-，若无外力作用，这些进程将永远不能再向前推进。" class="headerlink" title="死锁的定义：所谓死锁 (Deadlock)，是指多个进程因竞争资源而造成的一种僵局(Deadly-Embrace)，若无外力作用，这些进程将永远不能再向前推进。"></a>死锁的定义：所谓死锁 (Deadlock)，是指多个进程因竞争资源而造成的一种僵局(Deadly-Embrace)，若无外力作用，这些进程将永远不能再向前推进。</h6><p><strong>死锁产生的必要条件</strong></p><p><strong>互斥</strong>: 进程对所分配到的资源必须独立使用，即在一段时间内某资源只由一个进程占用，不能共享。如果此时还有其它进程请求该资源，则请求者只能等待，直至占有该资源的进程使用完毕，对资源进行释放。<br><strong>请求和保持</strong>：进程已经持有了至少一个资源，但又提出了新的资源请求，而该资源又已被其它进程占有，此时请求进程阻塞，但又对已获得的其它资源保持继续持有。<br><strong>不可剥夺</strong>: 在未使用完之前，不能剥夺进程已获得的资源，只能在使用完时由自己释放。<br><strong>循环等待</strong>：在发生死锁时，必然存在一个进程和进程之间等待相互资源的环形链。使得链中每个进程的资源需求都得不到满足。</p><h6 id="死锁的表示方法和判定"><a href="#死锁的表示方法和判定" class="headerlink" title="死锁的表示方法和判定"></a>死锁的表示方法和判定</h6><img src="file:///C:/Blog/source/_posts/image/ba475bbc57a61b0e81bca5148491b7345f315a03.png" title="" alt="图片18.png" width="424"><p>根据SRAG的定义，可以使用以下规则判定死锁<br>如果SRAG中无环路，则系统不会死锁。<br>如果SRAG中有环路，且处于此环中的每类资源只有一个，如图4.7(c)所示，则系统出现死锁。此时，环是系统存在死锁的充分必要条件。<br>如果SRAG中有环路，但是处于此环中的每类资源的个数不全为1，如图4.7(d)所示，则系统不一定会发生死锁。此时，环只是产生死锁的必要条件而不是充分条件。<br>系统存在死锁状态的充要条件是当且仅当系统的SRAG是不可完全简化的。如果资源满足某个进程的要求，则在SRAG中消去此进程的所有请求边和分配边，使其成为孤立结点。对所有进程执行该操作。如果所有进程都成为孤立结点，则称该图是可完全简化的；否则称该图是不可完全简化的。</p><h5 id="死锁预防"><a href="#死锁预防" class="headerlink" title="死锁预防"></a>死锁预防</h5><p>破坏“请求和保持”条件<br>请求：将对资源的申请放在进程运行之前一次性进行，得到了所需所有资源的进程在整个运行期间不会再提出资源要求，从而破坏了请求条件。<br>分配：在分配资源时，只要有一种资源不能满足某进程的要求，即使其它所需的各资源都空闲，也不分配给该进程，而让该进程等待。由于在该进程的等待期间，它并未占有任何资源，因而破坏了保持条件。 </p><p>破坏“不剥夺”条件<br>进程逐个地提出对资源的要求。 当一个已经保持了某些资源的进程，再提出新的资源请求而不能立即得到满足时，必须释放它已经保持了的所有资源，待以后需要时再重新申请。 已经占有的资源，在运行过程中会被暂时地释放掉，从而破坏了“不剥夺”条件。 </p><p>破坏“环路等待”条件<br>系统将所有资源按类型分成不同等级进行排列，并赋予不同的等级号。例如，资源a序号为1，资源b的序号为2，资源c的序号为3，…。所有进程对资源的请求必须严格按照资源序号递增的次序提出，这样，在所形成的资源分配图中，不可能再出现环路，破坏了“环路等待”条件。 这种方法称为有序资源分配法。</p><h4 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h4><p>死锁的预防会牺牲系统的并发性能和资源的利用率，死锁避免通过合理的资源分配确保不会出现循环等待的条件，除了能够避免死锁，还能够支持进程的并发及资源的合理使用。并且死锁避免的过程是动态的，没有强制和预先设置的规则。</p><h6 id="银行家算法"><a href="#银行家算法" class="headerlink" title="银行家算法"></a>银行家算法</h6><p>基本思想：先判断系统是否处于安全状态，然后试探性地接受一个进程的资源请求，试探性分配资源，计算分配之后剩余的可用资源是否能满足系统中其他进程的需要，并且是否有进程能够获取足够多的资源来完成进程，释放资源。<br>如果考虑了完成进程的资源释放和其他进程的需求，能够最终使得每个进程都能够顺利完成，则将真正实施该进程的分配需求；否则，说明系统将处于不安全状态，不会真正实施该分配需求，等待其他进程的资源申请。</p><h6 id="死算检测和恢复"><a href="#死算检测和恢复" class="headerlink" title="死算检测和恢复"></a>死算检测和恢复</h6><p>在资源分配的时候考虑一定的限制，对死锁的预防和避免与一定的效果，但是资源的充分共享方面又会有所欠缺。与之相对应的解决方案是考虑死锁的检测和恢复，不影响资源的合理使用和分配。</p><p>当系统为进程分配资源时，没有采取任何限制性措施，那么系统必须检测系统内部是否出现死锁情况：<br>保存有关资源的请求和分配信息；<br>提供一种算法通过这些信息来检测系统是否已进入死锁状态。<br>相关难点在于：何时以何种频率运行检测死锁的算法。<br>运行太频繁会浪费CPU的处理时间，运行太稀疏又会导致系统内部死锁情况长时间不能被发现。</p><p>当发现有进程死锁时，必须立即把它们从死锁状态中恢复出来。<br>死锁恢复方法<br>取消所有的死锁进程；<br>让每个死锁进程回退到某些安全性检查的时间点之前，并重新启动所有进程；<br>连续取消死锁进程直到不再存在死锁；<br>连续抢占资源直到不再存在死锁。</p><h2 id="五、内存管理"><a href="#五、内存管理" class="headerlink" title="五、内存管理"></a>五、内存管理</h2><h6 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h6><img src="file:///C:/Blog/source/_posts/image/b93249eaf32626ca675be65c09f2d104c7f39dc0.png" title="" alt="图片19.png" width="460"><p>内存<br>           主存储器简称内存或主存，是计算机系统中的主要部件，用于保存进程运行时的程序和数据，也称可执行存储器。<br>寄存器<br>　　  寄存器具有与处理机相同的速度，故对寄存器的访问速度最快，完全能与CPU协调工作，但价格却十分昂贵，因此容量不可能做得很大。 </p><p>高速缓冲存储器<br>　　高速缓存是现代计算机结构中的一个重要部件，它是介于寄存器和存储器之间的存储器，主要用于备份主存中较常用的数据，以减少处理机对主存储器的访问次数，这样可大幅度地提高程序执行速度。高速缓存容量远大于寄存器，而比内存约小两到三个数量级左右，从几十KB到几MB，访问速度快于主存储器。 </p><p>磁盘缓存<br>　　由于目前磁盘的I&#x2F;O速度远低于对主存的访问速度，为了缓和两者之间在速度上的不匹配，而设置了磁盘缓存，主要用于暂时存放频繁使用的一部分磁盘数据和信息，以减少访问磁盘的次数。但磁盘缓存与高速缓存不同，它本身并不是一种实际存在的存储器，而是利用主存中的部分存储空间暂时存放从磁盘中读出(或写入)的信息。主存也可以看作是辅存的高速缓存，因为，辅存中的数据必须复制到主存方能使用，反之，数据也必须先存在主存中，才能输出到辅存。</p><h6 id="地址重定位"><a href="#地址重定位" class="headerlink" title="地址重定位"></a>地址重定位</h6><p>        一个逻辑地址空间的程序装入到物理地址空间时，由于两个空间不一致，需要进行地址变换，称为地址重定位。<br>　    此时，相对地址空间（也称为逻辑地址空间）通过地址重定位机构转换到绝对地址空间（也称为物理地址空间）。</p><h6 id="单一连续、固定分区、可变化分区分配"><a href="#单一连续、固定分区、可变化分区分配" class="headerlink" title="单一连续、固定分区、可变化分区分配"></a>单一连续、固定分区、可变化分区分配</h6><p>单一连续分区存储管理<br>           这是最简单的一种存储管理方式，但只能用于单用户、单任务的操作系统中。<br>　　    在单道程序环境下，当时的存储器管理方式是把内存分为系统区和用户区两部分，系统区仅提供给OS使用，它通常是放在内存的低址部分。而在用户区内存中，仅装有一道用户程序，即整个内存的用户空间由该程序独占。这样的存储器分配方式被称为单一连续分配方式。</p><p>固定分区管理<br>固定分区式分配是最简单的一种可运行多道程序的存储管理方式。这是将内存用户空间划分为若干个固定大小的区域，在每个分区中只装入一道作业，这样，把用户空间划分为几个分区，便允许有几道作业并发运行。<br>当某一分区空闲时，便可以从外存的后备作业队列中选择一个适当大小的作业装入该分区，当该作业结束时，又可再从后备作业队列中找出另一作业调入该分区。</p><p>划分分区的方法　　</p><p>(1) 分区大小相等(指所有的内存分区大小相等)。　　</p><p>(2) 分区大小不等。将内存划分出多个较小的分区，适量的中等分区和少量的大分区。 　　内存分配　　</p><p>为了便于内存分配，通常将分区按其大小进行排队，并为之建立一张分区使用表，其中各表项包括每个分区的起始地址、大小及状态(是否已分配)</p><p><img src="C:\Blog\source_posts\image\d6421cdad702117fe6a9449a114a04c3327f4aa3.png" alt="图片20.png"></p><p>可变分区管理（动态分区管理）　　　<br>固定分区分配是最早的多道程序存储管理方式，由于每个分区的大小固定，必然会造成存储空间的浪费。<br>可变分区分配是指在系统运行的过程中根据作业对内存的实际需要，动态地为之分配内存空间的一种分区方法。<br>可变分区分配是在作业装入和处理过程中建立的分区，并使分区的大小与作业的大小相等。<br>分区的个数和大小不是固定不变的，而是根据装入的作业动态地划分。</p><h4 id="分区分配算法"><a href="#分区分配算法" class="headerlink" title="分区分配算法"></a>分区分配算法</h4><h6 id="首次适应算法"><a href="#首次适应算法" class="headerlink" title="首次适应算法"></a>首次适应算法</h6><p>该算法要求分区链以地址递增的次序链接。内存分配时，从链首开始顺序查找，直至找到一个能满足其大小要求的空闲区为止。然后，再按照作业的大小，从该区中划出一块内存分配给请求者，余下的空闲分区仍留在空闲链中。 </p><p>  该算法倾向于优先利用内存中低址部分的空闲区，高址部分的很少利用，从而保留了高址部分的大空闲区，为以后到达的大作业分配大的内存空间创造了条件。但低址部分留下许多难以利用的很小的空闲区 ，每次查找又都从低址部分开始，这样，增加了查找开销。</p><h6 id="最佳适应算法"><a href="#最佳适应算法" class="headerlink" title="最佳适应算法"></a>最佳适应算法</h6><p> “最佳”的含义是指每次为作业分配内存时，总是把既能满足要求又是最小的空闲分区分配给作业，避免“大材小用”。为加速查询，该算法要求将所有的空闲区按其大小以递增的顺序链接成一空闲区链。这样，第一次找到的满足要求的空闲区，必然是最优的。<br>            孤立地看，这似乎是最佳的，但从宏观上看却不一定。因为每次分配后所切割下的剩余部分，总是最小的，在存储器中会留下许多难以利用的小空闲区。</p><h6 id="最差适应算法"><a href="#最差适应算法" class="headerlink" title="最差适应算法"></a>最差适应算法</h6><p>        由于最坏适应分配算法选择空闲分区的策略正好与最佳适应算法相反：它在扫描整个空闲分区表或链表时，总是挑选一个最大的空闲区，从中分割一部分存储空间给作业使用，以至于存储器中缺乏大的空闲分区，故把它称为是最坏适应算法。</p><p>        该算法要求将所有空闲分区，按容量从大到小的顺序，形成一空闲分区链，查找时，只要看第一个分区能否满足作业要求即可。</p><h6 id="循环首次适应算法"><a href="#循环首次适应算法" class="headerlink" title="循环首次适应算法"></a>循环首次适应算法</h6><p>由首次适应算法演变而来，分配内存时，不再每次从链首开始查找，而是从上次找到的空闲分区的下一个空闲分区开始查找，直到找到第一个能满足要求的空闲分区，从中划出一块与请求大小相等的内存空间。</p><p>为实现该算法，应设置一起始查询指针，并采用循环查找方式。该算法能使内存中的空闲分区分布得更均匀，减少查找空闲分区的开销，但这会缺乏大的空闲分区。</p><h6 id="快速适应算法"><a href="#快速适应算法" class="headerlink" title="快速适应算法"></a>快速适应算法</h6><p>        该算法又称为分类搜索法，是将空闲分区根据其容量大小进行分类，对于每一类具有相同容量的所有空闲分区，单独设立一个空闲分区链表，这样，系统中存在多个空闲分区链表，同时在内存中设立一张管理索引表，该表的每一个表项对应了一种空闲分区类型，并记录了该类型空闲分区链表的表头指针。<br>           该算法在搜索空闲分区时分二步：第一步是根据进程长度从索引表中找到能容纳它的最小空闲区链表；第二步是从链表中取下第一块进行分配。</p><p> 优点：查找效率高。另外在进行空闲分区分配时，不会对任何分区产生分割，能保留大的空闲分区，同时也不会产生内存碎片。<br>缺点：在分区归还时算法比较复杂，系统开销较大。该算法在分配空闲分区时是以进程为单位，一个分区只属于一个进程，因此在为进程所分配的一个分区中，或多或少地存在一定的浪费。空闲分区划分越细，浪费越严重，整体上会造成可观的存储空间浪费，这是典型的以空间换时间的作法。</p><h6 id="哈希算法"><a href="#哈希算法" class="headerlink" title="哈希算法"></a>哈希算法</h6><p> 哈希算法是利用哈希快速查找的优点，以及空闲分区在可利用空闲分区表中的分布规律，建立哈希函数，构造一张以空闲分区大小为关键字的哈希表，该表的每一个表项记录了一个对应的空闲分区链表表头指针。        进行空闲分区分配时，根据所需空闲分区大小，通过哈希函数计算，即得到在哈希表中的位置，从中得到相应的空闲分区链表，实现最佳分配策略。</p><h6 id="伙伴系统"><a href="#伙伴系统" class="headerlink" title="伙伴系统"></a>伙伴系统</h6><p> 该算法规定，无论已分配分区或空闲分区，其大小均为2的k次幂(k为整数，l≤k≤m)，其中2l表示分配的最小块的尺寸，2m表示分配的最大的块的尺寸。通常， 2m是可供分配的整个内存的大小。</p><p>（1）开始时，可用于分配的整个空间被看成是一个大小为2m的块；</p><p>（2）如果请求的大小s满足2m-1 &lt; s &lt;&#x3D; 2m ，则分配整个空间；否则，该块被分成两个大小相等的伙伴，均为2m-1 ，如果有2m-2 &lt; s &lt;&#x3D; 2m-1 ，则给该请求分配两个伙伴中的一个，否则，其中的一个伙伴被继续分成两半。这个过程一直持续直到产生大于或等于s的最小块，并将其分配。</p><h4 id="页、块、页表、地址结构、分页地址变换、快表"><a href="#页、块、页表、地址结构、分页地址变换、快表" class="headerlink" title="页、块、页表、地址结构、分页地址变换、快表"></a>页、块、页表、地址结构、分页地址变换、快表</h4><p>如果离散分配的基本单位是页，则称为分页存储管理方式；如果离散分配的基本单位是段，则称为分段存储管理方式。 </p><p><strong>分页存储管理方式。</strong> 该方式是将用户程序的地址空间划分成若干个固定大小的区域（称为页或页面）；相应地，内存空间也被划分成若干个物理块，页和块的大小相等，典型大小为1KB。这样，用户程序的任意页可放在内存的任一块。<br><strong>分段存储管理方式。</strong> 该方式是把用户程序的地址空间分成若干个大小不等的段，每段可定义一组相对完整的逻辑信息。进行存储分配时，以段为单位，段在内存中可以不邻接。<br><strong>段页式存储管理方式。</strong> 分页和分段两种存储管理方式相结合的产物。</p><p><strong>页面和物理块</strong>　　</p><p>(1) 页面。分页存储管理是将内存分成大小固定的若干块，一般每一块的大小为1KB、2KB或4KB，每个这样的内存块称为页或物理块。将逻辑空间划分成若干个页，并为每页加以编号，从第0页开始，第1页，第2页，……。同时，将内存的物理地址空间划分成若干个块，同样为它们加以编号。进程分配内存时，将进程中的若干个页分别装入到多个可以不相邻的物理块中。会产生“页内碎片”。　　</p><p>(2) 页面大小。 过小，一方面可以减小内存碎片，另一方面会使进程占用较多页面，导致进程页表项过长，占用大量内存；过大，会使页内碎片增大。 </p><p><strong>页表</strong><br>　　    在进行存储分配时，允许将进程的各个页离散地存储在内存中不同的物理块中，但系统应能保证进程的正确运行。<br>　页表中至少应包含以下信息：<br>　(1) 页号：进程各页的序号；<br>　(2) 物理块号：进程各页对应存放在内存中的物理块的块号。</p><p><strong>地址变换过程</strong></p><p> 地址变换机构的基本任务是实现逻辑地址到物理地址的转换。由于页内地址与块内物理地址是一一对应的，故无须再转换。因此地址变换机构的任务，实际上是将逻辑地址中的页号，转换为内存中的物理块号。又因为页面映射表的作用就是用于实现从页号到物理块号的变换，所以地址变换任务是借助于页表来完成的。</p><p>页表的功能可以由一组专门的寄存器来实现，一个页表项用一个寄存器。但现代计算机系统大多都将页表驻留在内存，并设置一个页表寄存器PTR(Page-Table Register)，用于存放页表在内存的始址和页表的长度。</p><p>进程要访问某个逻辑地址中的数据时。</p><p>（1）分页地址机构自动地将有效地址分为页号和页内地址两部分；</p><p>（2）再以页号为索引去检索页表；</p><p>（3）将页表始址与页号和页表项长度的乘积相加，便得到该表项在页表中的位置，于是从中得到该页的物理块号，将之装入物理地址寄存器中；</p><p>（4）同时再将有效地址寄存器中的页内地址直接送入物理地址寄存器的块内地址字段中。</p><p><strong>快表</strong></p><p>    由于页表作为数据本身是存放在内存中的，这使CPU在每存取一个数据时，都要两次访问内存。第一次是访问内存中的页表，从中找到指定页的物理块号，再将块号与页内偏移量W拼接，以形成物理地址。第二次访问内存时，才是从第一次所得地址中获得所需数据(或向此地址中写入数据)。因此，采用这种方式将使计算机的处理速度降低近1&#x2F;2。可见，以此高昂代价来换取存储器空间利用率的提高，是得不偿失的。</p><p>            为提高地址变换速度，可在地址变换机构中增设一个称为快表的高速缓冲存储器，用以存放当前访问的页表项。<br>           此时地址变换过程是：CPU给出有效地址后，地址变换机构自动地将页号P送入快表高速缓存，并与其中的所有页号比较，若有与此相匹配的页号，则直接读出该页所对应的物理块号，送物理地址寄存器。若未找到，则再访问内存中的页表，找到后，把从页表项中读出的物理块号送地址寄存器；同时将此页表项存入快表。若快表已满，OS须找到一个老的且已被认为不再需要的页表项将它换出。</p><p><img src="C:\Blog\source_posts\image\561540b15d3eef435faac3a94ebebd72420948db.png" alt="图片21.png"></p><p>假如查找快表需要20ns，访问内存需要100ns，那么总的访问时间是120ns。如果不能在快表中找到该页表项，那么必须先访问位于内存中的页表得到相应的物理块号(花费120ns)，然后再访问内存(花费100ns)，总共需要220ns。则：<br>      EAT＝0.85×(20+100)＋(1-0.85)×(20+200)＝135ns</p><h6 id="两级页表、多级页表"><a href="#两级页表、多级页表" class="headerlink" title="两级页表、多级页表"></a>两级页表、多级页表</h6><p>两级页表<br>        对于要求连续的内存空间来存放页表的问题，可以利用将页表进行分页，并离散的将各个页面分别存放在不同的物理块中的办法来加以解决，同样也要为离散分配的页表再建立一张页表，称为外层页表(Outer Page Table) ，在每个页表项中记录了页表页面的物理块号。</p><p>多级页表<br>随着64位机器的普及，两级页表会出现外部页表非常大，要占用相当大的内存空间的问题。<br>可以采用三级或三级以上页表的方式，将原来两级页表中的外部页表再进行分页，然后利用第二级的外层页表来映射页表之间的关系。<br>多级页表类似于多级目录。<br>UNIX系统中使用了三级页表来实现地址映射。</p><h4 id="段、段表、地址结构、分段地址变换"><a href="#段、段表、地址结构、分段地址变换" class="headerlink" title="段、段表、地址结构、分段地址变换"></a>段、段表、地址结构、分段地址变换</h4><p>动态分区分配方式中，系统为整个进程分配一个连续的内存空间。而在分段式存储管理系统中则以段为单位分配内存，每段分配一个连续的内存区域，但各段之间不要求连续。<br>        其内存的分配和回收类似于动态分区分配方式，但两者之间也有不同。在分段存储管理系统中，一个作业可以有多个段，这些段允许离散地存放在内存的不同分区当中，而分区存储管理方式则要求整个作业存放在一个内存分区中。</p><p><strong>段表</strong></p><p>        作业的一个段被分配一个连续的分区，而进程中的各个段可以离散地放入内存中不同的分区中，为能从物理内存中找出每个逻辑段所对应的位置，在系统中为每个进程建立一张段映射表，简称段表。每个段在表中占有一个表项，其中记录了该段在内存中的起始地址（基址）和段的长度（段长）。段表可以存放在一组寄存器中，更常见的是存放在内存中。</p><h4 id="分页和分段的区别"><a href="#分页和分段的区别" class="headerlink" title="分页和分段的区别"></a>分页和分段的区别</h4><p>分页和分段系统有许多相似之处，但在概念上二者完全不同：<br>(1) 页是信息的物理单位，仅仅是出于系统管理的需要；段是信息的逻辑单位，其目的是满足用户的需要。<br>(2) 页的大小固定且由系统确定，一个系统只能有一种大小的页面；段的长度不固定，决定于用户所编写的程序；<br>(3) 分页的作业地址空间是一维的；分段的作业地址空间是二维的。<br>(4) 通常分段的段内空间会比分页的页面空间大，因此段表会比页表短。</p><h6 id="段页式存储管理"><a href="#段页式存储管理" class="headerlink" title="段页式存储管理"></a>段页式存储管理</h6><p>基本原理<br>            段页式系统的基本原理是分段和分页原理的结合，即先将用户程序分成若干个段，再把每个段分成若干个页，并为每一个段赋予一个段名。在段页式系统中，其地址结构由段号、段内页号及页内地址三部分所组成</p><p>   为实现从逻辑地址到物理地址的变换，系统需同时配置段表和页表，段表的内容是页表始址和页表长度，这与分段式系统不同。</p><img src="file:///C:/Blog/source/_posts/image/b47d6688f6e2e672f70ecddb9d3a0dbe07a89cfc.png" title="" alt="图片22.png" width="562"><p>地址变换过程<br>        在段页式系统中，为了便于实现地址变换，需要配置一个段表寄存器，其中存放段表始址和段表长度。进行地址变换时按如下步骤进行：<br>(1) 通过段表寄存器将段号与段表长度进行比较，如果未越界则查找段表在内存中的位置，否则越界中断；<br>(2) 访问段表，将页表长度与页号进行比较，如果未越界则根据段号查找页表所在的位置；<br>(3) 访问页表，根据页号查找该页所在的物理块号；<br>(4) 将物理块号和地址结构中的页内地址相加，形成内存单元的物理地址。</p><p><img src="C:\Blog\source_posts\image\f75a5fb533c54bb320279f1026da463f95e5f480.png" alt="图片23.png"></p><h6 id="基本原理：局部性原理、虚拟存储器"><a href="#基本原理：局部性原理、虚拟存储器" class="headerlink" title="基本原理：局部性原理、虚拟存储器"></a>基本原理：局部性原理、虚拟存储器</h6><p><strong>局部性原理</strong></p><p>程序运行时存在的局部性现象，很早就已被人发现，但直到1968年，P.Denning才真正指出：程序在执行时将呈现出局部性规律，即在一较短的时间内，程序的执行仅局限于某个部分，相应地，它所访问的存储空间也局限于某个区域。</p><p> 局限性又表现在下述两个方面：</p><p>(1) 时间局限性。程序中的某条指令被执行，不久后会再次执行；某个数据被访问，不久后将再次被访问。产生时间局限性的典型原因是在程序中存在着大量的循环操作。　　(2) 空间局限性。 程序访问了某个存储单元，不久后，其附近的存储单元也将被访问。</p><p><strong>虚拟存储器</strong></p><p>基于局部性原理可知，应用程序在运行之前没有必要将之全部装入内存，而仅须将那些当前要运行的少数页面或段先装入内存便可运行，其余部分暂留在盘上。<br>        局部性原理是实现虚拟存储管理的理论基础。<br>        所谓虚拟存储器是指仅把作业的一部分装入内存便可运行的存储器系统，是具有请求调入功能和置换功能，能从逻辑上对内存容量进行扩充的一种存储器系统。虚拟存储器的逻辑容量由系统的寻址能力和外存容量之和所决定，与实际的内存容量无关。</p><p>虚拟存储特征　</p><p>(1) 多次性：是指一个作业被分成多次来调入内存，即作业运行时不需将其全部装入内存，只需将当前要运行的那部分程序和数据装入，以后运行到某些部分时再将其调入。 (2) 对换性：是指允许作业中的程序和数据，在作业运行过程中换进、换出。</p><p>(3) 虚拟性：是指能够从逻辑上扩充内存容量，使用户所看到的内存容量远大于实际内存容量。</p><p> 虚拟性以多次性和对换性为基础，多次性和对换性以离散分配为基础。</p><h6 id="请求分页存储管理"><a href="#请求分页存储管理" class="headerlink" title="请求分页存储管理"></a>请求分页存储管理</h6><p> 每当所要访问的页面不在内存时，便要产生一次缺页中断，请求OS将所缺之页调入内存。缺页中断虽要经历与一般中断相同的几个步骤，但它是一种特殊的中断，与一般中断的区别主要是：</p><p> (1) 在指令执行期间产生和处理中断信号。通常CPU都是在一条指令执行完后去检查是否有中断请求到达。有则响应，无则继续执行下一条指令。而缺页中断是在指令执行期间，发现所要访问的指令和数据不在内存时产生和处理的。</p><p> (2) 一条指令在执行期间，可能产生多次缺页中断。这时硬件机构应能保存多次中断时的状态，并保证最后能返回到中断前产生缺页中断的指令处，继续执行。</p><h4 id="页面置换算法：最佳置换、FIFO、LRU、第二次机会、CLOCK-置换"><a href="#页面置换算法：最佳置换、FIFO、LRU、第二次机会、CLOCK-置换" class="headerlink" title="页面置换算法：最佳置换、FIFO、LRU、第二次机会、CLOCK 置换"></a>页面置换算法：最佳置换、FIFO、LRU、第二次机会、CLOCK 置换</h4><h6 id="请求分页存储管理系统性能分析：缺页率、抖动、页面大小"><a href="#请求分页存储管理系统性能分析：缺页率、抖动、页面大小" class="headerlink" title="请求分页存储管理系统性能分析：缺页率、抖动、页面大小"></a>请求分页存储管理系统性能分析：缺页率、抖动、页面大小</h6><p>缺页率的影响因素: 1) 页面大小；2) 进程分配物理块的数目；3) 页面置换算法；4) 程序固有特性。</p><p>假设使用了“快表”以提高访问内存的速度，则CPU访问内存所花费的时间由以下3个部分组成：<br>(1) 页面在“快表”时的存取时间，只需1个读写周期时间；<br>(2) 页面不在“快表”而在页表时的存取时间，需要2个读写周期时间；<br>(3) 页面既不在“快表”也不在页表时，发生缺页中断处理的时间。<br>        缺页中断处理的时间又有3个部分组成：<br>(1) 缺页中断服务时间；<br>(2) 页面传送时间，包括：寻道时间、旋转时间和数据传送时间。<br>(3) 进程重新执行时间。</p><p>置换算法的好坏将直接影响系统的性能，不适当的算法可能导致进程发生“抖动” (Thrash-ing)。即刚被换出的页很快又被访问，需重新调入。为此，又需再选一页调出；而此刚被换出的页，很快又被访问，因而又需将它调入。如此频繁地更换页面，以至一个进程在运行中，将把大部分时间花在完成页面置换的工作上，我们称该进程发生了“抖动”。      抖动现象分为局部抖动和全局抖动两种类型。抖动现象如下两种类型：<br>        1) 局部抖动：进程采用局部置换策略，产生缺页时，只能置换自身拥有的某个页面，而不能置换其它进程的页面<br>        2) 全局抖动：由进程之间的相互作用引起的，若进程采用的是全局置换策略，当一个进程发生缺页中断时，需要从其它进程那里获取物理块，从而导致这些进程在运行中需要物理块，产生了缺页中断时，又需要从其它进程那里获取物理块。</p><p>页面大小的选择<br>页面大小的选择涉及到内部碎片、页表大小、页面失效率的高低等诸多问题<br>页面越小，内部碎片就越少，内存利用率就越高，但同时就会产生较大的页表，占用较大的内存空间；<br>页面过大，会导致页面在内外存之间传输时间的增加，从而降低了系统的有效访问时间；<br>页面较小，内存中包含的页面数就较多，相应的，缺页率就会较低，但随着页面的增大，缺页率也会随之升高，当页面大小超过进程的大小时，缺页率又开始下降。<br>选择最优的页面大小需要在这几个互相矛盾的因素之间权衡利弊。另外，页面的大小还与主存的大小、程序设计技术和程序结构以及快表等因素有关。</p><h6 id="请求分段存储管理"><a href="#请求分段存储管理" class="headerlink" title="请求分段存储管理"></a>请求分段存储管理</h6><p>请求分段存储管理系统是在分段存储管理系统的基础上增加了请求调段功能和段置换功能后所形成的分段虚拟存储管理系统。由于作业的各段是根据请求被装入内存的，因此，这种存储管理也称为请求分段存储管理。</p><p> 请求分段存储管理系统把作业的所有分段的副本都存放在外存中，当作业被调度投入执行时，先把当前需要的—段或几段装入内存。在执行过程中，出现缺段中断时，再把存储在外存上的段交换至内存，以此实现请求分段存储管理。</p><h2 id="六、设备管理"><a href="#六、设备管理" class="headerlink" title="六、设备管理"></a>六、设备管理</h2><h6 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h6><h4 id="I-x2F-O系统控制方式"><a href="#I-x2F-O系统控制方式" class="headerlink" title="I&#x2F;O系统控制方式"></a>I&#x2F;O系统控制方式</h4><h6 id="程序直接控制方式"><a href="#程序直接控制方式" class="headerlink" title="程序直接控制方式"></a>程序直接控制方式</h6><h6 id="中断控制方式"><a href="#中断控制方式" class="headerlink" title="中断控制方式"></a>中断控制方式</h6><h6 id="DMA控制方式"><a href="#DMA控制方式" class="headerlink" title="DMA控制方式"></a>DMA控制方式</h6><h6 id="通道控制方式"><a href="#通道控制方式" class="headerlink" title="通道控制方式"></a>通道控制方式</h6><h4 id="通道类型"><a href="#通道类型" class="headerlink" title="通道类型"></a>通道类型</h4><h6 id="磁盘简述"><a href="#磁盘简述" class="headerlink" title="磁盘简述"></a>磁盘简述</h6><h4 id="磁盘调度：FCFS-SSTF-SCAN-C-SCAN-N步扫描，F-SCAN"><a href="#磁盘调度：FCFS-SSTF-SCAN-C-SCAN-N步扫描，F-SCAN" class="headerlink" title="磁盘调度：FCFS,SSTF,SCAN,C-SCAN,N步扫描，F-SCAN"></a>磁盘调度：FCFS,SSTF,SCAN,C-SCAN,N步扫描，F-SCAN</h4><h2 id="七、文件管理"><a href="#七、文件管理" class="headerlink" title="七、文件管理"></a>七、文件管理</h2><h6 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h6><h4 id="物理结构"><a href="#物理结构" class="headerlink" title="物理结构"></a>物理结构</h4><ul><li><p>顺序结构</p></li><li><p>链接结构</p></li><li><p>索引结构</p></li><li><p>直接文件</p></li><li><p>哈希文件</p></li></ul><h6 id="文件控制块"><a href="#文件控制块" class="headerlink" title="文件控制块"></a>文件控制块</h6><h6 id="索引节点"><a href="#索引节点" class="headerlink" title="索引节点"></a>索引节点</h6><h6 id="目录查询"><a href="#目录查询" class="headerlink" title="目录查询"></a>目录查询</h6><h6 id="文件共享：符号链接实现共享、索引节点实现共享"><a href="#文件共享：符号链接实现共享、索引节点实现共享" class="headerlink" title="文件共享：符号链接实现共享、索引节点实现共享"></a>文件共享：符号链接实现共享、索引节点实现共享</h6><h6 id="文件安全：存取控制矩阵、存取控制表、用户权限表、口令方法"><a href="#文件安全：存取控制矩阵、存取控制表、用户权限表、口令方法" class="headerlink" title="文件安全：存取控制矩阵、存取控制表、用户权限表、口令方法"></a>文件安全：存取控制矩阵、存取控制表、用户权限表、口令方法</h6><h2 id="十、系统安全"><a href="#十、系统安全" class="headerlink" title="十、系统安全"></a>十、系统安全</h2><h6 id="逻辑炸弹、缓冲区溢出、SQP注入"><a href="#逻辑炸弹、缓冲区溢出、SQP注入" class="headerlink" title="逻辑炸弹、缓冲区溢出、SQP注入"></a>逻辑炸弹、缓冲区溢出、SQP注入</h6><p><strong>逻辑炸弹</strong>：逻辑炸弹是指在特定逻辑条件得到满足时实施破坏的计算机程序。可能造成计算机数据丢失、计算机无法从硬盘或软盘引导，甚至导致整个系统瘫痪，并出现物理损坏的虚假现象。<br>逻辑炸弹触发时的特定条件称为逻辑诱因。</p><p>逻辑炸弹的危害：直接破坏计算机软件产品的使用者的计算机数据。<br>引发连带的社会灾难，包括直接或间接的损失，如企业亏损、资料丢失、科学研究的永久性失败、当事人承受精神打击、刑事犯罪等。</p><p><strong>缓冲区溢出</strong>：是指当计算机向缓冲区内填充数据位数时超过了缓冲区本身的容量，使得溢出的数据覆盖在合法数据上。理想的情况是，程序检查数据长度、同时不允许输入超过缓冲区长度的字符；然而绝大多数程序都会假设数据长度总是与所分配的储存空间相匹配，这就为缓冲区溢出现象的发生埋下了隐患。<br>利用缓冲区溢出攻击，可以导致程序运行失败、系统宕机、重新启动等后果。更严重的是，可以利用它执行非授权指令，甚至可以取得系统特权进而执行某些非法操作。</p><p><strong>SQL注入</strong>：是对数据库进行攻击的常用手段之一。<br>它利用现有应用程序可以将SQL命令注入到后台数据库引擎执行的能力，通过在Web表单、域名输入或页面请求的查询字符串等内容中输入恶意的SQL语句得到一个存在安全漏洞的网站的数据库，最终达到欺骗服务器执行恶意SQL命令的目的。<br>SQL注入攻击通过构建特殊的输入作为参数传入Web应用程序，而这些输入多为SQL语法里的一些组合，通过执行SQL语句进而执行攻击者所需的操作，其发生原因主要是程序没有过滤用户输入的数据，致使非法数据侵入系统。<br>SQL注入可分为平台层注入和代码层注入。前者由不安全的数据库配置或数据库平台的漏洞所致；后者主要是由于程序员对输入未进行细致过滤，从而执行了非法的数据查询。</p><h6 id="特洛伊木马、计算机病毒、蠕虫、rootkit"><a href="#特洛伊木马、计算机病毒、蠕虫、rootkit" class="headerlink" title="特洛伊木马、计算机病毒、蠕虫、rootkit"></a>特洛伊木马、计算机病毒、蠕虫、rootkit</h6><p><strong>特洛伊木马：</strong> 没有复制能力，其特点是伪装成一个实用工具或者一个可爱的游戏，诱使用户将其安装在PC或者服务器上。<br>在不经意间，特洛伊木马可能对使用者的计算机系统产生破坏，或窃取数据特别是使用者的各种账户及口令等重要且需要保密的信息，甚至直接控制计算机系统。一个完整的特洛伊木马套装程序包含两个部分：服务端（服务器部分）和客户端（控制器部分）。植入对方计算机的是服务端，而攻击者正是利用客户端进入运行了服务端的计算机。</p><p><strong>计算机病毒：</strong> 是指编制者在计算机程序中插入的、破坏计算机功能或者破坏数据、影响计算机使用并且能够自我复制的一组计算机指令或恶意的程序代码。<br>与生物病毒不相同的是，计算机病毒不是自然存在的生命体，而是某些人利用计算机软件或硬件固有的脆弱性编制出来的，其本质是一组指令集或程序代码。病毒能通过某种途径长期潜伏在计算机的存储介质（或程序）中，当达到某种条件时即被激活；同时，它还可以通过修改其他程序将自己的精确拷贝或者可能演化的形式植入其他程序中，从而感染更多程序。</p><p><strong>计算机病毒的特点</strong><br><strong>传染性</strong>是病毒的基本特征，是指计算机病毒在一定条件下可以自我复制，能对其它文件或系统进行一系列非法操作，并使之成为新的传染源。<br><strong>繁殖性</strong>：计算机病毒可以将与自身完全相同的副本植入其他程序或者存储介质的特定区域，使每一个受感染程序都同时包含病毒的一个克隆体。是否具备繁殖、感染的特征，是判断某一段程序为计算机病毒的首要条件。</p><p><strong>潜伏性</strong>：计算机病毒的潜伏性是指计算机病毒依附于其他载体寄生的一种能力，这使侵入的病毒可以潜伏在系统中直到条件成熟才会发作。<br>破坏性：所有的计算机病毒都是可执行程序，所以他们对计算机系统而言必然存在一个共同的危害，就是一旦执行便会占用系统资源，严重时会降低计算机系统工作效率。<br><strong>隐蔽性</strong>：计算机病毒通常具有很强的隐蔽性，这是计算机病毒难以被查杀的一个重要原因。<br><strong>可触发性</strong>：病毒因某个事件或数值的出现，诱使病毒实施感染或进行攻击的特性称为可触发性。</p><p><strong>计算机病毒、特洛伊木马与逻辑炸弹的比较</strong><br>在计算机程序中，病毒、木马与逻辑炸弹是常见的三种破坏手段，它们相互联系又各有区别。三者的共性在于它们都对计算机系统产生危害。<br>病毒是通过自我复制进行传播的计算机程序，自我复制是其基本特性，但它的破坏机制却不是必备的，所以现实中也存在一些只传染复制而不实施恶性破坏的、所谓的“良性”病毒。<br>木马虽然也是一种程序，但它只具备破坏性却不能完成自我复制。典型木马程序是以“冒充”来作为传播手段的，这同病毒在新目标中植入自己的副本这种“繁殖”方式显而易见存在差别。<br>逻辑炸弹一般隐含在具有正常功能的软件内部，并不像典型的木马程序那样仅仅只是模仿程序的外表而没有真正的程序功能。</p><h6 id="蠕虫"><a href="#蠕虫" class="headerlink" title="蠕虫"></a>蠕虫</h6><p>所谓蠕虫，又被称为蠕虫病毒，其实是一种结合了蠕虫特性与病毒机理（技术特点）的产物。目前主流的定义认为，蠕虫是一种能够利用系统漏洞通过网络进行自我传播的恶意程序。<br>蠕虫同时集成了蠕虫和病毒两者的特征，从而使其自身更加强大、传播能力也更强，它还有一个显著特点是不一定需要附着在其他程序上而可以独立存在。当形成规模与传播速度相当快时，蠕虫攻击会极大地消耗网络资源，从而导致大面积网络拥塞甚至瘫痪。</p><p>蠕虫分为主机蠕虫与网络蠕虫。其中，主机蠕虫完全包含在它们运行的计算机中，并且使用网络的连接将自身拷贝到其他的计算机中。主机蠕虫在完成自身的拷贝加入到另外的主机之后，就会终止自身的行为。<br>如果根据攻击目的进行划分，蠕虫又可以分成两类：一类是面对大规模计算机网络发动拒绝服务的计算机蠕虫，另一类则是针对个人用户的执行大量垃圾代码的计算机蠕虫。<br>蠕虫由两部分组成：一个主程序和一个引导程序。主程序一旦在电脑中建立就会去收集与当前电脑联网的其它主机信息。随后，主程序会尝试利用系统缺陷去在这些远程计算机上建立其引导程序。</p><h6 id="rootkit"><a href="#rootkit" class="headerlink" title="rootkit"></a>rootkit</h6><p>rootkit一词最早出现在Unix系统中。系统入侵者为了取得系统管理员级的root权限或为了清除被系统记录的入侵痕迹，会重新汇编一些软件工具，这些工具就被称为kit。由此rootkit可以视作一项技术。<br>一种公认的定义认为：rootkit是指其主要功能为隐藏其他程式进程的软件，它可能是一个或多个软件的组合。从其定义不难看出，rootkit是一种特殊的恶意软件，其功能是在安装目标上隐藏自身及指定的文件、进程或网络链接等信息。目前，rootkit更多的是指那些被作为驱动程序加载到操作系统内核中的恶意软件。</p><p>通常情况下，rootkit总是和木马、后门等其他恶意程序结合使用。rootkit通过加载特殊的驱动、修改系统内核，进而达到隐藏信息的目的。<br>rootkit是一种奇特的程序，它具有隐身功能。无论是作为文件存在的静止时刻，还是作为进程存在的活动时刻都不会被察觉。<br>这一特性恰是一些人梦寐以求的——不论是计算机黑客，还是计算机取证人员。前者可以在入侵后置入rootkit，秘密窥探敏感信息或等待时机、伺机而动；后者则可以利用rootkit实时监控嫌疑人的不法行为，这不仅帮助搜集证据还有利于采取及时行动。</p>]]></content>
      
      
      <categories>
          
          <category> 大学学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 考研 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-tensor</title>
      <link href="/2023/05/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-tensor/"/>
      <url>/2023/05/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-tensor/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-ResNet</title>
      <link href="/2023/05/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-ResNet/"/>
      <url>/2023/05/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-ResNet/</url>
      
        <content type="html"><![CDATA[<h6 id="网络层数越来越深出现的问题："><a href="#网络层数越来越深出现的问题：" class="headerlink" title="网络层数越来越深出现的问题："></a>网络层数越来越深出现的问题：</h6><ol><li><p>梯度消失或梯度爆炸问题</p></li><li><p>退化问题</p></li></ol><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/resnet1.png"></p><h4 id="ResNet创新点："><a href="#ResNet创新点：" class="headerlink" title="ResNet创新点："></a>ResNet创新点：</h4><ul><li><p>超级深的网络结构</p></li><li><p>提出Residual模块</p></li><li><p>使用Batch Normalization 加速训练，丢弃Dropout</p></li></ul><h6 id="ResNet34网络结构图如下"><a href="#ResNet34网络结构图如下" class="headerlink" title="ResNet34网络结构图如下"></a>ResNet34网络结构图如下</h6><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/resnet2.png"></p><h5 id="Residual模块"><a href="#Residual模块" class="headerlink" title="Residual模块"></a>Residual模块</h5><p>左图主要针对的是网络层数较少的网络，右图主要针对网络层数较多的网络</p><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/resnet3.png"></p><p>在左边的结构中，主线是将输入特征矩阵经过两个 3 × 3 的卷积层，右边有一个弧线直接从输入连接到输出，与卷积后的特征矩阵按元素相加得到最终特征矩阵结果。右边的结构的主分支则是在输入与输出都加上了 1 × 1 的卷积层，用来实现降维和升维。右边的参数节省了 94%</p><h5 id="实现代码"><a href="#实现代码" class="headerlink" title="实现代码"></a>实现代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.relu = nn.ReLU()</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=out_channel, out_channels=out_channel,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(out_channel)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Bottleneck</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    注意：原论文中，在虚线残差结构的主分支上，第一个1x1卷积层的步距是2，第二个3x3卷积层步距是1。</span></span><br><span class="line"><span class="string">    但在pytorch官方实现过程中是第一个1x1卷积层的步距是1，第二个3x3卷积层步距是2，</span></span><br><span class="line"><span class="string">    这么做的好处是能够在top1上提升大概0.5%的准确率。</span></span><br><span class="line"><span class="string">    可参考Resnet v1.5 https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_channel, out_channel, stride=<span class="number">1</span>, downsample=<span class="literal">None</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>, width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(Bottleneck, self).__init__()</span><br><span class="line"></span><br><span class="line">        width = <span class="built_in">int</span>(out_channel * (width_per_group / <span class="number">64.</span>)) * groups</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)  <span class="comment"># squeeze channels</span></span><br><span class="line">        self.bn1 = nn.BatchNorm2d(width)</span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,</span><br><span class="line">                               kernel_size=<span class="number">3</span>, stride=stride, bias=<span class="literal">False</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(width)</span><br><span class="line">        <span class="comment"># -----------------------------------------</span></span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel*self.expansion,</span><br><span class="line">                               kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, bias=<span class="literal">False</span>)  <span class="comment"># unsqueeze channels</span></span><br><span class="line">        self.bn3 = nn.BatchNorm2d(out_channel*self.expansion)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.downsample = downsample</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        identity = x</span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            identity = self.downsample(x)</span><br><span class="line"></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        out = self.conv3(out)</span><br><span class="line">        out = self.bn3(out)</span><br><span class="line"></span><br><span class="line">        out += identity</span><br><span class="line">        out = self.relu(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ResNet</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,</span></span><br><span class="line"><span class="params">                 block,</span></span><br><span class="line"><span class="params">                 blocks_num,</span></span><br><span class="line"><span class="params">                 num_classes=<span class="number">1000</span>,</span></span><br><span class="line"><span class="params">                 include_top=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">                 groups=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">                 width_per_group=<span class="number">64</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ResNet, self).__init__()</span><br><span class="line">        self.include_top = include_top</span><br><span class="line">        self.in_channel = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.groups = groups</span><br><span class="line">        self.width_per_group = width_per_group</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, self.in_channel, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>,</span><br><span class="line">                               padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(self.in_channel)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.maxpool = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.layer1 = self._make_layer(block, <span class="number">64</span>, blocks_num[<span class="number">0</span>])</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, blocks_num[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, blocks_num[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, blocks_num[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            self.avgpool = nn.AdaptiveAvgPool2d((<span class="number">1</span>, <span class="number">1</span>))  <span class="comment"># output size = (1, 1)</span></span><br><span class="line">            self.fc = nn.Linear(<span class="number">512</span> * block.expansion, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m, nn.Conv2d):</span><br><span class="line">                nn.init.kaiming_normal_(m.weight, mode=<span class="string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_make_layer</span>(<span class="params">self, block, channel, block_num, stride=<span class="number">1</span></span>):</span><br><span class="line">        downsample = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> self.in_channel != channel * block.expansion:</span><br><span class="line">            downsample = nn.Sequential(</span><br><span class="line">                nn.Conv2d(self.in_channel, channel * block.expansion, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(channel * block.expansion))</span><br><span class="line"></span><br><span class="line">        layers = []</span><br><span class="line">        layers.append(block(self.in_channel,</span><br><span class="line">                            channel,</span><br><span class="line">                            downsample=downsample,</span><br><span class="line">                            stride=stride,</span><br><span class="line">                            groups=self.groups,</span><br><span class="line">                            width_per_group=self.width_per_group))</span><br><span class="line">        self.in_channel = channel * block.expansion</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, block_num):</span><br><span class="line">            layers.append(block(self.in_channel,</span><br><span class="line">                                channel,</span><br><span class="line">                                groups=self.groups,</span><br><span class="line">                                width_per_group=self.width_per_group))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu(x)</span><br><span class="line">        x = self.maxpool(x)</span><br><span class="line"></span><br><span class="line">        x = self.layer1(x)</span><br><span class="line">        x = self.layer2(x)</span><br><span class="line">        x = self.layer3(x)</span><br><span class="line">        x = self.layer4(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.include_top:</span><br><span class="line">            x = self.avgpool(x)</span><br><span class="line">            x = torch.flatten(x, <span class="number">1</span>)</span><br><span class="line">            x = self.fc(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet34</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(BasicBlock, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet50</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet50-19c8e357.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnet101</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnet101-5d3b4d8f.pth</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>], num_classes=num_classes, include_top=include_top)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnext50_32x4d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">4</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resnext101_32x8d</span>(<span class="params">num_classes=<span class="number">1000</span>, include_top=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="comment"># https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth</span></span><br><span class="line">    groups = <span class="number">32</span></span><br><span class="line">    width_per_group = <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> ResNet(Bottleneck, [<span class="number">3</span>, <span class="number">4</span>, <span class="number">23</span>, <span class="number">3</span>],</span><br><span class="line">                  num_classes=num_classes,</span><br><span class="line">                  include_top=include_top,</span><br><span class="line">                  groups=groups,</span><br><span class="line">                  width_per_group=width_per_group)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load ImageNet pretrain parameter</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    my_output_channel = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># load pretrain weights</span></span><br><span class="line">    <span class="comment"># download url: https://download.pytorch.org/models/resnet34-333f7ec4.pth</span></span><br><span class="line">    model_weight_path = <span class="string">&quot;./resnet34-pre.pth&quot;</span></span><br><span class="line">    <span class="keyword">assert</span> os.path.exists(model_weight_path), <span class="string">&quot;file &#123;&#125; does not exist.&quot;</span>.<span class="built_in">format</span>(model_weight_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># option1</span></span><br><span class="line">    net = resnet34()</span><br><span class="line">    net.load_state_dict(torch.load(model_weight_path, map_location=device))</span><br><span class="line">    <span class="comment"># change fc layer structure</span></span><br><span class="line">    in_channel = net.fc.in_features</span><br><span class="line">    net.fc = nn.Linear(in_channel, my_output_channel)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># option2</span></span><br><span class="line">    <span class="comment"># net = resnet34(num_classes=my_output_channel)</span></span><br><span class="line">    <span class="comment"># pre_weights = torch.load(model_weight_path, map_location=device)</span></span><br><span class="line">    <span class="comment"># del_key = []</span></span><br><span class="line">    <span class="comment"># for key, _ in pre_weights.items():</span></span><br><span class="line">    <span class="comment">#     if &quot;fc&quot; in key:</span></span><br><span class="line">    <span class="comment">#         del_key.append(key)</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># for key in del_key:</span></span><br><span class="line">    <span class="comment">#     del pre_weights[key]</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># missing_keys, unexpected_keys = net.load_state_dict(pre_weights, strict=False)</span></span><br><span class="line">    <span class="comment"># print(&quot;[missing_keys]:&quot;, *missing_keys, sep=&quot;\n&quot;)</span></span><br><span class="line">    <span class="comment"># print(&quot;[unexpected_keys]:&quot;, *unexpected_keys, sep=&quot;\n&quot;)</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 图像分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python-文件读写</title>
      <link href="/2023/05/21/python-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/"/>
      <url>/2023/05/21/python-%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-多标签图像分类</title>
      <link href="/2023/05/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/"/>
      <url>/2023/05/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="多标签图像分类论文学习"><a href="#多标签图像分类论文学习" class="headerlink" title="多标签图像分类论文学习"></a>多标签图像分类论文学习</h1><h2 id="CSRA-ICCV2021"><a href="#CSRA-ICCV2021" class="headerlink" title="CSRA(ICCV2021)"></a>CSRA(ICCV2021)</h2><p>Residual Attention: A Simple but Effective Method for Multi-Label Recognition</p><p>灵感来源</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312292056643.png" alt="image-20231229205601601"></p><h3 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h3><ul><li>提出一个简单但是有效的方式来提升预训练模型（不需要进一步的训练）</li><li>提出CSRA模块（在四个多标签数据集中取得优异结果）</li><li>对于主要的注意力模型的一个直观地可视化</li></ul><h3 id="总体架构"><a href="#总体架构" class="headerlink" title="总体架构"></a>总体架构</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312292057588.png" alt="image-20231229205751527"></p><p>大致流程：图片通过CNN Backbone 得到特征图，再通过1×1的卷积，将特征图变为C×h×w，C代表的是类的个数，然后将特征送入多头CSRA模块，得到输出y&#96; ,将所有的 y相加，得到最终的输出。</p><h3 id="Residual-attention"><a href="#Residual-attention" class="headerlink" title="Residual attention"></a>Residual attention</h3><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312292047012.png" alt="image-20231229204747899"></p><p>举例：X:  h×w×d(7×7×2048)，代表的是$x_1,x_2,x_3…x_{49},其中x_{i}∈R^{2048}$,经过1×1的卷积部分，得到$m_1,m_2,…m_{i},其中m_i代表的是第i类$,</p><h4 id="Average-pooling"><a href="#Average-pooling" class="headerlink" title="Average pooling"></a>Average pooling</h4><p>平均池化：$g&#x3D;\sum_{k&#x3D;1}^{49}x_k$</p><h4 id="Spatial-pooling"><a href="#Spatial-pooling" class="headerlink" title="Spatial pooling"></a>Spatial pooling</h4><p>空间池化： $s_j^i&#x3D;\frac{exp(Tx_j^Tm_i)}{ \sum_{k&#x3D;1}^{49}exp(Tx_k^Tm_i)}$,并且$\sum_{k&#x3D;1}^{49}s_k^i&#x3D;1$</p><p>其中i表示第i个类，j表示第j个空间位置，T是一个大于0的控制参数</p><p>可以把$s_j^i$ 看作第 i 类出现在位置 j 的概率 </p><p>$a^i&#x3D;\sum_{k&#x3D;1}^{49}s_k^ix_k$</p><p>解释CSRA模块</p><p>$y^i&#x3D;m_i^Tg+λm_i^Ta^i$&#x3D;$\frac{1}{49}\sum_{k&#x3D;1}^{49}x_k^Tm_i+λ\sum_{k&#x3D;1}^{49}\frac{exp(Tx_k^Tm_i)}{ \sum_{l&#x3D;1}^{49}exp(Tx_l^Tm_i)}x_k^Tm_i$</p><p>当 T 趋近于 ∞，加号后面的部分就变为$λmax(x_1^Tm_i,..,x_{49}^Tm_i)$,相当于代码中的λ×y_max。</p><p>更具体地说：</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312292222788.png" alt="image-20231229222233728"> </p><p>多头注意力：由于T的值很难调整，所以采用多头注意力机制，T值虽然不同，但是λ都一样。</p><p><img src="https://imagefortjy.oss-cn-shenzhen.aliyuncs.com/img/202312292234066.png" alt="image-20231229223451020"></p><h4 id="最大下采样的作用"><a href="#最大下采样的作用" class="headerlink" title="最大下采样的作用"></a>最大下采样的作用</h4><p>y_max是在每个类别的所有空间位置中找到最大值，所以，它可以被看作是一种特定于类的注意力机制。作者说可以把它推测为将注意力集中在不同对象类别在不同位置的分类分数上。直观地说，这种注意力机制对于多标签识别非常有用，特别是当对象来自许多类别且大小不同时。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-音频分类</title>
      <link href="/2023/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E9%9F%B3%E9%A2%91%E5%88%86%E7%B1%BB/"/>
      <url>/2023/05/18/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E9%9F%B3%E9%A2%91%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-数据集的划分</title>
      <link href="/2023/05/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-k%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C/"/>
      <url>/2023/05/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-k%E4%BA%A4%E5%8F%89%E6%A3%80%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="数据集的划分方法"><a href="#数据集的划分方法" class="headerlink" title="数据集的划分方法"></a>数据集的划分方法</h1><h2 id="留出法"><a href="#留出法" class="headerlink" title="留出法"></a>留出法</h2><p>留出法：将已知数据集分成两个互斥的部分，其中一部分用来训练模型，另一部分用来测试模型，评估其误差，作为泛化误差的估计</p><p><strong>通常，训练集:测试集&#x3D;7:3（按照分层采样的方式）</strong></p><h2 id="交叉验证法"><a href="#交叉验证法" class="headerlink" title="交叉验证法"></a>交叉验证法</h2><p>k折交叉验证法：将数据集D划分K个大小相似的互斥的数据子集，子集数据尽可能保证数据分布的一致性（分层采样），每次从中选取一个数据集作为测试集，其余用作训练集，可以进行k次训练和测试，得到评估均值。也叫k折交叉验证。重复p次，就是p次k折交叉验证。K的取值会影响到结果的稳定性和保真性，通常k取10。</p><p>常用于分类</p><p><img src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86.jpg"></p><p>实现代码</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, KFold</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, TensorDataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设我们有一些数据</span></span><br><span class="line">X = torch.randn(<span class="number">1000</span>, <span class="number">20</span>) <span class="comment">#创建一个形状为（1000,20）的张量</span></span><br><span class="line">y = torch.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">1000</span>,)) </span><br><span class="line"><span class="comment">#创建了一个长度为1000的一维张量，</span></span><br><span class="line"><span class="comment">#其中每个元素都是从[0, 2)区间内随机抽取的整数。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 留出法</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line">train_dataset = TensorDataset(X_train, y_train)</span><br><span class="line">test_dataset = TensorDataset(X_test, y_test)</span><br><span class="line"><span class="comment">#接受两个张量作为输入，分别表示特征数据和标签数据。</span></span><br><span class="line"><span class="comment">#TensorDataset对象可以将这两个张量打包在一起，方便后续的数据加载。</span></span><br><span class="line">trainloader = DataLoader(train_dataset, batch_size=<span class="number">100</span>)</span><br><span class="line">testloader = DataLoader(test_dataset, batch_size=<span class="number">50</span>)</span><br><span class="line"><span class="comment">#DataLoader类可以从数据集中批量抽取数据，并将其封装成一个迭代器</span></span><br><span class="line"><span class="comment">#，方便后续的数据加载。</span></span><br><span class="line"><span class="comment"># k折交叉验证</span></span><br><span class="line">kfold = KFold(n_splits=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> fold, (train_index, test_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kfold.split(X)):</span><br><span class="line">    X_train_fold = X[train_index]</span><br><span class="line">    y_train_fold = y[train_index]</span><br><span class="line">    X_test_fold = X[test_index]</span><br><span class="line">    y_test_fold = y[test_index]</span><br><span class="line">    <span class="comment">#kfold.split(X)函数会返回两个索引数组，</span></span><br><span class="line">    <span class="comment">#分别表示训练集和测试集中的样本索引。</span></span><br><span class="line">    train_dataset_fold = TensorDataset(X_train_fold, y_train_fold)</span><br><span class="line">    test_dataset_fold = TensorDataset(X_test_fold, y_test_fold)</span><br><span class="line"></span><br><span class="line">    trainloader_fold = DataLoader(train_dataset_fold, batch_size=<span class="number">100</span>)</span><br><span class="line">    testloader_fold = DataLoader(test_dataset_fold, batch_size=<span class="number">50</span>)</span><br></pre></td></tr></table></figure><h2 id="自助法"><a href="#自助法" class="headerlink" title="自助法"></a>自助法</h2><p>自助法：是一种产生样本的抽样方法，其实质是有放回的随机抽样。在给定的m个样本的数据集中随机抽取一条记录，然后将该记录放入训练集同时放回原数据集（该记录在下一次仍有可能被采到），继续下一次抽样，该操作执行m次后，就得到了所需训练集。</p><p>样本在m次不被取到的概率为$(1-\frac{1}{m})^m$,取极限得0.368，所以测试集大约占36.8%。</p><p>常用于数据集小，难以有效划分数据集时。</p><p>一般使用交叉验证法和留出法居多。</p><h2 id="解释一下批次，批量大小以及迭代次数"><a href="#解释一下批次，批量大小以及迭代次数" class="headerlink" title="解释一下批次，批量大小以及迭代次数"></a>解释一下批次，批量大小以及迭代次数</h2><p>批次（epoch)指的是在训练过程中，遍历整个训练集的次数。</p><p>批量大小(batch_size)指的是在每次迭代中，从训练集中抽取的样本数量。</p><p>迭代次数(iteration)是指 训练集数量&#x2F;批量大小</p><p>假设批量大小为100，批次为10，训练集量为10000个样本，那么需要训练10次，每一次有100次迭代，每一次迭代包括100个样本</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Marktext用法、hexo指令以及一些乱七八糟的指令</title>
      <link href="/2023/05/11/Marktext%E7%94%A8%E6%B3%95%E4%BB%A5%E5%8F%8Ahexo%E6%8C%87%E4%BB%A4/"/>
      <url>/2023/05/11/Marktext%E7%94%A8%E6%B3%95%E4%BB%A5%E5%8F%8Ahexo%E6%8C%87%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="Martext用法学习记录"><a href="#Martext用法学习记录" class="headerlink" title="Martext用法学习记录"></a>Martext用法学习记录</h1><h4 id="键可以显示出所有可用功能的窗口"><a href="#键可以显示出所有可用功能的窗口" class="headerlink" title="@键可以显示出所有可用功能的窗口"></a><code>@</code>键可以显示出所有可用功能的窗口</h4><img title="" src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/2023-05-11-10-38-58-image.png" alt="" width="323"><h4 id="Ctrl-j-可以打开侧边栏"><a href="#Ctrl-j-可以打开侧边栏" class="headerlink" title="Ctrl+j 可以打开侧边栏"></a><code>Ctrl+j</code> 可以打开侧边栏</h4><img title="" src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/2023-05-11-10-42-56-image.png" alt="" width="355"><h4 id="Ctrl-shift-T可以创建表格"><a href="#Ctrl-shift-T可以创建表格" class="headerlink" title="Ctrl+shift+T可以创建表格"></a><code>Ctrl+shift+T</code>可以创建表格</h4><img title="" src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/2023-05-11-10-46-32-image.png" alt="" width="461"><img title="" src="https://imagfromtjy.oss-cn-shenzhen.aliyuncs.com/img/2023-05-11-10-46-54-image.png" alt="" width="450"><h4 id="s-可以创建小表情"><a href="#s-可以创建小表情" class="headerlink" title=":s:可以创建小表情"></a><code>:s:</code>可以创建小表情</h4><p>:sleeping::sneezing_face::kissing_closed_eyes:</p><h4 id="改变字体颜色加上-lt-font-color-39-red-39-gt-lt-font-gt"><a href="#改变字体颜色加上-lt-font-color-39-red-39-gt-lt-font-gt" class="headerlink" title="改变字体颜色加上 &lt;font color=&#39;red&#39;&gt;&lt;/font&gt;"></a>改变字体颜色加上<code> &lt;font color=&#39;red&#39;&gt;&lt;/font&gt;</code></h4><p> <font color='red'>改变字体颜色</font></p><h4 id="Ctrl-Shift-i快速导入图片"><a href="#Ctrl-Shift-i快速导入图片" class="headerlink" title="Ctrl+Shift+i快速导入图片"></a><code>Ctrl+Shift+i</code>快速导入图片</h4><p><img src="/"></p><h4 id="数学符号书写语法"><a href="#数学符号书写语法" class="headerlink" title="数学符号书写语法"></a>数学符号书写语法</h4><h4 id="上标：-x-2-x-2"><a href="#上标：-x-2-x-2" class="headerlink" title="上标：$x^{2}$  $x^2$"></a>上标：<code>$x^&#123;2&#125;$</code>  $x^2$</h4><h4 id="下标：-x-i-x-i"><a href="#下标：-x-i-x-i" class="headerlink" title="下标：$x_{i}$ $x_{i}$"></a>下标：<code>$x_&#123;i&#125;$</code> $x_{i}$</h4><h5 id="分数：-frac-分子-分母-frac-分子-分母"><a href="#分数：-frac-分子-分母-frac-分子-分母" class="headerlink" title="分数：$\frac{分子}{分母}$  $\frac{分子}{分母}$"></a>分数：<code>$\frac&#123;分子&#125;&#123;分母&#125;$</code>  $\frac{分子}{分母}$</h5><h4 id="累加：-sum-i-0-y-x-i-sum-i-x3D-0-y-x-i"><a href="#累加：-sum-i-0-y-x-i-sum-i-x3D-0-y-x-i" class="headerlink" title="累加：$\sum_{i=0}^{y}x_i$  $\sum_{i&#x3D;0}^{y}x_i$"></a>累加：<code>$\sum_&#123;i=0&#125;^&#123;y&#125;x_i$</code>  $\sum_{i&#x3D;0}^{y}x_i$</h4><h4 id="累和：-prod-i-0-y-x-i-prod-i-x3D-0-y-x-i"><a href="#累和：-prod-i-0-y-x-i-prod-i-x3D-0-y-x-i" class="headerlink" title="累和：$\prod_{i=0}^{y}x_i$  $\prod_{i&#x3D;0}^{y}x_i$"></a>累和：<code>$\prod_&#123;i=0&#125;^&#123;y&#125;x_i$</code>  $\prod_{i&#x3D;0}^{y}x_i$</h4><h4 id="极限：-lim-x-to-infty-表达式-lim-x-to-infty-表达式"><a href="#极限：-lim-x-to-infty-表达式-lim-x-to-infty-表达式" class="headerlink" title="极限：$\lim_{x \to \infty} 表达式$  $\lim_{x \to \infty}  表达式$"></a>极限：<code>$\lim_&#123;x \to \infty&#125; 表达式$</code>  $\lim_{x \to \infty}  表达式$</h4><h4 id="积分：-int-1-y-x-int-1-y-x"><a href="#积分：-int-1-y-x-int-1-y-x" class="headerlink" title="积分：$\int_{1}^{y}{x}$   $\int_{1}^{y}{x}$"></a>积分：<code>$\int_&#123;1&#125;^&#123;y&#125;&#123;x&#125;$</code>   $\int_{1}^{y}{x}$</h4><h4 id="矩阵："><a href="#矩阵：" class="headerlink" title="矩阵："></a>矩阵：</h4><h5 id="不带括号：-begin-matrix-1-amp-2-amp-3-4-amp-5-amp-6-end-matrix"><a href="#不带括号：-begin-matrix-1-amp-2-amp-3-4-amp-5-amp-6-end-matrix" class="headerlink" title="不带括号：\begin{matrix} 1&amp;2&amp;3\\4&amp;5&amp;6\end{matrix}"></a>不带括号：<code>\begin&#123;matrix&#125; 1&amp;2&amp;3\\4&amp;5&amp;6\end&#123;matrix&#125;</code></h5><p>$\begin{matrix} 1&amp;2&amp;3\4&amp;5&amp;6\end{matrix}$</p><h5 id="带括号：-left-begin-matrix-1-amp-2-amp-3-4-amp-5-amp-6-end-matrix-right"><a href="#带括号：-left-begin-matrix-1-amp-2-amp-3-4-amp-5-amp-6-end-matrix-right" class="headerlink" title="带括号： $\left\{\begin{matrix} 1&amp;2&amp;3\\4&amp;5&amp;6 \end{matrix}\right\}$"></a>带括号： <code>$\left\&#123;\begin&#123;matrix&#125; 1&amp;2&amp;3\\4&amp;5&amp;6 \end&#123;matrix&#125;\right\&#125;$</code></h5><p>$\left{\begin{matrix} 1&amp;2&amp;3\4&amp;5&amp;6 \end{matrix}\right}$</p><h4 id="对数：-log-lg-ln-log-lg-ln"><a href="#对数：-log-lg-ln-log-lg-ln" class="headerlink" title="对数：$\log$ $\lg$ $\ln$  $\log$  $\lg $  $ \ln$"></a>对数：<code>$\log$ $\lg$ $\ln$</code>  $\log$  $\lg $  $ \ln$</h4><h4 id="公式加注释：-f-x-begin-cases-0-amp-text-if-x-is-even-1-amp-text-if-x-is-odd-end-cases-​f-x-x3D-begin-cases-0-amp-text-if-x-is-even-1-amp-text-if-x-is-odd-end-cases"><a href="#公式加注释：-f-x-begin-cases-0-amp-text-if-x-is-even-1-amp-text-if-x-is-odd-end-cases-​f-x-x3D-begin-cases-0-amp-text-if-x-is-even-1-amp-text-if-x-is-odd-end-cases" class="headerlink" title="公式加注释：$f(x)= \begin{cases} 0,&amp; \text{if x is even} \\ 1, &amp; \text{if x is odd} \end{cases}$$​f(x)&#x3D; \begin{cases} 0,&amp; \text{if x is even} \ 1, &amp; \text{if x is odd} \end{cases}$"></a>公式加注释：<code>$f(x)= \begin&#123;cases&#125; 0,&amp; \text&#123;if x is even&#125; \\ 1, &amp; \text&#123;if x is odd&#125; \end&#123;cases&#125;$</code>$​f(x)&#x3D; \begin{cases} 0,&amp; \text{if x is even} \ 1, &amp; \text{if x is odd} \end{cases}$</h4><h4 id="希腊字母："><a href="#希腊字母：" class="headerlink" title="希腊字母："></a>希腊字母：</h4><h5 id="theta-theta"><a href="#theta-theta" class="headerlink" title="$\theta$  $\theta$"></a>$\theta$  <code>$\theta$</code></h5><h5 id="alpha-alpha"><a href="#alpha-alpha" class="headerlink" title="$\alpha$  $\alpha$"></a>$\alpha$  <code>$\alpha$</code></h5><h5 id="beta-beta"><a href="#beta-beta" class="headerlink" title="$\beta$   $\beta$"></a>$\beta$  <code> $\beta$</code></h5><h5 id="gamma-gamma"><a href="#gamma-gamma" class="headerlink" title="$\gamma$  $\gamma$ "></a>$\gamma$  <code>$\gamma$ </code></h5><h5 id="pi-pi"><a href="#pi-pi" class="headerlink" title="$\pi$    $\pi$"></a>$\pi$   <code> $\pi$</code></h5><h5 id="lambda-lambda"><a href="#lambda-lambda" class="headerlink" title="$\lambda$  $\lambda$"></a>$\lambda$  <code>$\lambda$</code></h5><h5 id="sigma-sigma"><a href="#sigma-sigma" class="headerlink" title="$\sigma$   $\sigma$"></a>$\sigma$   <code>$\sigma$</code></h5><h5 id="omega-omega"><a href="#omega-omega" class="headerlink" title="$\omega$  $\omega$"></a>$\omega$  <code>$\omega$</code></h5><h5 id="chi-chi"><a href="#chi-chi" class="headerlink" title="$\chi$   $\chi$"></a>$\chi$   <code>$\chi$</code></h5><h5 id="tau-tau"><a href="#tau-tau" class="headerlink" title="$\tau$   $\tau$"></a>$\tau$   <code>$\tau$</code></h5><h1 id="hexo指令介绍"><a href="#hexo指令介绍" class="headerlink" title="hexo指令介绍"></a><code>hexo</code>指令介绍</h1><h4 id="hexo-cl"><a href="#hexo-cl" class="headerlink" title="hexo cl"></a><code>hexo cl</code></h4><p>hexo clean 的简写，清除本地缓存，把博客文件夹的public文件夹删除，public文件夹是基于本地文件生成用于上传到仓库或者其他网站服务器的文件夹，相当于中间站，删除不会影响本地的内容。</p><h4 id="hexo-g"><a href="#hexo-g" class="headerlink" title="hexo g"></a><code>hexo g</code></h4><p>hexo generate 的简写，意思是生成public文件夹</p><p><code>hexo d</code> </p><p>hexo deploy 的简写，将生成的public文件夹部署到网上</p><h4 id="hexo-s"><a href="#hexo-s" class="headerlink" title="hexo s"></a><code>hexo s</code></h4><p>启动本地服务器，用于预览主题</p><h4 id="hexo-n-name"><a href="#hexo-n-name" class="headerlink" title="hexo n name"></a><code>hexo n name</code></h4><p>hexo new 的简写，新建一个name.md文件</p><p><code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo s </code>    组合指令</p><h1 id="ping指令介绍"><a href="#ping指令介绍" class="headerlink" title="ping指令介绍"></a><code>ping</code>指令介绍</h1><p>ping 命令是测试网络是否畅通的工具，用于测试网络连接的程序</p><p><code>ping 域名</code> ：检查这个域名是否正常工作，或者可以看见自己做的域名解析是否生效，另外还可以看见访问这个域名的速度怎么样</p><p><code>ping -n 20 域名</code>：向目标域名发送 20 个数据包。这个命令可以用来测试网络的连通情况和分析网络速度</p><p><code>ping -t 域名</code>：该命令会一直进行下去，直到按“Ctrl+C”组合键停止。按“Ctrl+Break”组合键可以在不停止的情况下查看统计数据</p><p><code>ping -l 5600 -n 2 域名</code>：-l后面的数据代表了发送数据包的大小，而-n后面的数据设置了回送请求个数</p><p><code>ping  -i 3 域名</code>：-i后面的数据设置了请求报文的TTL，这个值每经过一个路由器就会减一，当值等于1的时候就会丢弃该分组，造成超时</p><p><code>ping -n 1 -r 7 域名</code>：-r 后面的数据设置了需记录的路由个数，第二次执行命令的时候路由产生了变化，这是因为每个IP分组都是独立路由的结果。</p>]]></content>
      
      
      <categories>
          
          <category> 博客 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 指令 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
